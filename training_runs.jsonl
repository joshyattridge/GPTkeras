{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 1.0067337512969972, "val_loss": 0.9486046218872071, "val_acc": 0.48}, {"epoch": 2, "train_loss": 0.8849996709823609, "val_loss": 0.8621234798431396, "val_acc": 0.62}, {"epoch": 3, "train_loss": 0.8115028691291809, "val_loss": 0.8022264242172241, "val_acc": 0.625}, {"epoch": 4, "train_loss": 0.7351275897026062, "val_loss": 0.7204679083824158, "val_acc": 0.71}, {"epoch": 5, "train_loss": 0.6702573442459107, "val_loss": 0.6614776492118836, "val_acc": 0.745}, {"epoch": 6, "train_loss": 0.6230600023269653, "val_loss": 0.6293046283721924, "val_acc": 0.7}, {"epoch": 7, "train_loss": 0.5724204540252685, "val_loss": 0.5763677334785462, "val_acc": 0.77}, {"epoch": 8, "train_loss": 0.5391963791847229, "val_loss": 0.5506830549240113, "val_acc": 0.78}, {"epoch": 9, "train_loss": 0.5158848261833191, "val_loss": 0.5331798553466797, "val_acc": 0.775}, {"epoch": 10, "train_loss": 0.5068857944011689, "val_loss": 0.5028002834320069, "val_acc": 0.795}, {"epoch": 11, "train_loss": 0.481302992105484, "val_loss": 0.5057082343101501, "val_acc": 0.78}, {"epoch": 12, "train_loss": 0.4715612351894379, "val_loss": 0.4817227506637573, "val_acc": 0.81}, {"epoch": 13, "train_loss": 0.4670651149749756, "val_loss": 0.48264613389968875, "val_acc": 0.79}, {"epoch": 14, "train_loss": 0.4556010210514069, "val_loss": 0.4762165117263794, "val_acc": 0.8}, {"epoch": 15, "train_loss": 0.44913923382759097, "val_loss": 0.4761407923698425, "val_acc": 0.785}, {"epoch": 16, "train_loss": 0.4393912923336029, "val_loss": 0.47137898206710815, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.43734227418899535, "val_loss": 0.4586651849746704, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.43485326766967775, "val_loss": 0.4705222225189209, "val_acc": 0.78}, {"epoch": 19, "train_loss": 0.43416388273239137, "val_loss": 0.45806352615356444, "val_acc": 0.795}, {"epoch": 20, "train_loss": 0.4270804274082184, "val_loss": 0.45139691829681394, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.4272925400733948, "val_loss": 0.4609017658233643, "val_acc": 0.795}, {"epoch": 22, "train_loss": 0.4343694758415222, "val_loss": 0.4610349702835083, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.42693010330200193, "val_loss": 0.4553215456008911, "val_acc": 0.79}, {"epoch": 24, "train_loss": 0.42645740032196044, "val_loss": 0.4537976336479187, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}], "final_metrics": {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 1.0067337512969972, "val_loss": 0.9486046218872071, "val_acc": 0.48}, {"epoch": 2, "train_loss": 0.8849996709823609, "val_loss": 0.8621234798431396, "val_acc": 0.62}, {"epoch": 3, "train_loss": 0.8115028691291809, "val_loss": 0.8022264242172241, "val_acc": 0.625}, {"epoch": 4, "train_loss": 0.7351275897026062, "val_loss": 0.7204679083824158, "val_acc": 0.71}, {"epoch": 5, "train_loss": 0.6702573442459107, "val_loss": 0.6614776492118836, "val_acc": 0.745}, {"epoch": 6, "train_loss": 0.6230600023269653, "val_loss": 0.6293046283721924, "val_acc": 0.7}, {"epoch": 7, "train_loss": 0.5724204540252685, "val_loss": 0.5763677334785462, "val_acc": 0.77}, {"epoch": 8, "train_loss": 0.5391963791847229, "val_loss": 0.5506830549240113, "val_acc": 0.78}, {"epoch": 9, "train_loss": 0.5158848261833191, "val_loss": 0.5331798553466797, "val_acc": 0.775}, {"epoch": 10, "train_loss": 0.5068857944011689, "val_loss": 0.5028002834320069, "val_acc": 0.795}, {"epoch": 11, "train_loss": 0.481302992105484, "val_loss": 0.5057082343101501, "val_acc": 0.78}, {"epoch": 12, "train_loss": 0.4715612351894379, "val_loss": 0.4817227506637573, "val_acc": 0.81}, {"epoch": 13, "train_loss": 0.4670651149749756, "val_loss": 0.48264613389968875, "val_acc": 0.79}, {"epoch": 14, "train_loss": 0.4556010210514069, "val_loss": 0.4762165117263794, "val_acc": 0.8}, {"epoch": 15, "train_loss": 0.44913923382759097, "val_loss": 0.4761407923698425, "val_acc": 0.785}, {"epoch": 16, "train_loss": 0.4393912923336029, "val_loss": 0.47137898206710815, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.43734227418899535, "val_loss": 0.4586651849746704, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.43485326766967775, "val_loss": 0.4705222225189209, "val_acc": 0.78}, {"epoch": 19, "train_loss": 0.43416388273239137, "val_loss": 0.45806352615356444, "val_acc": 0.795}, {"epoch": 20, "train_loss": 0.4270804274082184, "val_loss": 0.45139691829681394, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.4272925400733948, "val_loss": 0.4609017658233643, "val_acc": 0.795}, {"epoch": 22, "train_loss": 0.4343694758415222, "val_loss": 0.4610349702835083, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.42693010330200193, "val_loss": 0.4553215456008911, "val_acc": 0.79}, {"epoch": 24, "train_loss": 0.42645740032196044, "val_loss": 0.4537976336479187, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}], "final_metrics": {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 1.0067337512969972, "val_loss": 0.9486046218872071, "val_acc": 0.48}, {"epoch": 2, "train_loss": 0.8849996709823609, "val_loss": 0.8621234798431396, "val_acc": 0.62}, {"epoch": 3, "train_loss": 0.8115028691291809, "val_loss": 0.8022264242172241, "val_acc": 0.625}, {"epoch": 4, "train_loss": 0.7351275897026062, "val_loss": 0.7204679083824158, "val_acc": 0.71}, {"epoch": 5, "train_loss": 0.6702573442459107, "val_loss": 0.6614776492118836, "val_acc": 0.745}, {"epoch": 6, "train_loss": 0.6230600023269653, "val_loss": 0.6293046283721924, "val_acc": 0.7}, {"epoch": 7, "train_loss": 0.5724204540252685, "val_loss": 0.5763677334785462, "val_acc": 0.77}, {"epoch": 8, "train_loss": 0.5391963791847229, "val_loss": 0.5506830549240113, "val_acc": 0.78}, {"epoch": 9, "train_loss": 0.5158848261833191, "val_loss": 0.5331798553466797, "val_acc": 0.775}, {"epoch": 10, "train_loss": 0.5068857944011689, "val_loss": 0.5028002834320069, "val_acc": 0.795}, {"epoch": 11, "train_loss": 0.481302992105484, "val_loss": 0.5057082343101501, "val_acc": 0.78}, {"epoch": 12, "train_loss": 0.4715612351894379, "val_loss": 0.4817227506637573, "val_acc": 0.81}, {"epoch": 13, "train_loss": 0.4670651149749756, "val_loss": 0.48264613389968875, "val_acc": 0.79}, {"epoch": 14, "train_loss": 0.4556010210514069, "val_loss": 0.4762165117263794, "val_acc": 0.8}, {"epoch": 15, "train_loss": 0.44913923382759097, "val_loss": 0.4761407923698425, "val_acc": 0.785}, {"epoch": 16, "train_loss": 0.4393912923336029, "val_loss": 0.47137898206710815, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.43734227418899535, "val_loss": 0.4586651849746704, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.43485326766967775, "val_loss": 0.4705222225189209, "val_acc": 0.78}, {"epoch": 19, "train_loss": 0.43416388273239137, "val_loss": 0.45806352615356444, "val_acc": 0.795}, {"epoch": 20, "train_loss": 0.4270804274082184, "val_loss": 0.45139691829681394, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.4272925400733948, "val_loss": 0.4609017658233643, "val_acc": 0.795}, {"epoch": 22, "train_loss": 0.4343694758415222, "val_loss": 0.4610349702835083, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.42693010330200193, "val_loss": 0.4553215456008911, "val_acc": 0.79}, {"epoch": 24, "train_loss": 0.42645740032196044, "val_loss": 0.4537976336479187, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}], "final_metrics": {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 1.0067337512969972, "val_loss": 0.9486046218872071, "val_acc": 0.48}, {"epoch": 2, "train_loss": 0.8849996709823609, "val_loss": 0.8621234798431396, "val_acc": 0.62}, {"epoch": 3, "train_loss": 0.8115028691291809, "val_loss": 0.8022264242172241, "val_acc": 0.625}, {"epoch": 4, "train_loss": 0.7351275897026062, "val_loss": 0.7204679083824158, "val_acc": 0.71}, {"epoch": 5, "train_loss": 0.6702573442459107, "val_loss": 0.6614776492118836, "val_acc": 0.745}, {"epoch": 6, "train_loss": 0.6230600023269653, "val_loss": 0.6293046283721924, "val_acc": 0.7}, {"epoch": 7, "train_loss": 0.5724204540252685, "val_loss": 0.5763677334785462, "val_acc": 0.77}, {"epoch": 8, "train_loss": 0.5391963791847229, "val_loss": 0.5506830549240113, "val_acc": 0.78}, {"epoch": 9, "train_loss": 0.5158848261833191, "val_loss": 0.5331798553466797, "val_acc": 0.775}, {"epoch": 10, "train_loss": 0.5068857944011689, "val_loss": 0.5028002834320069, "val_acc": 0.795}, {"epoch": 11, "train_loss": 0.481302992105484, "val_loss": 0.5057082343101501, "val_acc": 0.78}, {"epoch": 12, "train_loss": 0.4715612351894379, "val_loss": 0.4817227506637573, "val_acc": 0.81}, {"epoch": 13, "train_loss": 0.4670651149749756, "val_loss": 0.48264613389968875, "val_acc": 0.79}, {"epoch": 14, "train_loss": 0.4556010210514069, "val_loss": 0.4762165117263794, "val_acc": 0.8}, {"epoch": 15, "train_loss": 0.44913923382759097, "val_loss": 0.4761407923698425, "val_acc": 0.785}, {"epoch": 16, "train_loss": 0.4393912923336029, "val_loss": 0.47137898206710815, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.43734227418899535, "val_loss": 0.4586651849746704, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.43485326766967775, "val_loss": 0.4705222225189209, "val_acc": 0.78}, {"epoch": 19, "train_loss": 0.43416388273239137, "val_loss": 0.45806352615356444, "val_acc": 0.795}, {"epoch": 20, "train_loss": 0.4270804274082184, "val_loss": 0.45139691829681394, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.4272925400733948, "val_loss": 0.4609017658233643, "val_acc": 0.795}, {"epoch": 22, "train_loss": 0.4343694758415222, "val_loss": 0.4610349702835083, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.42693010330200193, "val_loss": 0.4553215456008911, "val_acc": 0.79}, {"epoch": 24, "train_loss": 0.42645740032196044, "val_loss": 0.4537976336479187, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}], "final_metrics": {"epoch": 25, "train_loss": 0.425895471572876, "val_loss": 0.4554232311248779, "val_acc": 0.79}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 0.9843513011932373, "val_loss": 0.9047394967079163, "val_acc": 0.58}, {"epoch": 2, "train_loss": 0.8216707110404968, "val_loss": 0.7304354548454285, "val_acc": 0.615}, {"epoch": 3, "train_loss": 0.644498143196106, "val_loss": 0.5736521720886231, "val_acc": 0.765}, {"epoch": 4, "train_loss": 0.5265160298347473, "val_loss": 0.48795778274536133, "val_acc": 0.8}, {"epoch": 5, "train_loss": 0.4809634304046631, "val_loss": 0.485700216293335, "val_acc": 0.77}, {"epoch": 6, "train_loss": 0.47356014132499696, "val_loss": 0.43869666576385496, "val_acc": 0.84}, {"epoch": 7, "train_loss": 0.46164268255233765, "val_loss": 0.4357795214653015, "val_acc": 0.795}, {"epoch": 8, "train_loss": 0.46565366148948667, "val_loss": 0.5257364082336425, "val_acc": 0.77}, {"epoch": 9, "train_loss": 0.4681847643852234, "val_loss": 0.4632417726516724, "val_acc": 0.805}, {"epoch": 10, "train_loss": 0.44565892457962036, "val_loss": 0.4130148732662201, "val_acc": 0.79}, {"epoch": 11, "train_loss": 0.4438547670841217, "val_loss": 0.42187091827392575, "val_acc": 0.825}, {"epoch": 12, "train_loss": 0.4481248068809509, "val_loss": 0.41947678565979, "val_acc": 0.82}, {"epoch": 13, "train_loss": 0.44476788997650146, "val_loss": 0.4160110902786255, "val_acc": 0.82}, {"epoch": 14, "train_loss": 0.45466936826705934, "val_loss": 0.4393052566051483, "val_acc": 0.785}, {"epoch": 15, "train_loss": 0.4417376351356506, "val_loss": 0.4201365077495575, "val_acc": 0.805}, {"epoch": 16, "train_loss": 0.43338008642196657, "val_loss": 0.42957635164260866, "val_acc": 0.825}, {"epoch": 17, "train_loss": 0.42896307706832887, "val_loss": 0.44648670077323915, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.44044565558433535, "val_loss": 0.43951432466506957, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.45230750799179076, "val_loss": 0.4247144865989685, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.4673351335525513, "val_loss": 0.5026200127601623, "val_acc": 0.745}, {"epoch": 21, "train_loss": 0.4921862268447876, "val_loss": 0.41918158531188965, "val_acc": 0.82}, {"epoch": 22, "train_loss": 0.45764703273773194, "val_loss": 0.47118751287460325, "val_acc": 0.785}, {"epoch": 23, "train_loss": 0.4542748785018921, "val_loss": 0.4330461525917053, "val_acc": 0.81}, {"epoch": 24, "train_loss": 0.42691744565963746, "val_loss": 0.4137323522567749, "val_acc": 0.805}, {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}], "final_metrics": {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 0.9843513011932373, "val_loss": 0.9047394967079163, "val_acc": 0.58}, {"epoch": 2, "train_loss": 0.8216707110404968, "val_loss": 0.7304354548454285, "val_acc": 0.615}, {"epoch": 3, "train_loss": 0.644498143196106, "val_loss": 0.5736521720886231, "val_acc": 0.765}, {"epoch": 4, "train_loss": 0.5265160298347473, "val_loss": 0.48795778274536133, "val_acc": 0.8}, {"epoch": 5, "train_loss": 0.4809634304046631, "val_loss": 0.485700216293335, "val_acc": 0.77}, {"epoch": 6, "train_loss": 0.47356014132499696, "val_loss": 0.43869666576385496, "val_acc": 0.84}, {"epoch": 7, "train_loss": 0.46164268255233765, "val_loss": 0.4357795214653015, "val_acc": 0.795}, {"epoch": 8, "train_loss": 0.46565366148948667, "val_loss": 0.5257364082336425, "val_acc": 0.77}, {"epoch": 9, "train_loss": 0.4681847643852234, "val_loss": 0.4632417726516724, "val_acc": 0.805}, {"epoch": 10, "train_loss": 0.44565892457962036, "val_loss": 0.4130148732662201, "val_acc": 0.79}, {"epoch": 11, "train_loss": 0.4438547670841217, "val_loss": 0.42187091827392575, "val_acc": 0.825}, {"epoch": 12, "train_loss": 0.4481248068809509, "val_loss": 0.41947678565979, "val_acc": 0.82}, {"epoch": 13, "train_loss": 0.44476788997650146, "val_loss": 0.4160110902786255, "val_acc": 0.82}, {"epoch": 14, "train_loss": 0.45466936826705934, "val_loss": 0.4393052566051483, "val_acc": 0.785}, {"epoch": 15, "train_loss": 0.4417376351356506, "val_loss": 0.4201365077495575, "val_acc": 0.805}, {"epoch": 16, "train_loss": 0.43338008642196657, "val_loss": 0.42957635164260866, "val_acc": 0.825}, {"epoch": 17, "train_loss": 0.42896307706832887, "val_loss": 0.44648670077323915, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.44044565558433535, "val_loss": 0.43951432466506957, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.45230750799179076, "val_loss": 0.4247144865989685, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.4673351335525513, "val_loss": 0.5026200127601623, "val_acc": 0.745}, {"epoch": 21, "train_loss": 0.4921862268447876, "val_loss": 0.41918158531188965, "val_acc": 0.82}, {"epoch": 22, "train_loss": 0.45764703273773194, "val_loss": 0.47118751287460325, "val_acc": 0.785}, {"epoch": 23, "train_loss": 0.4542748785018921, "val_loss": 0.4330461525917053, "val_acc": 0.81}, {"epoch": 24, "train_loss": 0.42691744565963746, "val_loss": 0.4137323522567749, "val_acc": 0.805}, {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}], "final_metrics": {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 3))", "history": [{"epoch": 1, "train_loss": 0.9233387351036072, "val_loss": 0.7337618398666382, "val_acc": 0.635}, {"epoch": 2, "train_loss": 0.6119428086280823, "val_loss": 0.5381832480430603, "val_acc": 0.765}, {"epoch": 3, "train_loss": 0.47736916661262513, "val_loss": 0.5048772621154786, "val_acc": 0.765}, {"epoch": 4, "train_loss": 0.4529802858829498, "val_loss": 0.521127028465271, "val_acc": 0.77}, {"epoch": 5, "train_loss": 0.4556947362422943, "val_loss": 0.5043503403663635, "val_acc": 0.785}, {"epoch": 6, "train_loss": 0.4485176587104797, "val_loss": 0.525751895904541, "val_acc": 0.77}, {"epoch": 7, "train_loss": 0.4507916975021362, "val_loss": 0.5356218051910401, "val_acc": 0.745}, {"epoch": 8, "train_loss": 0.464043105840683, "val_loss": 0.48687992811203, "val_acc": 0.78}, {"epoch": 9, "train_loss": 0.42306200265884397, "val_loss": 0.487814838886261, "val_acc": 0.77}, {"epoch": 10, "train_loss": 0.43314430832862855, "val_loss": 0.49453447341918944, "val_acc": 0.795}, {"epoch": 11, "train_loss": 0.4319250214099884, "val_loss": 0.4698318541049957, "val_acc": 0.765}, {"epoch": 12, "train_loss": 0.42940823912620546, "val_loss": 0.47184057712554933, "val_acc": 0.785}, {"epoch": 13, "train_loss": 0.4121372807025909, "val_loss": 0.4870171666145325, "val_acc": 0.785}, {"epoch": 14, "train_loss": 0.42170664072036745, "val_loss": 0.4849669313430786, "val_acc": 0.78}, {"epoch": 15, "train_loss": 0.4161202108860016, "val_loss": 0.46417582631111143, "val_acc": 0.79}, {"epoch": 16, "train_loss": 0.4312313485145569, "val_loss": 0.5144489467144012, "val_acc": 0.77}, {"epoch": 17, "train_loss": 0.43907368898391724, "val_loss": 0.48780232906341553, "val_acc": 0.775}, {"epoch": 18, "train_loss": 0.4226838719844818, "val_loss": 0.48226341247558596, "val_acc": 0.775}, {"epoch": 19, "train_loss": 0.4212912058830261, "val_loss": 0.49470561504364013, "val_acc": 0.765}, {"epoch": 20, "train_loss": 0.42774937987327577, "val_loss": 0.48950278759002686, "val_acc": 0.775}, {"epoch": 21, "train_loss": 0.4480937576293945, "val_loss": 0.4906191110610962, "val_acc": 0.755}, {"epoch": 22, "train_loss": 0.4185711395740509, "val_loss": 0.5084364175796509, "val_acc": 0.775}, {"epoch": 23, "train_loss": 0.42288042783737184, "val_loss": 0.4917571520805359, "val_acc": 0.76}, {"epoch": 24, "train_loss": 0.4216306734085083, "val_loss": 0.4927514278888702, "val_acc": 0.775}, {"epoch": 25, "train_loss": 0.4291934835910797, "val_loss": 0.49987715125083926, "val_acc": 0.775}], "final_metrics": {"epoch": 25, "train_loss": 0.4291934835910797, "val_loss": 0.49987715125083926, "val_acc": 0.775}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 3))", "history": [{"epoch": 1, "train_loss": 0.9843513011932373, "val_loss": 0.9047394967079163, "val_acc": 0.58}, {"epoch": 2, "train_loss": 0.8216707110404968, "val_loss": 0.7304354548454285, "val_acc": 0.615}, {"epoch": 3, "train_loss": 0.644498143196106, "val_loss": 0.5736521720886231, "val_acc": 0.765}, {"epoch": 4, "train_loss": 0.5265160298347473, "val_loss": 0.48795778274536133, "val_acc": 0.8}, {"epoch": 5, "train_loss": 0.4809634304046631, "val_loss": 0.485700216293335, "val_acc": 0.77}, {"epoch": 6, "train_loss": 0.47356014132499696, "val_loss": 0.43869666576385496, "val_acc": 0.84}, {"epoch": 7, "train_loss": 0.46164268255233765, "val_loss": 0.4357795214653015, "val_acc": 0.795}, {"epoch": 8, "train_loss": 0.46565366148948667, "val_loss": 0.5257364082336425, "val_acc": 0.77}, {"epoch": 9, "train_loss": 0.4681847643852234, "val_loss": 0.4632417726516724, "val_acc": 0.805}, {"epoch": 10, "train_loss": 0.44565892457962036, "val_loss": 0.4130148732662201, "val_acc": 0.79}, {"epoch": 11, "train_loss": 0.4438547670841217, "val_loss": 0.42187091827392575, "val_acc": 0.825}, {"epoch": 12, "train_loss": 0.4481248068809509, "val_loss": 0.41947678565979, "val_acc": 0.82}, {"epoch": 13, "train_loss": 0.44476788997650146, "val_loss": 0.4160110902786255, "val_acc": 0.82}, {"epoch": 14, "train_loss": 0.45466936826705934, "val_loss": 0.4393052566051483, "val_acc": 0.785}, {"epoch": 15, "train_loss": 0.4417376351356506, "val_loss": 0.4201365077495575, "val_acc": 0.805}, {"epoch": 16, "train_loss": 0.43338008642196657, "val_loss": 0.42957635164260866, "val_acc": 0.825}, {"epoch": 17, "train_loss": 0.42896307706832887, "val_loss": 0.44648670077323915, "val_acc": 0.8}, {"epoch": 18, "train_loss": 0.44044565558433535, "val_loss": 0.43951432466506957, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.45230750799179076, "val_loss": 0.4247144865989685, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.4673351335525513, "val_loss": 0.5026200127601623, "val_acc": 0.745}, {"epoch": 21, "train_loss": 0.4921862268447876, "val_loss": 0.41918158531188965, "val_acc": 0.82}, {"epoch": 22, "train_loss": 0.45764703273773194, "val_loss": 0.47118751287460325, "val_acc": 0.785}, {"epoch": 23, "train_loss": 0.4542748785018921, "val_loss": 0.4330461525917053, "val_acc": 0.81}, {"epoch": 24, "train_loss": 0.42691744565963746, "val_loss": 0.4137323522567749, "val_acc": 0.805}, {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}], "final_metrics": {"epoch": 25, "train_loss": 0.4377139639854431, "val_loss": 0.4063666546344757, "val_acc": 0.82}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 48), nn.ReLU(), nn.Linear(48, 24), nn.ReLU(), nn.Linear(24, 3))", "history": [{"epoch": 1, "train_loss": 1.0331732249259948, "val_loss": 0.9443162059783936, "val_acc": 0.525}, {"epoch": 2, "train_loss": 0.867948682308197, "val_loss": 0.7564259314537048, "val_acc": 0.72}, {"epoch": 3, "train_loss": 0.6725446534156799, "val_loss": 0.552422034740448, "val_acc": 0.8}, {"epoch": 4, "train_loss": 0.5327745938301086, "val_loss": 0.44137310147285463, "val_acc": 0.835}, {"epoch": 5, "train_loss": 0.4957656478881836, "val_loss": 0.3921108460426331, "val_acc": 0.84}, {"epoch": 6, "train_loss": 0.48502216100692747, "val_loss": 0.39756796360015867, "val_acc": 0.86}, {"epoch": 7, "train_loss": 0.4773188877105713, "val_loss": 0.5084917306900024, "val_acc": 0.825}, {"epoch": 8, "train_loss": 0.4889907217025757, "val_loss": 0.4107833778858185, "val_acc": 0.825}, {"epoch": 9, "train_loss": 0.463825523853302, "val_loss": 0.39581693410873414, "val_acc": 0.865}, {"epoch": 10, "train_loss": 0.45649682760238647, "val_loss": 0.38396056175231935, "val_acc": 0.865}, {"epoch": 11, "train_loss": 0.4473332142829895, "val_loss": 0.3613078808784485, "val_acc": 0.84}, {"epoch": 12, "train_loss": 0.451347895860672, "val_loss": 0.35709758043289186, "val_acc": 0.86}, {"epoch": 13, "train_loss": 0.4463330614566803, "val_loss": 0.36405399441719055, "val_acc": 0.84}, {"epoch": 14, "train_loss": 0.4614866602420807, "val_loss": 0.38881322622299197, "val_acc": 0.845}, {"epoch": 15, "train_loss": 0.4587574207782745, "val_loss": 0.3870158803462982, "val_acc": 0.845}, {"epoch": 16, "train_loss": 0.47944381952285764, "val_loss": 0.3831759709119797, "val_acc": 0.825}, {"epoch": 17, "train_loss": 0.46759371042251585, "val_loss": 0.35558160305023195, "val_acc": 0.84}, {"epoch": 18, "train_loss": 0.441222847700119, "val_loss": 0.3923151183128357, "val_acc": 0.84}, {"epoch": 19, "train_loss": 0.4429554975032806, "val_loss": 0.38668938517570495, "val_acc": 0.83}, {"epoch": 20, "train_loss": 0.45185367345809935, "val_loss": 0.39916716814041137, "val_acc": 0.85}, {"epoch": 21, "train_loss": 0.45739450931549075, "val_loss": 0.35392323732376096, "val_acc": 0.85}, {"epoch": 22, "train_loss": 0.4425439763069153, "val_loss": 0.37757555484771727, "val_acc": 0.845}, {"epoch": 23, "train_loss": 0.435533047914505, "val_loss": 0.41575494050979617, "val_acc": 0.845}, {"epoch": 24, "train_loss": 0.4606682348251343, "val_loss": 0.4508614671230316, "val_acc": 0.82}, {"epoch": 25, "train_loss": 0.4814710247516632, "val_loss": 0.39321701049804686, "val_acc": 0.86}], "final_metrics": {"epoch": 25, "train_loss": 0.4814710247516632, "val_loss": 0.39321701049804686, "val_acc": 0.86}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 48), nn.ReLU(), nn.Linear(48, 36), nn.ReLU(), nn.Linear(36, 3))", "history": [{"epoch": 1, "train_loss": 0.9872208881378174, "val_loss": 0.831321291923523, "val_acc": 0.73}, {"epoch": 2, "train_loss": 0.6986266469955444, "val_loss": 0.564135286808014, "val_acc": 0.74}, {"epoch": 3, "train_loss": 0.515192643404007, "val_loss": 0.41384933233261106, "val_acc": 0.855}, {"epoch": 4, "train_loss": 0.47379114508628845, "val_loss": 0.3894459283351898, "val_acc": 0.85}, {"epoch": 5, "train_loss": 0.46878045320510864, "val_loss": 0.3767884659767151, "val_acc": 0.86}, {"epoch": 6, "train_loss": 0.45849798440933226, "val_loss": 0.3776595211029053, "val_acc": 0.85}, {"epoch": 7, "train_loss": 0.4657587718963623, "val_loss": 0.41562030553817747, "val_acc": 0.855}, {"epoch": 8, "train_loss": 0.5422514700889587, "val_loss": 0.6104249000549317, "val_acc": 0.725}, {"epoch": 9, "train_loss": 0.5550719344615936, "val_loss": 0.40562929391860963, "val_acc": 0.845}, {"epoch": 10, "train_loss": 0.49316105246543884, "val_loss": 0.38587780833244323, "val_acc": 0.865}, {"epoch": 11, "train_loss": 0.45426058769226074, "val_loss": 0.3829725134372711, "val_acc": 0.84}, {"epoch": 12, "train_loss": 0.4424034011363983, "val_loss": 0.3853782844543457, "val_acc": 0.88}, {"epoch": 13, "train_loss": 0.4525835645198822, "val_loss": 0.3937041735649109, "val_acc": 0.83}, {"epoch": 14, "train_loss": 0.44320823431015016, "val_loss": 0.3688957190513611, "val_acc": 0.87}, {"epoch": 15, "train_loss": 0.4361563384532928, "val_loss": 0.36699107885360716, "val_acc": 0.855}, {"epoch": 16, "train_loss": 0.4417396128177643, "val_loss": 0.3694485735893249, "val_acc": 0.87}, {"epoch": 17, "train_loss": 0.43770457863807677, "val_loss": 0.3657530784606934, "val_acc": 0.87}, {"epoch": 18, "train_loss": 0.45214008569717407, "val_loss": 0.3791801881790161, "val_acc": 0.855}, {"epoch": 19, "train_loss": 0.45583697080612184, "val_loss": 0.3799651634693146, "val_acc": 0.855}, {"epoch": 20, "train_loss": 0.4424219751358032, "val_loss": 0.3692040085792542, "val_acc": 0.88}, {"epoch": 21, "train_loss": 0.44021166205406187, "val_loss": 0.3906336259841919, "val_acc": 0.84}, {"epoch": 22, "train_loss": 0.4369510769844055, "val_loss": 0.37201473355293274, "val_acc": 0.87}, {"epoch": 23, "train_loss": 0.4407074224948883, "val_loss": 0.37785988569259643, "val_acc": 0.85}, {"epoch": 24, "train_loss": 0.43577964305877687, "val_loss": 0.3776939105987549, "val_acc": 0.88}, {"epoch": 25, "train_loss": 0.43879299640655517, "val_loss": 0.37006274223327634, "val_acc": 0.885}], "final_metrics": {"epoch": 25, "train_loss": 0.43879299640655517, "val_loss": 0.37006274223327634, "val_acc": 0.885}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 48), nn.ReLU(), nn.Linear(48, 3))", "history": [{"epoch": 1, "train_loss": 0.9439658379554748, "val_loss": 0.7013518381118774, "val_acc": 0.73}, {"epoch": 2, "train_loss": 0.6109218263626098, "val_loss": 0.5131269466876983, "val_acc": 0.755}, {"epoch": 3, "train_loss": 0.5351343739032746, "val_loss": 0.3637237048149109, "val_acc": 0.835}, {"epoch": 4, "train_loss": 0.5204328560829162, "val_loss": 0.3834173008799553, "val_acc": 0.87}, {"epoch": 5, "train_loss": 0.5213903319835663, "val_loss": 0.3792804580926895, "val_acc": 0.83}, {"epoch": 6, "train_loss": 0.472052538394928, "val_loss": 0.3601029706001282, "val_acc": 0.865}, {"epoch": 7, "train_loss": 0.46761040806770326, "val_loss": 0.3621868860721588, "val_acc": 0.865}, {"epoch": 8, "train_loss": 0.4718921113014221, "val_loss": 0.3737300902605057, "val_acc": 0.835}, {"epoch": 9, "train_loss": 0.47276705980300904, "val_loss": 0.360412395298481, "val_acc": 0.845}, {"epoch": 10, "train_loss": 0.4705814003944397, "val_loss": 0.38433645606040956, "val_acc": 0.855}, {"epoch": 11, "train_loss": 0.45168742775917053, "val_loss": 0.35841244041919706, "val_acc": 0.865}, {"epoch": 12, "train_loss": 0.46266516447067263, "val_loss": 0.3554943543672562, "val_acc": 0.855}, {"epoch": 13, "train_loss": 0.4622222065925598, "val_loss": 0.3431089973449707, "val_acc": 0.875}, {"epoch": 14, "train_loss": 0.45896952867507934, "val_loss": 0.40420363247394564, "val_acc": 0.815}, {"epoch": 15, "train_loss": 0.46801217794418337, "val_loss": 0.3580989527702332, "val_acc": 0.835}, {"epoch": 16, "train_loss": 0.46184300899505615, "val_loss": 0.35994656801223757, "val_acc": 0.865}, {"epoch": 17, "train_loss": 0.47049423277378083, "val_loss": 0.3565095922350883, "val_acc": 0.865}, {"epoch": 18, "train_loss": 0.46431282520294187, "val_loss": 0.39464322924613954, "val_acc": 0.82}, {"epoch": 19, "train_loss": 0.48560317516326906, "val_loss": 0.3694604027271271, "val_acc": 0.855}, {"epoch": 20, "train_loss": 0.4417827093601227, "val_loss": 0.35455340921878814, "val_acc": 0.855}, {"epoch": 21, "train_loss": 0.45112436890602114, "val_loss": 0.3422699576616287, "val_acc": 0.845}, {"epoch": 22, "train_loss": 0.445994873046875, "val_loss": 0.3708506888151169, "val_acc": 0.865}, {"epoch": 23, "train_loss": 0.44838969111442567, "val_loss": 0.36950834274291994, "val_acc": 0.82}, {"epoch": 24, "train_loss": 0.4646140027046204, "val_loss": 0.3595154356956482, "val_acc": 0.865}, {"epoch": 25, "train_loss": 0.44792795419692993, "val_loss": 0.3501656138896942, "val_acc": 0.845}], "final_metrics": {"epoch": 25, "train_loss": 0.44792795419692993, "val_loss": 0.3501656138896942, "val_acc": 0.845}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 36), nn.ReLU(), nn.Linear(36, 3))", "history": [{"epoch": 1, "train_loss": 0.9979492115974427, "val_loss": 0.8153562140464783, "val_acc": 0.675}, {"epoch": 2, "train_loss": 0.6378678059577942, "val_loss": 0.5811784148216248, "val_acc": 0.725}, {"epoch": 3, "train_loss": 0.4913734722137451, "val_loss": 0.4974441659450531, "val_acc": 0.81}, {"epoch": 4, "train_loss": 0.4464408326148987, "val_loss": 0.5315877020359039, "val_acc": 0.785}, {"epoch": 5, "train_loss": 0.4504900670051575, "val_loss": 0.6125648164749146, "val_acc": 0.745}, {"epoch": 6, "train_loss": 0.46323482632637025, "val_loss": 0.5226658797264099, "val_acc": 0.785}, {"epoch": 7, "train_loss": 0.42844408988952637, "val_loss": 0.522612853050232, "val_acc": 0.83}, {"epoch": 8, "train_loss": 0.4497501790523529, "val_loss": 0.5348682510852814, "val_acc": 0.785}, {"epoch": 9, "train_loss": 0.4471948778629303, "val_loss": 0.5328850209712982, "val_acc": 0.765}, {"epoch": 10, "train_loss": 0.42833448767662047, "val_loss": 0.5075570917129517, "val_acc": 0.825}, {"epoch": 11, "train_loss": 0.43085407733917236, "val_loss": 0.5635598731040955, "val_acc": 0.75}, {"epoch": 12, "train_loss": 0.418576580286026, "val_loss": 0.5279443097114563, "val_acc": 0.82}, {"epoch": 13, "train_loss": 0.4140996170043945, "val_loss": 0.5504993534088135, "val_acc": 0.775}, {"epoch": 14, "train_loss": 0.44243079781532285, "val_loss": 0.6207668209075927, "val_acc": 0.75}, {"epoch": 15, "train_loss": 0.45798959136009215, "val_loss": 0.6284111666679383, "val_acc": 0.725}, {"epoch": 16, "train_loss": 0.43953772068023683, "val_loss": 0.5668278551101684, "val_acc": 0.805}, {"epoch": 17, "train_loss": 0.4163303709030151, "val_loss": 0.49840619444847106, "val_acc": 0.81}, {"epoch": 18, "train_loss": 0.41188515901565553, "val_loss": 0.5307573223114014, "val_acc": 0.795}, {"epoch": 19, "train_loss": 0.40114330768585205, "val_loss": 0.5764698100090027, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.39878761768341064, "val_loss": 0.507626519203186, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.42388119697570803, "val_loss": 0.531118049621582, "val_acc": 0.775}, {"epoch": 22, "train_loss": 0.40146039247512816, "val_loss": 0.5995396161079407, "val_acc": 0.795}, {"epoch": 23, "train_loss": 0.4108853143453598, "val_loss": 0.49979966759681704, "val_acc": 0.795}, {"epoch": 24, "train_loss": 0.40326867818832396, "val_loss": 0.5912641263008118, "val_acc": 0.775}, {"epoch": 25, "train_loss": 0.40577601790428164, "val_loss": 0.5169864058494568, "val_acc": 0.81}], "final_metrics": {"epoch": 25, "train_loss": 0.40577601790428164, "val_loss": 0.5169864058494568, "val_acc": 0.81}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 80), nn.ReLU(), nn.Linear(80, 40), nn.ReLU(), nn.Linear(40, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)", "history": [{"epoch": 1, "train_loss": 1.0765558671951294, "val_loss": 1.0059922647476196, "val_acc": 0.43}, {"epoch": 2, "train_loss": 1.0044106602668763, "val_loss": 0.9503548264503479, "val_acc": 0.54}, {"epoch": 3, "train_loss": 0.9380770516395569, "val_loss": 0.8780529046058655, "val_acc": 0.635}, {"epoch": 4, "train_loss": 0.8631333255767822, "val_loss": 0.7964218091964722, "val_acc": 0.685}, {"epoch": 5, "train_loss": 0.7819881892204285, "val_loss": 0.7178042316436768, "val_acc": 0.735}, {"epoch": 6, "train_loss": 0.7034596514701843, "val_loss": 0.6448882389068603, "val_acc": 0.755}, {"epoch": 7, "train_loss": 0.6364943528175354, "val_loss": 0.5890285348892212, "val_acc": 0.79}, {"epoch": 8, "train_loss": 0.5882938385009766, "val_loss": 0.5463991045951844, "val_acc": 0.83}, {"epoch": 9, "train_loss": 0.5487142980098725, "val_loss": 0.5093592548370361, "val_acc": 0.815}, {"epoch": 10, "train_loss": 0.521161881685257, "val_loss": 0.4877327060699463, "val_acc": 0.845}, {"epoch": 11, "train_loss": 0.5001452207565308, "val_loss": 0.4672663569450378, "val_acc": 0.84}, {"epoch": 12, "train_loss": 0.48472745418548585, "val_loss": 0.45261771202087403, "val_acc": 0.835}, {"epoch": 13, "train_loss": 0.47400363683700564, "val_loss": 0.4412127685546875, "val_acc": 0.865}, {"epoch": 14, "train_loss": 0.46348042726516725, "val_loss": 0.43288636684417725, "val_acc": 0.83}, {"epoch": 15, "train_loss": 0.45968549966812133, "val_loss": 0.4271932888031006, "val_acc": 0.85}, {"epoch": 16, "train_loss": 0.45437562704086304, "val_loss": 0.425955171585083, "val_acc": 0.86}, {"epoch": 17, "train_loss": 0.4491789126396179, "val_loss": 0.4167302465438843, "val_acc": 0.84}, {"epoch": 18, "train_loss": 0.4434909760951996, "val_loss": 0.413555223941803, "val_acc": 0.86}, {"epoch": 19, "train_loss": 0.44223873734474184, "val_loss": 0.4114153647422791, "val_acc": 0.86}, {"epoch": 20, "train_loss": 0.4430671751499176, "val_loss": 0.4304996109008789, "val_acc": 0.825}, {"epoch": 21, "train_loss": 0.4583993363380432, "val_loss": 0.45885454177856444, "val_acc": 0.81}, {"epoch": 22, "train_loss": 0.4511582696437836, "val_loss": 0.41454927444458006, "val_acc": 0.84}, {"epoch": 23, "train_loss": 0.4429460060596466, "val_loss": 0.40525384664535524, "val_acc": 0.865}, {"epoch": 24, "train_loss": 0.4319571042060852, "val_loss": 0.4037656283378601, "val_acc": 0.865}, {"epoch": 25, "train_loss": 0.437227737903595, "val_loss": 0.40346537351608275, "val_acc": 0.85}], "final_metrics": {"epoch": 25, "train_loss": 0.437227737903595, "val_loss": 0.40346537351608275, "val_acc": 0.85}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 80), nn.ReLU(), nn.Linear(80, 36), nn.ReLU(), nn.Linear(36, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)", "history": [{"epoch": 1, "train_loss": 1.0438005590438844, "val_loss": 1.0294649124145507, "val_acc": 0.38}, {"epoch": 2, "train_loss": 1.0055925130844117, "val_loss": 1.0085884046554565, "val_acc": 0.385}, {"epoch": 3, "train_loss": 0.9789036774635315, "val_loss": 0.9846249866485596, "val_acc": 0.43}, {"epoch": 4, "train_loss": 0.9508915853500366, "val_loss": 0.961124815940857, "val_acc": 0.47}, {"epoch": 5, "train_loss": 0.9189963078498841, "val_loss": 0.9271582984924316, "val_acc": 0.525}, {"epoch": 6, "train_loss": 0.885154812335968, "val_loss": 0.8932843160629272, "val_acc": 0.565}, {"epoch": 7, "train_loss": 0.8490038347244263, "val_loss": 0.8580937051773071, "val_acc": 0.575}, {"epoch": 8, "train_loss": 0.8109800291061401, "val_loss": 0.8165991640090943, "val_acc": 0.645}, {"epoch": 9, "train_loss": 0.7755579257011413, "val_loss": 0.7828440546989441, "val_acc": 0.68}, {"epoch": 10, "train_loss": 0.7409285306930542, "val_loss": 0.752759051322937, "val_acc": 0.655}, {"epoch": 11, "train_loss": 0.7056369543075561, "val_loss": 0.7184064865112305, "val_acc": 0.72}, {"epoch": 12, "train_loss": 0.6772287082672119, "val_loss": 0.6880204486846924, "val_acc": 0.755}, {"epoch": 13, "train_loss": 0.648954405784607, "val_loss": 0.6690703749656677, "val_acc": 0.74}, {"epoch": 14, "train_loss": 0.6257246494293213, "val_loss": 0.6404221796989441, "val_acc": 0.785}, {"epoch": 15, "train_loss": 0.6013776516914368, "val_loss": 0.6206736707687378, "val_acc": 0.77}, {"epoch": 16, "train_loss": 0.58154287815094, "val_loss": 0.6064262962341309, "val_acc": 0.765}, {"epoch": 17, "train_loss": 0.5641917276382447, "val_loss": 0.584887056350708, "val_acc": 0.785}, {"epoch": 18, "train_loss": 0.5474191427230835, "val_loss": 0.5703072762489318, "val_acc": 0.795}, {"epoch": 19, "train_loss": 0.5331622815132141, "val_loss": 0.5577413320541382, "val_acc": 0.795}, {"epoch": 20, "train_loss": 0.5199651408195496, "val_loss": 0.5445384550094604, "val_acc": 0.8}, {"epoch": 21, "train_loss": 0.5085354328155518, "val_loss": 0.5372823119163513, "val_acc": 0.795}, {"epoch": 22, "train_loss": 0.4989055097103119, "val_loss": 0.5272831869125366, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.4885484075546265, "val_loss": 0.5179570579528808, "val_acc": 0.805}, {"epoch": 24, "train_loss": 0.48126389503479006, "val_loss": 0.5134204292297363, "val_acc": 0.795}, {"epoch": 25, "train_loss": 0.47482760667800905, "val_loss": 0.5051135563850403, "val_acc": 0.795}], "final_metrics": {"epoch": 25, "train_loss": 0.47482760667800905, "val_loss": 0.5051135563850403, "val_acc": 0.795}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 72), nn.ReLU(), nn.Linear(72, 36), nn.ReLU(), nn.Linear(36, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)", "history": [{"epoch": 1, "train_loss": 1.062393536567688, "val_loss": 1.0478929710388183, "val_acc": 0.365}, {"epoch": 2, "train_loss": 0.9948228812217712, "val_loss": 1.0056805419921875, "val_acc": 0.38}, {"epoch": 3, "train_loss": 0.9571702790260315, "val_loss": 0.9664533567428589, "val_acc": 0.44}, {"epoch": 4, "train_loss": 0.9144830775260925, "val_loss": 0.9188230228424072, "val_acc": 0.495}, {"epoch": 5, "train_loss": 0.8632630491256714, "val_loss": 0.8611530590057374, "val_acc": 0.56}, {"epoch": 6, "train_loss": 0.803255386352539, "val_loss": 0.7990764999389648, "val_acc": 0.605}, {"epoch": 7, "train_loss": 0.7418636822700501, "val_loss": 0.7330247163772583, "val_acc": 0.665}, {"epoch": 8, "train_loss": 0.6848647809028625, "val_loss": 0.6735874223709106, "val_acc": 0.72}, {"epoch": 9, "train_loss": 0.6335124611854553, "val_loss": 0.6207397437095642, "val_acc": 0.76}, {"epoch": 10, "train_loss": 0.5916448593139648, "val_loss": 0.5851589965820313, "val_acc": 0.755}, {"epoch": 11, "train_loss": 0.5575744175910949, "val_loss": 0.5426178908348084, "val_acc": 0.805}, {"epoch": 12, "train_loss": 0.5314720964431763, "val_loss": 0.5122013711929321, "val_acc": 0.825}, {"epoch": 13, "train_loss": 0.5082479548454285, "val_loss": 0.5008996534347534, "val_acc": 0.81}, {"epoch": 14, "train_loss": 0.4930655646324158, "val_loss": 0.47585229635238646, "val_acc": 0.815}, {"epoch": 15, "train_loss": 0.47730843544006346, "val_loss": 0.48797629833221434, "val_acc": 0.785}, {"epoch": 16, "train_loss": 0.47256574749946595, "val_loss": 0.4572697925567627, "val_acc": 0.825}, {"epoch": 17, "train_loss": 0.46626594305038455, "val_loss": 0.45077523708343503, "val_acc": 0.83}, {"epoch": 18, "train_loss": 0.457293484210968, "val_loss": 0.45060070991516116, "val_acc": 0.81}, {"epoch": 19, "train_loss": 0.4548076367378235, "val_loss": 0.4560770320892334, "val_acc": 0.8}, {"epoch": 20, "train_loss": 0.44798476338386534, "val_loss": 0.43641924142837524, "val_acc": 0.825}, {"epoch": 21, "train_loss": 0.44242801904678347, "val_loss": 0.4371703052520752, "val_acc": 0.83}, {"epoch": 22, "train_loss": 0.44123226165771484, "val_loss": 0.4271639108657837, "val_acc": 0.815}, {"epoch": 23, "train_loss": 0.4377032971382141, "val_loss": 0.4382672429084778, "val_acc": 0.83}, {"epoch": 24, "train_loss": 0.43649051547050477, "val_loss": 0.4266464614868164, "val_acc": 0.82}, {"epoch": 25, "train_loss": 0.43380473494529725, "val_loss": 0.4210402655601502, "val_acc": 0.83}], "final_metrics": {"epoch": 25, "train_loss": 0.43380473494529725, "val_loss": 0.4210402655601502, "val_acc": 0.83}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 96), nn.ReLU(), nn.Linear(96, 48), nn.ReLU(), nn.Linear(48, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\nself.training_epochs = 100", "history": [{"epoch": 1, "train_loss": 1.0319561243057251, "val_loss": 1.0138249158859254, "val_acc": 0.36}, {"epoch": 2, "train_loss": 0.9906067752838135, "val_loss": 0.9799142837524414, "val_acc": 0.375}, {"epoch": 3, "train_loss": 0.9594621706008911, "val_loss": 0.945289466381073, "val_acc": 0.5}, {"epoch": 4, "train_loss": 0.9252884864807129, "val_loss": 0.9082825636863708, "val_acc": 0.585}, {"epoch": 5, "train_loss": 0.8884915161132813, "val_loss": 0.8617584300041199, "val_acc": 0.655}, {"epoch": 6, "train_loss": 0.844860486984253, "val_loss": 0.8129772543907166, "val_acc": 0.7}, {"epoch": 7, "train_loss": 0.8017785882949829, "val_loss": 0.7645641136169433, "val_acc": 0.735}, {"epoch": 8, "train_loss": 0.759719831943512, "val_loss": 0.7246300315856934, "val_acc": 0.76}, {"epoch": 9, "train_loss": 0.7169208979606628, "val_loss": 0.6741317725181579, "val_acc": 0.785}, {"epoch": 10, "train_loss": 0.6775595188140869, "val_loss": 0.6360636758804321, "val_acc": 0.8}, {"epoch": 11, "train_loss": 0.643191864490509, "val_loss": 0.604211698770523, "val_acc": 0.81}, {"epoch": 12, "train_loss": 0.6124561142921447, "val_loss": 0.5732138097286225, "val_acc": 0.83}, {"epoch": 13, "train_loss": 0.5858426260948181, "val_loss": 0.5475680553913116, "val_acc": 0.835}, {"epoch": 14, "train_loss": 0.5653048920631408, "val_loss": 0.5247560763359069, "val_acc": 0.835}, {"epoch": 15, "train_loss": 0.5456440901756286, "val_loss": 0.5102836430072785, "val_acc": 0.83}, {"epoch": 16, "train_loss": 0.529451289176941, "val_loss": 0.4912598550319672, "val_acc": 0.845}, {"epoch": 17, "train_loss": 0.518208634853363, "val_loss": 0.4800461483001709, "val_acc": 0.84}, {"epoch": 18, "train_loss": 0.5046341347694397, "val_loss": 0.46960596442222596, "val_acc": 0.83}, {"epoch": 19, "train_loss": 0.4943617534637451, "val_loss": 0.4586793041229248, "val_acc": 0.84}, {"epoch": 20, "train_loss": 0.48489543199539187, "val_loss": 0.4489930748939514, "val_acc": 0.83}, {"epoch": 21, "train_loss": 0.4820341968536377, "val_loss": 0.4493670642375946, "val_acc": 0.835}, {"epoch": 22, "train_loss": 0.47647158861160277, "val_loss": 0.4339857530593872, "val_acc": 0.86}, {"epoch": 23, "train_loss": 0.46710374593734744, "val_loss": 0.43683096408843997, "val_acc": 0.835}, {"epoch": 24, "train_loss": 0.46241175174713134, "val_loss": 0.4265419238805771, "val_acc": 0.845}, {"epoch": 25, "train_loss": 0.45877357840538024, "val_loss": 0.4214173525571823, "val_acc": 0.845}, {"epoch": 26, "train_loss": 0.45559764981269835, "val_loss": 0.4292959213256836, "val_acc": 0.835}, {"epoch": 27, "train_loss": 0.45260956764221194, "val_loss": 0.4198136681318283, "val_acc": 0.835}, {"epoch": 28, "train_loss": 0.4517574667930603, "val_loss": 0.4158206611871719, "val_acc": 0.845}, {"epoch": 29, "train_loss": 0.44889635443687437, "val_loss": 0.4160994720458984, "val_acc": 0.845}, {"epoch": 30, "train_loss": 0.4450040471553802, "val_loss": 0.4085003399848938, "val_acc": 0.85}, {"epoch": 31, "train_loss": 0.4425819873809814, "val_loss": 0.418591525554657, "val_acc": 0.845}, {"epoch": 32, "train_loss": 0.44105586528778074, "val_loss": 0.41005520164966586, "val_acc": 0.85}, {"epoch": 33, "train_loss": 0.4407601141929626, "val_loss": 0.4107343256473541, "val_acc": 0.845}, {"epoch": 34, "train_loss": 0.43794735074043273, "val_loss": 0.40459534525871277, "val_acc": 0.845}, {"epoch": 35, "train_loss": 0.4367329633235931, "val_loss": 0.41092418015003207, "val_acc": 0.845}, {"epoch": 36, "train_loss": 0.434916090965271, "val_loss": 0.40715066730976107, "val_acc": 0.84}, {"epoch": 37, "train_loss": 0.43313016831874845, "val_loss": 0.4022649216651917, "val_acc": 0.865}, {"epoch": 38, "train_loss": 0.4341498410701752, "val_loss": 0.4081120103597641, "val_acc": 0.845}, {"epoch": 39, "train_loss": 0.4307923436164856, "val_loss": 0.4031477212905884, "val_acc": 0.85}, {"epoch": 40, "train_loss": 0.43066112279891966, "val_loss": 0.403596031665802, "val_acc": 0.855}, {"epoch": 41, "train_loss": 0.42998466968536375, "val_loss": 0.40775288045406344, "val_acc": 0.845}, {"epoch": 42, "train_loss": 0.4304485070705414, "val_loss": 0.4004924386739731, "val_acc": 0.855}, {"epoch": 43, "train_loss": 0.4293177938461304, "val_loss": 0.4024375337362289, "val_acc": 0.845}, {"epoch": 44, "train_loss": 0.4286016142368317, "val_loss": 0.40338676035404203, "val_acc": 0.85}, {"epoch": 45, "train_loss": 0.42719562292099, "val_loss": 0.4061324280500412, "val_acc": 0.855}, {"epoch": 46, "train_loss": 0.429239866733551, "val_loss": 0.4012430268526077, "val_acc": 0.86}, {"epoch": 47, "train_loss": 0.42860532164573667, "val_loss": 0.41066294312477114, "val_acc": 0.845}, {"epoch": 48, "train_loss": 0.42668367505073546, "val_loss": 0.40458351612091065, "val_acc": 0.85}, {"epoch": 49, "train_loss": 0.42446401715278625, "val_loss": 0.40068900644779204, "val_acc": 0.87}, {"epoch": 50, "train_loss": 0.42774189710617067, "val_loss": 0.4049648106098175, "val_acc": 0.85}, {"epoch": 51, "train_loss": 0.4260916447639465, "val_loss": 0.4037727344036102, "val_acc": 0.855}, {"epoch": 52, "train_loss": 0.42633685111999514, "val_loss": 0.40612083554267886, "val_acc": 0.845}, {"epoch": 53, "train_loss": 0.42485095500946046, "val_loss": 0.4007147806882858, "val_acc": 0.84}, {"epoch": 54, "train_loss": 0.42543395042419435, "val_loss": 0.40636380910873415, "val_acc": 0.835}, {"epoch": 55, "train_loss": 0.4260089123249054, "val_loss": 0.41058544874191283, "val_acc": 0.855}, {"epoch": 56, "train_loss": 0.42501619577407834, "val_loss": 0.3995422786474228, "val_acc": 0.84}, {"epoch": 57, "train_loss": 0.42270841002464293, "val_loss": 0.4034698563814163, "val_acc": 0.86}, {"epoch": 58, "train_loss": 0.4220821213722229, "val_loss": 0.40210228621959687, "val_acc": 0.86}, {"epoch": 59, "train_loss": 0.4278917956352234, "val_loss": 0.4043920201063156, "val_acc": 0.845}, {"epoch": 60, "train_loss": 0.4248733866214752, "val_loss": 0.4028216028213501, "val_acc": 0.85}, {"epoch": 61, "train_loss": 0.42523149490356443, "val_loss": 0.4015629422664642, "val_acc": 0.87}, {"epoch": 62, "train_loss": 0.4217054724693298, "val_loss": 0.4008678954839706, "val_acc": 0.85}, {"epoch": 63, "train_loss": 0.4221062672138214, "val_loss": 0.40387266099452973, "val_acc": 0.865}, {"epoch": 64, "train_loss": 0.4210393142700195, "val_loss": 0.407860170006752, "val_acc": 0.85}, {"epoch": 65, "train_loss": 0.42173179864883426, "val_loss": 0.40307464957237243, "val_acc": 0.85}, {"epoch": 66, "train_loss": 0.42123395442962647, "val_loss": 0.40909774124622345, "val_acc": 0.85}, {"epoch": 67, "train_loss": 0.42244859218597414, "val_loss": 0.4109238994121551, "val_acc": 0.835}, {"epoch": 68, "train_loss": 0.4212895131111145, "val_loss": 0.40156277537345886, "val_acc": 0.85}, {"epoch": 69, "train_loss": 0.4211867606639862, "val_loss": 0.4012387156486511, "val_acc": 0.85}, {"epoch": 70, "train_loss": 0.4211975634098053, "val_loss": 0.40159685134887696, "val_acc": 0.855}, {"epoch": 71, "train_loss": 0.42168057203292847, "val_loss": 0.4088381630182266, "val_acc": 0.85}, {"epoch": 72, "train_loss": 0.4197359263896942, "val_loss": 0.40211722254753113, "val_acc": 0.855}, {"epoch": 73, "train_loss": 0.4189593946933746, "val_loss": 0.40541421353816987, "val_acc": 0.84}, {"epoch": 74, "train_loss": 0.41966782093048094, "val_loss": 0.40659475803375245, "val_acc": 0.855}, {"epoch": 75, "train_loss": 0.4190409803390503, "val_loss": 0.40710447549819945, "val_acc": 0.86}, {"epoch": 76, "train_loss": 0.42136038780212404, "val_loss": 0.4008593863248825, "val_acc": 0.855}, {"epoch": 77, "train_loss": 0.42224673509597777, "val_loss": 0.408659462928772, "val_acc": 0.855}, {"epoch": 78, "train_loss": 0.42183650732040406, "val_loss": 0.3993508392572403, "val_acc": 0.87}, {"epoch": 79, "train_loss": 0.4199815475940704, "val_loss": 0.41342208623886106, "val_acc": 0.84}, {"epoch": 80, "train_loss": 0.42089111685752867, "val_loss": 0.40060950100421905, "val_acc": 0.86}, {"epoch": 81, "train_loss": 0.4197251009941101, "val_loss": 0.4076483678817749, "val_acc": 0.845}, {"epoch": 82, "train_loss": 0.41713897526264193, "val_loss": 0.40658224761486056, "val_acc": 0.855}, {"epoch": 83, "train_loss": 0.41936116218566893, "val_loss": 0.40211338579654693, "val_acc": 0.865}, {"epoch": 84, "train_loss": 0.4184699320793152, "val_loss": 0.4075039458274841, "val_acc": 0.86}, {"epoch": 85, "train_loss": 0.4177690076828003, "val_loss": 0.4056009465456009, "val_acc": 0.85}, {"epoch": 86, "train_loss": 0.41817561149597166, "val_loss": 0.4075148808956146, "val_acc": 0.855}, {"epoch": 87, "train_loss": 0.41940890908241274, "val_loss": 0.4073800104856491, "val_acc": 0.845}, {"epoch": 88, "train_loss": 0.4209182453155518, "val_loss": 0.4032338607311249, "val_acc": 0.865}, {"epoch": 89, "train_loss": 0.41830753445625307, "val_loss": 0.40446597039699556, "val_acc": 0.85}, {"epoch": 90, "train_loss": 0.4172596788406372, "val_loss": 0.4115888297557831, "val_acc": 0.855}, {"epoch": 91, "train_loss": 0.4166842317581177, "val_loss": 0.4018926656246185, "val_acc": 0.85}, {"epoch": 92, "train_loss": 0.41725308895111085, "val_loss": 0.4044822263717651, "val_acc": 0.865}, {"epoch": 93, "train_loss": 0.4202509844303131, "val_loss": 0.40876414835453034, "val_acc": 0.87}, {"epoch": 94, "train_loss": 0.4199858856201172, "val_loss": 0.41716247200965884, "val_acc": 0.835}, {"epoch": 95, "train_loss": 0.415551929473877, "val_loss": 0.4012631785869598, "val_acc": 0.855}, {"epoch": 96, "train_loss": 0.4162851440906525, "val_loss": 0.40050120890140534, "val_acc": 0.855}, {"epoch": 97, "train_loss": 0.4181710147857666, "val_loss": 0.4083098113536835, "val_acc": 0.855}, {"epoch": 98, "train_loss": 0.4207123601436615, "val_loss": 0.4109478485584259, "val_acc": 0.845}, {"epoch": 99, "train_loss": 0.4157202434539795, "val_loss": 0.40523714900016783, "val_acc": 0.875}, {"epoch": 100, "train_loss": 0.4160871767997742, "val_loss": 0.4096068638563156, "val_acc": 0.845}], "final_metrics": {"epoch": 100, "train_loss": 0.4160871767997742, "val_loss": 0.4096068638563156, "val_acc": 0.845}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0003)\nself.training_epochs = 150", "history": [{"epoch": 1, "train_loss": 1.0986077165603638, "val_loss": 1.044871325492859, "val_acc": 0.535}, {"epoch": 2, "train_loss": 1.0175113487243652, "val_loss": 1.0054742431640624, "val_acc": 0.425}, {"epoch": 3, "train_loss": 0.9806482458114624, "val_loss": 0.9829743814468384, "val_acc": 0.43}, {"epoch": 4, "train_loss": 0.9548699283599853, "val_loss": 0.9589884924888611, "val_acc": 0.51}, {"epoch": 5, "train_loss": 0.9285247921943665, "val_loss": 0.9309466814994812, "val_acc": 0.56}, {"epoch": 6, "train_loss": 0.899099452495575, "val_loss": 0.9029026007652283, "val_acc": 0.575}, {"epoch": 7, "train_loss": 0.8702902340888977, "val_loss": 0.8718494319915772, "val_acc": 0.605}, {"epoch": 8, "train_loss": 0.8401532459259033, "val_loss": 0.840448145866394, "val_acc": 0.66}, {"epoch": 9, "train_loss": 0.8088535523414612, "val_loss": 0.8108210515975952, "val_acc": 0.665}, {"epoch": 10, "train_loss": 0.7780022168159485, "val_loss": 0.7793210005760193, "val_acc": 0.725}, {"epoch": 11, "train_loss": 0.7458614802360535, "val_loss": 0.7475914454460144, "val_acc": 0.73}, {"epoch": 12, "train_loss": 0.7172442507743836, "val_loss": 0.7160392832756043, "val_acc": 0.735}, {"epoch": 13, "train_loss": 0.6867597007751465, "val_loss": 0.6894955372810364, "val_acc": 0.78}, {"epoch": 14, "train_loss": 0.6596914958953858, "val_loss": 0.6650893950462341, "val_acc": 0.76}, {"epoch": 15, "train_loss": 0.6345571827888489, "val_loss": 0.639151132106781, "val_acc": 0.79}, {"epoch": 16, "train_loss": 0.6147655272483825, "val_loss": 0.6204077506065369, "val_acc": 0.81}, {"epoch": 17, "train_loss": 0.5919959998130798, "val_loss": 0.6022270512580872, "val_acc": 0.79}, {"epoch": 18, "train_loss": 0.5725746250152588, "val_loss": 0.5858044242858886, "val_acc": 0.815}, {"epoch": 19, "train_loss": 0.5572301077842713, "val_loss": 0.5706143283843994, "val_acc": 0.815}, {"epoch": 20, "train_loss": 0.5418313515186309, "val_loss": 0.5597568726539612, "val_acc": 0.805}, {"epoch": 21, "train_loss": 0.5289731931686401, "val_loss": 0.5485700845718384, "val_acc": 0.81}, {"epoch": 22, "train_loss": 0.5176691269874573, "val_loss": 0.5370365858078003, "val_acc": 0.81}, {"epoch": 23, "train_loss": 0.5084001994132996, "val_loss": 0.5279134702682495, "val_acc": 0.8}, {"epoch": 24, "train_loss": 0.5013503110408783, "val_loss": 0.5239774227142334, "val_acc": 0.8}, {"epoch": 25, "train_loss": 0.49016090273857116, "val_loss": 0.5151221060752869, "val_acc": 0.795}, {"epoch": 26, "train_loss": 0.48073199987411497, "val_loss": 0.5114755415916443, "val_acc": 0.795}, {"epoch": 27, "train_loss": 0.47530193328857423, "val_loss": 0.5068527460098267, "val_acc": 0.795}, {"epoch": 28, "train_loss": 0.46653807878494263, "val_loss": 0.5036252260208129, "val_acc": 0.785}, {"epoch": 29, "train_loss": 0.4620796084403992, "val_loss": 0.4977580189704895, "val_acc": 0.785}, {"epoch": 30, "train_loss": 0.45731926083564756, "val_loss": 0.49386566638946533, "val_acc": 0.8}, {"epoch": 31, "train_loss": 0.45202529907226563, "val_loss": 0.4946816039085388, "val_acc": 0.78}, {"epoch": 32, "train_loss": 0.4478937029838562, "val_loss": 0.4882796692848206, "val_acc": 0.775}, {"epoch": 33, "train_loss": 0.44468515157699584, "val_loss": 0.4905742943286896, "val_acc": 0.785}, {"epoch": 34, "train_loss": 0.4392272996902466, "val_loss": 0.48485124111175537, "val_acc": 0.8}, {"epoch": 35, "train_loss": 0.4381916391849518, "val_loss": 0.484577910900116, "val_acc": 0.78}, {"epoch": 36, "train_loss": 0.4368313157558441, "val_loss": 0.4854594659805298, "val_acc": 0.79}, {"epoch": 37, "train_loss": 0.4343619704246521, "val_loss": 0.4806157398223877, "val_acc": 0.785}, {"epoch": 38, "train_loss": 0.4299095058441162, "val_loss": 0.4853218054771423, "val_acc": 0.775}, {"epoch": 39, "train_loss": 0.42669034481048584, "val_loss": 0.47766961336135866, "val_acc": 0.8}, {"epoch": 40, "train_loss": 0.4250720834732056, "val_loss": 0.4805058717727661, "val_acc": 0.77}, {"epoch": 41, "train_loss": 0.4221489667892456, "val_loss": 0.4815141534805298, "val_acc": 0.79}, {"epoch": 42, "train_loss": 0.4223475003242493, "val_loss": 0.4814504885673523, "val_acc": 0.785}, {"epoch": 43, "train_loss": 0.41890008926391603, "val_loss": 0.4826102066040039, "val_acc": 0.795}, {"epoch": 44, "train_loss": 0.41931363582611086, "val_loss": 0.47988950729370117, "val_acc": 0.79}, {"epoch": 45, "train_loss": 0.41629043221473694, "val_loss": 0.4818358778953552, "val_acc": 0.79}, {"epoch": 46, "train_loss": 0.4164836668968201, "val_loss": 0.4829516315460205, "val_acc": 0.785}, {"epoch": 47, "train_loss": 0.4149432384967804, "val_loss": 0.47902469635009765, "val_acc": 0.775}, {"epoch": 48, "train_loss": 0.41407173216342924, "val_loss": 0.48142573595046995, "val_acc": 0.795}, {"epoch": 49, "train_loss": 0.41235244512557984, "val_loss": 0.48139856338500975, "val_acc": 0.78}, {"epoch": 50, "train_loss": 0.4110798919200897, "val_loss": 0.484100775718689, "val_acc": 0.78}, {"epoch": 51, "train_loss": 0.4105114102363586, "val_loss": 0.48634924173355104, "val_acc": 0.79}, {"epoch": 52, "train_loss": 0.41011601567268374, "val_loss": 0.48085191249847414, "val_acc": 0.785}, {"epoch": 53, "train_loss": 0.40832485556602477, "val_loss": 0.48128525018692014, "val_acc": 0.79}, {"epoch": 54, "train_loss": 0.4094429862499237, "val_loss": 0.4822966480255127, "val_acc": 0.795}, {"epoch": 55, "train_loss": 0.409007203578949, "val_loss": 0.4848321318626404, "val_acc": 0.79}, {"epoch": 56, "train_loss": 0.4065357482433319, "val_loss": 0.4857937693595886, "val_acc": 0.79}, {"epoch": 57, "train_loss": 0.4055742144584656, "val_loss": 0.4841343903541565, "val_acc": 0.785}, {"epoch": 58, "train_loss": 0.40465993881225587, "val_loss": 0.4846239185333252, "val_acc": 0.79}, {"epoch": 59, "train_loss": 0.40471540451049803, "val_loss": 0.48382590532302855, "val_acc": 0.785}, {"epoch": 60, "train_loss": 0.40706878662109375, "val_loss": 0.4860524010658264, "val_acc": 0.795}, {"epoch": 61, "train_loss": 0.4098097896575928, "val_loss": 0.48497132301330564, "val_acc": 0.795}, {"epoch": 62, "train_loss": 0.4042047500610352, "val_loss": 0.48541096210479734, "val_acc": 0.775}, {"epoch": 63, "train_loss": 0.4051977550983429, "val_loss": 0.49185886144638064, "val_acc": 0.79}, {"epoch": 64, "train_loss": 0.40514185667037966, "val_loss": 0.48868597745895387, "val_acc": 0.795}, {"epoch": 65, "train_loss": 0.40387666821479795, "val_loss": 0.48709323406219485, "val_acc": 0.785}, {"epoch": 66, "train_loss": 0.4026605331897736, "val_loss": 0.4884264636039734, "val_acc": 0.785}, {"epoch": 67, "train_loss": 0.40276912212371824, "val_loss": 0.48824055790901183, "val_acc": 0.8}, {"epoch": 68, "train_loss": 0.4011110174655914, "val_loss": 0.48521926641464236, "val_acc": 0.79}, {"epoch": 69, "train_loss": 0.39963545322418215, "val_loss": 0.48551366329193113, "val_acc": 0.785}, {"epoch": 70, "train_loss": 0.4005608427524567, "val_loss": 0.4890788149833679, "val_acc": 0.795}, {"epoch": 71, "train_loss": 0.39946626901626586, "val_loss": 0.4893937087059021, "val_acc": 0.78}, {"epoch": 72, "train_loss": 0.3990755641460419, "val_loss": 0.4878263974189758, "val_acc": 0.79}, {"epoch": 73, "train_loss": 0.3993453669548035, "val_loss": 0.48996298789978027, "val_acc": 0.79}, {"epoch": 74, "train_loss": 0.40153430700302123, "val_loss": 0.4910195255279541, "val_acc": 0.795}, {"epoch": 75, "train_loss": 0.4002257788181305, "val_loss": 0.4885628390312195, "val_acc": 0.795}, {"epoch": 76, "train_loss": 0.39779184460639955, "val_loss": 0.48895410537719725, "val_acc": 0.79}, {"epoch": 77, "train_loss": 0.39896854043006896, "val_loss": 0.4939969825744629, "val_acc": 0.785}, {"epoch": 78, "train_loss": 0.3988002574443817, "val_loss": 0.49060198307037356, "val_acc": 0.79}, {"epoch": 79, "train_loss": 0.39758585929870605, "val_loss": 0.4903415203094482, "val_acc": 0.795}, {"epoch": 80, "train_loss": 0.39642340898513795, "val_loss": 0.49438395857810974, "val_acc": 0.79}, {"epoch": 81, "train_loss": 0.39642976999282836, "val_loss": 0.4938041067123413, "val_acc": 0.79}, {"epoch": 82, "train_loss": 0.3978517240285873, "val_loss": 0.48837953090667724, "val_acc": 0.79}, {"epoch": 83, "train_loss": 0.3986591553688049, "val_loss": 0.4904441833496094, "val_acc": 0.79}, {"epoch": 84, "train_loss": 0.39571476459503174, "val_loss": 0.491941921710968, "val_acc": 0.795}, {"epoch": 85, "train_loss": 0.3952385699748993, "val_loss": 0.4942370343208313, "val_acc": 0.795}, {"epoch": 86, "train_loss": 0.39598562955856326, "val_loss": 0.4937477684020996, "val_acc": 0.79}, {"epoch": 87, "train_loss": 0.39975985646247864, "val_loss": 0.4926149415969849, "val_acc": 0.79}, {"epoch": 88, "train_loss": 0.3947114151716232, "val_loss": 0.49330466985702515, "val_acc": 0.79}, {"epoch": 89, "train_loss": 0.39638448238372803, "val_loss": 0.4944334554672241, "val_acc": 0.79}, {"epoch": 90, "train_loss": 0.3955266726016998, "val_loss": 0.4933211302757263, "val_acc": 0.795}, {"epoch": 91, "train_loss": 0.3953338158130646, "val_loss": 0.49456592798233034, "val_acc": 0.8}, {"epoch": 92, "train_loss": 0.39378347039222716, "val_loss": 0.49534889698028567, "val_acc": 0.785}, {"epoch": 93, "train_loss": 0.3979406690597534, "val_loss": 0.4956754159927368, "val_acc": 0.79}, {"epoch": 94, "train_loss": 0.3966605269908905, "val_loss": 0.4922584843635559, "val_acc": 0.79}, {"epoch": 95, "train_loss": 0.3949671429395676, "val_loss": 0.4926376008987427, "val_acc": 0.78}, {"epoch": 96, "train_loss": 0.39414175629615783, "val_loss": 0.49444648027420046, "val_acc": 0.79}, {"epoch": 97, "train_loss": 0.3962846255302429, "val_loss": 0.4943032193183899, "val_acc": 0.795}, {"epoch": 98, "train_loss": 0.3959320271015167, "val_loss": 0.49886155366897583, "val_acc": 0.805}, {"epoch": 99, "train_loss": 0.39185410022735595, "val_loss": 0.4959088706970215, "val_acc": 0.785}, {"epoch": 100, "train_loss": 0.39386609196662903, "val_loss": 0.49518814802169797, "val_acc": 0.795}, {"epoch": 101, "train_loss": 0.39654345631599425, "val_loss": 0.49509048223495483, "val_acc": 0.785}, {"epoch": 102, "train_loss": 0.39795856654644013, "val_loss": 0.4980288910865784, "val_acc": 0.8}, {"epoch": 103, "train_loss": 0.392687908411026, "val_loss": 0.4931144142150879, "val_acc": 0.795}, {"epoch": 104, "train_loss": 0.39345641136169435, "val_loss": 0.49785452127456664, "val_acc": 0.795}, {"epoch": 105, "train_loss": 0.3914334124326706, "val_loss": 0.49820933818817137, "val_acc": 0.795}, {"epoch": 106, "train_loss": 0.39221266865730287, "val_loss": 0.49501057624816897, "val_acc": 0.8}, {"epoch": 107, "train_loss": 0.39273531913757326, "val_loss": 0.4940523862838745, "val_acc": 0.79}, {"epoch": 108, "train_loss": 0.39329087615013125, "val_loss": 0.4951260757446289, "val_acc": 0.8}, {"epoch": 109, "train_loss": 0.3925156140327454, "val_loss": 0.4971908164024353, "val_acc": 0.8}, {"epoch": 110, "train_loss": 0.391120982170105, "val_loss": 0.49672738075256345, "val_acc": 0.795}, {"epoch": 111, "train_loss": 0.3914961576461792, "val_loss": 0.49631778478622435, "val_acc": 0.79}, {"epoch": 112, "train_loss": 0.39217079877853395, "val_loss": 0.49984007835388183, "val_acc": 0.785}, {"epoch": 113, "train_loss": 0.39176278829574585, "val_loss": 0.49873091220855714, "val_acc": 0.795}, {"epoch": 114, "train_loss": 0.3903854191303253, "val_loss": 0.4939627695083618, "val_acc": 0.795}, {"epoch": 115, "train_loss": 0.39058592200279235, "val_loss": 0.4989144730567932, "val_acc": 0.795}, {"epoch": 116, "train_loss": 0.39305347084999087, "val_loss": 0.49806380152702334, "val_acc": 0.8}, {"epoch": 117, "train_loss": 0.39145947694778443, "val_loss": 0.5003158068656921, "val_acc": 0.795}, {"epoch": 118, "train_loss": 0.3905174279212952, "val_loss": 0.4977632308006287, "val_acc": 0.795}, {"epoch": 119, "train_loss": 0.39250032901763915, "val_loss": 0.4969581937789917, "val_acc": 0.795}, {"epoch": 120, "train_loss": 0.39279696583747864, "val_loss": 0.4967759466171265, "val_acc": 0.795}, {"epoch": 121, "train_loss": 0.3892299473285675, "val_loss": 0.49744966745376584, "val_acc": 0.79}, {"epoch": 122, "train_loss": 0.39135300159454345, "val_loss": 0.4974040365219116, "val_acc": 0.795}, {"epoch": 123, "train_loss": 0.3896801769733429, "val_loss": 0.4992922067642212, "val_acc": 0.785}, {"epoch": 124, "train_loss": 0.39127564907073975, "val_loss": 0.4942412805557251, "val_acc": 0.795}, {"epoch": 125, "train_loss": 0.3917898797988892, "val_loss": 0.5031727433204651, "val_acc": 0.79}, {"epoch": 126, "train_loss": 0.38907137751579285, "val_loss": 0.49672779083251956, "val_acc": 0.795}, {"epoch": 127, "train_loss": 0.38986397445201876, "val_loss": 0.49772297620773315, "val_acc": 0.795}, {"epoch": 128, "train_loss": 0.388914874792099, "val_loss": 0.49661033630371093, "val_acc": 0.795}, {"epoch": 129, "train_loss": 0.38918296456336976, "val_loss": 0.4980087184906006, "val_acc": 0.795}, {"epoch": 130, "train_loss": 0.3886602908372879, "val_loss": 0.503338315486908, "val_acc": 0.78}, {"epoch": 131, "train_loss": 0.3886307668685913, "val_loss": 0.49770636796951295, "val_acc": 0.795}, {"epoch": 132, "train_loss": 0.39124581336975095, "val_loss": 0.5026524996757508, "val_acc": 0.795}, {"epoch": 133, "train_loss": 0.3905586576461792, "val_loss": 0.4989689016342163, "val_acc": 0.79}, {"epoch": 134, "train_loss": 0.38720239281654356, "val_loss": 0.5015652108192444, "val_acc": 0.795}, {"epoch": 135, "train_loss": 0.38912246584892274, "val_loss": 0.5018989586830139, "val_acc": 0.79}, {"epoch": 136, "train_loss": 0.3877319324016571, "val_loss": 0.5003660655021668, "val_acc": 0.79}, {"epoch": 137, "train_loss": 0.388101704120636, "val_loss": 0.49695258378982543, "val_acc": 0.79}, {"epoch": 138, "train_loss": 0.38792735934257505, "val_loss": 0.4976435911655426, "val_acc": 0.8}, {"epoch": 139, "train_loss": 0.38917989253997803, "val_loss": 0.4986227250099182, "val_acc": 0.795}, {"epoch": 140, "train_loss": 0.38827039241790773, "val_loss": 0.5009181749820709, "val_acc": 0.8}, {"epoch": 141, "train_loss": 0.3879183101654053, "val_loss": 0.5004443359375, "val_acc": 0.79}, {"epoch": 142, "train_loss": 0.3889973258972168, "val_loss": 0.4996447992324829, "val_acc": 0.79}, {"epoch": 143, "train_loss": 0.38745670557022094, "val_loss": 0.5019338798522949, "val_acc": 0.8}, {"epoch": 144, "train_loss": 0.3877867674827576, "val_loss": 0.4994721555709839, "val_acc": 0.79}, {"epoch": 145, "train_loss": 0.3878095042705536, "val_loss": 0.506639416217804, "val_acc": 0.795}, {"epoch": 146, "train_loss": 0.38710691809654235, "val_loss": 0.5017288637161255, "val_acc": 0.79}, {"epoch": 147, "train_loss": 0.3879910957813263, "val_loss": 0.5005723381042481, "val_acc": 0.79}, {"epoch": 148, "train_loss": 0.38723079442977903, "val_loss": 0.5027157282829284, "val_acc": 0.79}, {"epoch": 149, "train_loss": 0.3868654465675354, "val_loss": 0.5006203460693359, "val_acc": 0.795}, {"epoch": 150, "train_loss": 0.38740944147109985, "val_loss": 0.49875783920288086, "val_acc": 0.8}], "final_metrics": {"epoch": 150, "train_loss": 0.38740944147109985, "val_loss": 0.49875783920288086, "val_acc": 0.8}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\nself.training_epochs = 120", "history": [{"epoch": 1, "train_loss": 1.0740371704101563, "val_loss": 1.0340130805969239, "val_acc": 0.35}, {"epoch": 2, "train_loss": 1.0095359015464782, "val_loss": 0.9936525201797486, "val_acc": 0.345}, {"epoch": 3, "train_loss": 0.9738677883148193, "val_loss": 0.9575244784355164, "val_acc": 0.42}, {"epoch": 4, "train_loss": 0.9381651496887207, "val_loss": 0.9237750840187072, "val_acc": 0.495}, {"epoch": 5, "train_loss": 0.8953563475608826, "val_loss": 0.889819610118866, "val_acc": 0.555}, {"epoch": 6, "train_loss": 0.8503879857063293, "val_loss": 0.8481944465637207, "val_acc": 0.59}, {"epoch": 7, "train_loss": 0.8021959257125855, "val_loss": 0.8035484147071839, "val_acc": 0.63}, {"epoch": 8, "train_loss": 0.753682541847229, "val_loss": 0.7561942791938782, "val_acc": 0.7}, {"epoch": 9, "train_loss": 0.7066000485420227, "val_loss": 0.7176525926589966, "val_acc": 0.71}, {"epoch": 10, "train_loss": 0.6634846663475037, "val_loss": 0.6828559398651123, "val_acc": 0.735}, {"epoch": 11, "train_loss": 0.6281055378913879, "val_loss": 0.6501006698608398, "val_acc": 0.765}, {"epoch": 12, "train_loss": 0.598601405620575, "val_loss": 0.6238131356239319, "val_acc": 0.765}, {"epoch": 13, "train_loss": 0.5754628014564515, "val_loss": 0.5981166505813599, "val_acc": 0.79}, {"epoch": 14, "train_loss": 0.5523285961151123, "val_loss": 0.5816900444030761, "val_acc": 0.765}, {"epoch": 15, "train_loss": 0.5320632112026215, "val_loss": 0.5589237833023071, "val_acc": 0.8}, {"epoch": 16, "train_loss": 0.5180019736289978, "val_loss": 0.5495627880096435, "val_acc": 0.78}, {"epoch": 17, "train_loss": 0.5067921185493469, "val_loss": 0.5308135104179382, "val_acc": 0.82}, {"epoch": 18, "train_loss": 0.4942544174194336, "val_loss": 0.5230046772956848, "val_acc": 0.8}, {"epoch": 19, "train_loss": 0.4834601640701294, "val_loss": 0.5098974418640136, "val_acc": 0.81}, {"epoch": 20, "train_loss": 0.4772459864616394, "val_loss": 0.50254390001297, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.4671908259391785, "val_loss": 0.500298011302948, "val_acc": 0.81}, {"epoch": 22, "train_loss": 0.463827109336853, "val_loss": 0.48530265808105466, "val_acc": 0.82}, {"epoch": 23, "train_loss": 0.4578283166885376, "val_loss": 0.48339549779891966, "val_acc": 0.82}, {"epoch": 24, "train_loss": 0.4526663851737976, "val_loss": 0.47734185457229616, "val_acc": 0.815}, {"epoch": 25, "train_loss": 0.4485300993919373, "val_loss": 0.4739557957649231, "val_acc": 0.815}, {"epoch": 26, "train_loss": 0.4479418194293976, "val_loss": 0.47141589879989626, "val_acc": 0.815}, {"epoch": 27, "train_loss": 0.44450979828834536, "val_loss": 0.4618832087516785, "val_acc": 0.825}, {"epoch": 28, "train_loss": 0.44223268508911134, "val_loss": 0.47001389503479, "val_acc": 0.81}, {"epoch": 29, "train_loss": 0.43691309332847594, "val_loss": 0.46099101424217226, "val_acc": 0.82}, {"epoch": 30, "train_loss": 0.4336355841159821, "val_loss": 0.458663250207901, "val_acc": 0.815}, {"epoch": 31, "train_loss": 0.4323920977115631, "val_loss": 0.4566278827190399, "val_acc": 0.815}, {"epoch": 32, "train_loss": 0.43513486862182615, "val_loss": 0.46094979166984557, "val_acc": 0.81}, {"epoch": 33, "train_loss": 0.43086796879768374, "val_loss": 0.4518561911582947, "val_acc": 0.835}, {"epoch": 34, "train_loss": 0.4305531787872314, "val_loss": 0.45135363936424255, "val_acc": 0.815}, {"epoch": 35, "train_loss": 0.428315589427948, "val_loss": 0.45034778594970704, "val_acc": 0.825}, {"epoch": 36, "train_loss": 0.4245684635639191, "val_loss": 0.45611764788627623, "val_acc": 0.805}, {"epoch": 37, "train_loss": 0.42517594575881956, "val_loss": 0.4483467984199524, "val_acc": 0.82}, {"epoch": 38, "train_loss": 0.4259498953819275, "val_loss": 0.45064815521240237, "val_acc": 0.815}, {"epoch": 39, "train_loss": 0.4263349652290344, "val_loss": 0.44873411417007447, "val_acc": 0.82}, {"epoch": 40, "train_loss": 0.4224701505899429, "val_loss": 0.4448886036872864, "val_acc": 0.83}, {"epoch": 41, "train_loss": 0.42297847926616666, "val_loss": 0.4484788739681244, "val_acc": 0.815}, {"epoch": 42, "train_loss": 0.42236403226852415, "val_loss": 0.4448411238193512, "val_acc": 0.825}, {"epoch": 43, "train_loss": 0.4227813184261322, "val_loss": 0.4422664475440979, "val_acc": 0.825}, {"epoch": 44, "train_loss": 0.4189653921127319, "val_loss": 0.4415518021583557, "val_acc": 0.825}, {"epoch": 45, "train_loss": 0.418986439704895, "val_loss": 0.4459953820705414, "val_acc": 0.825}, {"epoch": 46, "train_loss": 0.4169079995155334, "val_loss": 0.4409850800037384, "val_acc": 0.82}, {"epoch": 47, "train_loss": 0.41871939539909364, "val_loss": 0.45082209348678587, "val_acc": 0.815}, {"epoch": 48, "train_loss": 0.42335092782974243, "val_loss": 0.44021530032157896, "val_acc": 0.825}, {"epoch": 49, "train_loss": 0.42165684819221494, "val_loss": 0.443160218000412, "val_acc": 0.815}, {"epoch": 50, "train_loss": 0.4190299677848816, "val_loss": 0.4481510066986084, "val_acc": 0.835}, {"epoch": 51, "train_loss": 0.4172440207004547, "val_loss": 0.43871230602264405, "val_acc": 0.83}, {"epoch": 52, "train_loss": 0.4160932695865631, "val_loss": 0.4423878085613251, "val_acc": 0.83}, {"epoch": 53, "train_loss": 0.4169148802757263, "val_loss": 0.4402914929389954, "val_acc": 0.83}, {"epoch": 54, "train_loss": 0.4152619755268097, "val_loss": 0.44639105439186094, "val_acc": 0.82}, {"epoch": 55, "train_loss": 0.4154028749465942, "val_loss": 0.44067800760269166, "val_acc": 0.83}, {"epoch": 56, "train_loss": 0.4139005529880524, "val_loss": 0.44258190512657164, "val_acc": 0.835}, {"epoch": 57, "train_loss": 0.4147755026817322, "val_loss": 0.44180328369140626, "val_acc": 0.815}, {"epoch": 58, "train_loss": 0.4137567317485809, "val_loss": 0.4409261441230774, "val_acc": 0.83}, {"epoch": 59, "train_loss": 0.41289190292358396, "val_loss": 0.4419413983821869, "val_acc": 0.825}, {"epoch": 60, "train_loss": 0.41214037775993345, "val_loss": 0.4466667985916138, "val_acc": 0.825}, {"epoch": 61, "train_loss": 0.4128404927253723, "val_loss": 0.44154996633529664, "val_acc": 0.825}, {"epoch": 62, "train_loss": 0.4143389391899109, "val_loss": 0.4425244772434235, "val_acc": 0.825}, {"epoch": 63, "train_loss": 0.41226067185401916, "val_loss": 0.441603125333786, "val_acc": 0.83}, {"epoch": 64, "train_loss": 0.412435542345047, "val_loss": 0.4456294798851013, "val_acc": 0.84}, {"epoch": 65, "train_loss": 0.4134541690349579, "val_loss": 0.44243338346481326, "val_acc": 0.825}, {"epoch": 66, "train_loss": 0.41311755299568176, "val_loss": 0.4402625024318695, "val_acc": 0.825}, {"epoch": 67, "train_loss": 0.4108302617073059, "val_loss": 0.44023269057273867, "val_acc": 0.82}, {"epoch": 68, "train_loss": 0.4107982099056244, "val_loss": 0.44168651700019834, "val_acc": 0.82}, {"epoch": 69, "train_loss": 0.4130429446697235, "val_loss": 0.44205214977264407, "val_acc": 0.83}, {"epoch": 70, "train_loss": 0.4128735184669495, "val_loss": 0.44466461181640626, "val_acc": 0.81}, {"epoch": 71, "train_loss": 0.411333738565445, "val_loss": 0.44069844245910644, "val_acc": 0.835}, {"epoch": 72, "train_loss": 0.4125355446338654, "val_loss": 0.44108038902282715, "val_acc": 0.815}, {"epoch": 73, "train_loss": 0.4104280936717987, "val_loss": 0.4408962559700012, "val_acc": 0.825}, {"epoch": 74, "train_loss": 0.41111218452453613, "val_loss": 0.4437751936912537, "val_acc": 0.83}, {"epoch": 75, "train_loss": 0.4166543889045715, "val_loss": 0.44241740584373473, "val_acc": 0.815}, {"epoch": 76, "train_loss": 0.4104826641082764, "val_loss": 0.44386648178100585, "val_acc": 0.835}, {"epoch": 77, "train_loss": 0.40928578972816465, "val_loss": 0.43966044306755064, "val_acc": 0.825}, {"epoch": 78, "train_loss": 0.4092017936706543, "val_loss": 0.4423393821716309, "val_acc": 0.82}, {"epoch": 79, "train_loss": 0.4107293963432312, "val_loss": 0.45014787077903745, "val_acc": 0.81}, {"epoch": 80, "train_loss": 0.40997666358947754, "val_loss": 0.44008278250694277, "val_acc": 0.82}, {"epoch": 81, "train_loss": 0.41074917793273924, "val_loss": 0.4427090358734131, "val_acc": 0.825}, {"epoch": 82, "train_loss": 0.40890213966369626, "val_loss": 0.4407627260684967, "val_acc": 0.83}, {"epoch": 83, "train_loss": 0.41072448015213014, "val_loss": 0.4433207023143768, "val_acc": 0.83}, {"epoch": 84, "train_loss": 0.40794636130332945, "val_loss": 0.44535589814186094, "val_acc": 0.82}, {"epoch": 85, "train_loss": 0.40867468297481535, "val_loss": 0.43996771931648254, "val_acc": 0.825}, {"epoch": 86, "train_loss": 0.4071746265888214, "val_loss": 0.4439687144756317, "val_acc": 0.825}, {"epoch": 87, "train_loss": 0.40771296977996824, "val_loss": 0.44370744824409486, "val_acc": 0.83}, {"epoch": 88, "train_loss": 0.40870895981788635, "val_loss": 0.4428690564632416, "val_acc": 0.815}, {"epoch": 89, "train_loss": 0.4094197452068329, "val_loss": 0.4444387662410736, "val_acc": 0.825}, {"epoch": 90, "train_loss": 0.40668397188186645, "val_loss": 0.4433457922935486, "val_acc": 0.825}, {"epoch": 91, "train_loss": 0.4077732288837433, "val_loss": 0.43968631148338316, "val_acc": 0.82}, {"epoch": 92, "train_loss": 0.41268895626068114, "val_loss": 0.4408675050735474, "val_acc": 0.825}, {"epoch": 93, "train_loss": 0.41477251052856445, "val_loss": 0.4477096104621887, "val_acc": 0.81}, {"epoch": 94, "train_loss": 0.4080051100254059, "val_loss": 0.4433304536342621, "val_acc": 0.815}, {"epoch": 95, "train_loss": 0.4069037961959839, "val_loss": 0.4418879568576813, "val_acc": 0.815}, {"epoch": 96, "train_loss": 0.4053881275653839, "val_loss": 0.4406408262252808, "val_acc": 0.82}, {"epoch": 97, "train_loss": 0.4059084248542786, "val_loss": 0.44545098423957824, "val_acc": 0.825}, {"epoch": 98, "train_loss": 0.40635637640953065, "val_loss": 0.4406936359405518, "val_acc": 0.825}, {"epoch": 99, "train_loss": 0.40653793811798095, "val_loss": 0.4509667885303497, "val_acc": 0.82}, {"epoch": 100, "train_loss": 0.4063219749927521, "val_loss": 0.4421260416507721, "val_acc": 0.83}, {"epoch": 101, "train_loss": 0.4066081213951111, "val_loss": 0.44513702869415284, "val_acc": 0.825}, {"epoch": 102, "train_loss": 0.40742902994155883, "val_loss": 0.4394786810874939, "val_acc": 0.82}, {"epoch": 103, "train_loss": 0.40701172351837156, "val_loss": 0.4468041455745697, "val_acc": 0.83}, {"epoch": 104, "train_loss": 0.4053192710876465, "val_loss": 0.4433884048461914, "val_acc": 0.82}, {"epoch": 105, "train_loss": 0.4075837182998657, "val_loss": 0.4496894359588623, "val_acc": 0.83}, {"epoch": 106, "train_loss": 0.4056865239143372, "val_loss": 0.4439795434474945, "val_acc": 0.82}, {"epoch": 107, "train_loss": 0.40554617404937743, "val_loss": 0.4445004999637604, "val_acc": 0.82}, {"epoch": 108, "train_loss": 0.40491551637649537, "val_loss": 0.44797892212867735, "val_acc": 0.815}, {"epoch": 109, "train_loss": 0.40731747150421144, "val_loss": 0.45053586602211, "val_acc": 0.82}, {"epoch": 110, "train_loss": 0.40640286028385164, "val_loss": 0.4454794728755951, "val_acc": 0.815}, {"epoch": 111, "train_loss": 0.40621774554252627, "val_loss": 0.4457006323337555, "val_acc": 0.82}, {"epoch": 112, "train_loss": 0.40925248980522155, "val_loss": 0.44309295892715456, "val_acc": 0.825}, {"epoch": 113, "train_loss": 0.4049244427680969, "val_loss": 0.4519407296180725, "val_acc": 0.825}, {"epoch": 114, "train_loss": 0.4050642228126526, "val_loss": 0.44336753129959106, "val_acc": 0.825}, {"epoch": 115, "train_loss": 0.40486380100250247, "val_loss": 0.4488834512233734, "val_acc": 0.81}, {"epoch": 116, "train_loss": 0.4063458383083344, "val_loss": 0.4455248463153839, "val_acc": 0.82}, {"epoch": 117, "train_loss": 0.4034701359272003, "val_loss": 0.4497881352901459, "val_acc": 0.825}, {"epoch": 118, "train_loss": 0.4055578112602234, "val_loss": 0.44500488400459287, "val_acc": 0.83}, {"epoch": 119, "train_loss": 0.40445306301116946, "val_loss": 0.4512513184547424, "val_acc": 0.82}, {"epoch": 120, "train_loss": 0.4036095547676086, "val_loss": 0.4465682125091553, "val_acc": 0.82}], "final_metrics": {"epoch": 120, "train_loss": 0.4036095547676086, "val_loss": 0.4465682125091553, "val_acc": 0.82}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 120), nn.ReLU(), nn.Linear(120, 60), nn.ReLU(), nn.Linear(60, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\nself.training_epochs = 130", "history": [{"epoch": 1, "train_loss": 1.046080813407898, "val_loss": 1.0206580305099486, "val_acc": 0.38}, {"epoch": 2, "train_loss": 0.9986635446548462, "val_loss": 0.9841287517547608, "val_acc": 0.435}, {"epoch": 3, "train_loss": 0.9592918539047242, "val_loss": 0.9472139978408813, "val_acc": 0.5}, {"epoch": 4, "train_loss": 0.9194452667236328, "val_loss": 0.9061439943313598, "val_acc": 0.57}, {"epoch": 5, "train_loss": 0.8791347956657409, "val_loss": 0.8667922806739807, "val_acc": 0.615}, {"epoch": 6, "train_loss": 0.8349070453643799, "val_loss": 0.8232178211212158, "val_acc": 0.655}, {"epoch": 7, "train_loss": 0.7885913634300232, "val_loss": 0.7799123287200928, "val_acc": 0.695}, {"epoch": 8, "train_loss": 0.7453670048713684, "val_loss": 0.7394797658920288, "val_acc": 0.735}, {"epoch": 9, "train_loss": 0.704873640537262, "val_loss": 0.7014695310592651, "val_acc": 0.74}, {"epoch": 10, "train_loss": 0.6649197602272033, "val_loss": 0.668226203918457, "val_acc": 0.72}, {"epoch": 11, "train_loss": 0.633000648021698, "val_loss": 0.6360630893707275, "val_acc": 0.77}, {"epoch": 12, "train_loss": 0.6011240768432617, "val_loss": 0.6102234435081482, "val_acc": 0.77}, {"epoch": 13, "train_loss": 0.5744077110290527, "val_loss": 0.5863240814208984, "val_acc": 0.775}, {"epoch": 14, "train_loss": 0.5528232049942017, "val_loss": 0.5699158072471618, "val_acc": 0.78}, {"epoch": 15, "train_loss": 0.5334833931922912, "val_loss": 0.5560105848312378, "val_acc": 0.775}, {"epoch": 16, "train_loss": 0.5148792695999146, "val_loss": 0.5398299062252044, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.5021594953536987, "val_loss": 0.5281651306152344, "val_acc": 0.79}, {"epoch": 18, "train_loss": 0.4892787265777588, "val_loss": 0.522405526638031, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.48016606211662294, "val_loss": 0.5164582800865173, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.4707299566268921, "val_loss": 0.5113013899326324, "val_acc": 0.795}, {"epoch": 21, "train_loss": 0.4619897770881653, "val_loss": 0.5050050759315491, "val_acc": 0.8}, {"epoch": 22, "train_loss": 0.45575446605682374, "val_loss": 0.49811068654060364, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.4515153193473816, "val_loss": 0.4969341230392456, "val_acc": 0.795}, {"epoch": 24, "train_loss": 0.4498379862308502, "val_loss": 0.49324283361434934, "val_acc": 0.795}, {"epoch": 25, "train_loss": 0.44365150332450864, "val_loss": 0.4867686104774475, "val_acc": 0.79}, {"epoch": 26, "train_loss": 0.43938992500305174, "val_loss": 0.4914607059955597, "val_acc": 0.8}, {"epoch": 27, "train_loss": 0.43303908586502077, "val_loss": 0.4842813873291016, "val_acc": 0.79}, {"epoch": 28, "train_loss": 0.43203317165374755, "val_loss": 0.48383856415748594, "val_acc": 0.79}, {"epoch": 29, "train_loss": 0.43136333584785463, "val_loss": 0.48374409675598146, "val_acc": 0.79}, {"epoch": 30, "train_loss": 0.429069972038269, "val_loss": 0.4798386180400848, "val_acc": 0.805}, {"epoch": 31, "train_loss": 0.4221464157104492, "val_loss": 0.4787785816192627, "val_acc": 0.79}, {"epoch": 32, "train_loss": 0.4248225426673889, "val_loss": 0.4823336315155029, "val_acc": 0.79}, {"epoch": 33, "train_loss": 0.41869053840637205, "val_loss": 0.47967121839523313, "val_acc": 0.795}, {"epoch": 34, "train_loss": 0.41778531908988953, "val_loss": 0.47403782963752744, "val_acc": 0.795}, {"epoch": 35, "train_loss": 0.4155029881000519, "val_loss": 0.48033029556274415, "val_acc": 0.8}, {"epoch": 36, "train_loss": 0.41698202133178713, "val_loss": 0.47444539666175845, "val_acc": 0.79}, {"epoch": 37, "train_loss": 0.41566004037857057, "val_loss": 0.4769180965423584, "val_acc": 0.795}, {"epoch": 38, "train_loss": 0.4125363254547119, "val_loss": 0.4772766399383545, "val_acc": 0.79}, {"epoch": 39, "train_loss": 0.411943861246109, "val_loss": 0.47545030593872073, "val_acc": 0.8}, {"epoch": 40, "train_loss": 0.4114830017089844, "val_loss": 0.4746139359474182, "val_acc": 0.79}, {"epoch": 41, "train_loss": 0.41185990810394285, "val_loss": 0.47752572536468507, "val_acc": 0.795}, {"epoch": 42, "train_loss": 0.4105217909812927, "val_loss": 0.48333418369293213, "val_acc": 0.795}, {"epoch": 43, "train_loss": 0.4118406653404236, "val_loss": 0.473460785150528, "val_acc": 0.795}, {"epoch": 44, "train_loss": 0.4074704551696777, "val_loss": 0.4745356154441833, "val_acc": 0.8}, {"epoch": 45, "train_loss": 0.40680001974105834, "val_loss": 0.47993106961250304, "val_acc": 0.79}, {"epoch": 46, "train_loss": 0.4101542782783508, "val_loss": 0.47399947643280027, "val_acc": 0.795}, {"epoch": 47, "train_loss": 0.40972543001174927, "val_loss": 0.4746036994457245, "val_acc": 0.795}, {"epoch": 48, "train_loss": 0.4098985517024994, "val_loss": 0.4776806330680847, "val_acc": 0.795}, {"epoch": 49, "train_loss": 0.40498234748840334, "val_loss": 0.47650168418884276, "val_acc": 0.78}, {"epoch": 50, "train_loss": 0.4027708399295807, "val_loss": 0.4809239888191223, "val_acc": 0.79}, {"epoch": 51, "train_loss": 0.40400451779365537, "val_loss": 0.47584814310073853, "val_acc": 0.79}, {"epoch": 52, "train_loss": 0.4038704061508179, "val_loss": 0.47579436779022216, "val_acc": 0.795}, {"epoch": 53, "train_loss": 0.4032391536235809, "val_loss": 0.4773291707038879, "val_acc": 0.79}, {"epoch": 54, "train_loss": 0.40459141254425046, "val_loss": 0.4770197081565857, "val_acc": 0.79}, {"epoch": 55, "train_loss": 0.402735538482666, "val_loss": 0.4780078423023224, "val_acc": 0.795}, {"epoch": 56, "train_loss": 0.4004347610473633, "val_loss": 0.47756205797195433, "val_acc": 0.79}, {"epoch": 57, "train_loss": 0.40095921874046325, "val_loss": 0.47513184905052186, "val_acc": 0.79}, {"epoch": 58, "train_loss": 0.40095521330833434, "val_loss": 0.4803905463218689, "val_acc": 0.79}, {"epoch": 59, "train_loss": 0.4002896761894226, "val_loss": 0.47975627303123475, "val_acc": 0.785}, {"epoch": 60, "train_loss": 0.40066645860671996, "val_loss": 0.4829568934440613, "val_acc": 0.795}, {"epoch": 61, "train_loss": 0.40106587767601015, "val_loss": 0.4793413305282593, "val_acc": 0.79}, {"epoch": 62, "train_loss": 0.4005211591720581, "val_loss": 0.4807421135902405, "val_acc": 0.79}, {"epoch": 63, "train_loss": 0.40128716826438904, "val_loss": 0.487160906791687, "val_acc": 0.785}, {"epoch": 64, "train_loss": 0.3994377374649048, "val_loss": 0.48207943797111513, "val_acc": 0.795}, {"epoch": 65, "train_loss": 0.401176598072052, "val_loss": 0.47971654891967774, "val_acc": 0.79}, {"epoch": 66, "train_loss": 0.3999519085884094, "val_loss": 0.4852354097366333, "val_acc": 0.8}, {"epoch": 67, "train_loss": 0.3980220448970795, "val_loss": 0.472989741563797, "val_acc": 0.79}, {"epoch": 68, "train_loss": 0.39769484639167785, "val_loss": 0.4835757660865784, "val_acc": 0.79}, {"epoch": 69, "train_loss": 0.4004886281490326, "val_loss": 0.4887578821182251, "val_acc": 0.785}, {"epoch": 70, "train_loss": 0.3991873359680176, "val_loss": 0.4799796199798584, "val_acc": 0.785}, {"epoch": 71, "train_loss": 0.3972455728054047, "val_loss": 0.4861442089080811, "val_acc": 0.8}, {"epoch": 72, "train_loss": 0.39535014510154726, "val_loss": 0.4783569669723511, "val_acc": 0.795}, {"epoch": 73, "train_loss": 0.3953301846981049, "val_loss": 0.48378199100494385, "val_acc": 0.785}, {"epoch": 74, "train_loss": 0.39629820346832273, "val_loss": 0.4814017939567566, "val_acc": 0.785}, {"epoch": 75, "train_loss": 0.3980815589427948, "val_loss": 0.4882767868041992, "val_acc": 0.79}, {"epoch": 76, "train_loss": 0.39888773441314695, "val_loss": 0.48682607889175417, "val_acc": 0.795}, {"epoch": 77, "train_loss": 0.39540681838989256, "val_loss": 0.48743533611297607, "val_acc": 0.785}, {"epoch": 78, "train_loss": 0.39813382625579835, "val_loss": 0.4794740700721741, "val_acc": 0.785}, {"epoch": 79, "train_loss": 0.39540987730026245, "val_loss": 0.48608374714851377, "val_acc": 0.785}, {"epoch": 80, "train_loss": 0.3971983504295349, "val_loss": 0.48937543511390685, "val_acc": 0.795}, {"epoch": 81, "train_loss": 0.39779198050498965, "val_loss": 0.47812148571014407, "val_acc": 0.8}, {"epoch": 82, "train_loss": 0.39706762552261354, "val_loss": 0.4821474838256836, "val_acc": 0.785}, {"epoch": 83, "train_loss": 0.39402456641197203, "val_loss": 0.4872829794883728, "val_acc": 0.79}, {"epoch": 84, "train_loss": 0.3963795328140259, "val_loss": 0.4778429687023163, "val_acc": 0.8}, {"epoch": 85, "train_loss": 0.3936093544960022, "val_loss": 0.4893969535827637, "val_acc": 0.79}, {"epoch": 86, "train_loss": 0.395048126578331, "val_loss": 0.48731630086898803, "val_acc": 0.78}, {"epoch": 87, "train_loss": 0.3927945041656494, "val_loss": 0.48414504051208496, "val_acc": 0.795}, {"epoch": 88, "train_loss": 0.3943507385253906, "val_loss": 0.4849556767940521, "val_acc": 0.79}, {"epoch": 89, "train_loss": 0.3944045901298523, "val_loss": 0.49577214002609254, "val_acc": 0.78}, {"epoch": 90, "train_loss": 0.39206985354423524, "val_loss": 0.4874954414367676, "val_acc": 0.785}, {"epoch": 91, "train_loss": 0.39260507822036744, "val_loss": 0.4794314992427826, "val_acc": 0.8}, {"epoch": 92, "train_loss": 0.39700170278549196, "val_loss": 0.48796929240226744, "val_acc": 0.79}, {"epoch": 93, "train_loss": 0.39483961820602415, "val_loss": 0.4934203577041626, "val_acc": 0.785}, {"epoch": 94, "train_loss": 0.39535784006118774, "val_loss": 0.4935476541519165, "val_acc": 0.785}, {"epoch": 95, "train_loss": 0.39294187545776366, "val_loss": 0.48322405099868776, "val_acc": 0.79}, {"epoch": 96, "train_loss": 0.39471572637557983, "val_loss": 0.4855415487289429, "val_acc": 0.785}, {"epoch": 97, "train_loss": 0.3916616976261139, "val_loss": 0.486689772605896, "val_acc": 0.785}, {"epoch": 98, "train_loss": 0.3928194606304169, "val_loss": 0.48203092694282534, "val_acc": 0.79}, {"epoch": 99, "train_loss": 0.39082234382629394, "val_loss": 0.49019610524177554, "val_acc": 0.785}, {"epoch": 100, "train_loss": 0.39332085609436035, "val_loss": 0.4853692281246185, "val_acc": 0.79}, {"epoch": 101, "train_loss": 0.39118425130844114, "val_loss": 0.4886410927772522, "val_acc": 0.785}, {"epoch": 102, "train_loss": 0.3907734298706055, "val_loss": 0.4829967176914215, "val_acc": 0.79}, {"epoch": 103, "train_loss": 0.39379421710968016, "val_loss": 0.48868278980255125, "val_acc": 0.795}, {"epoch": 104, "train_loss": 0.3911682105064392, "val_loss": 0.4881113052368164, "val_acc": 0.8}, {"epoch": 105, "train_loss": 0.3911468625068665, "val_loss": 0.4952227449417114, "val_acc": 0.785}, {"epoch": 106, "train_loss": 0.3912519258260727, "val_loss": 0.48269698619842527, "val_acc": 0.79}, {"epoch": 107, "train_loss": 0.3923303306102753, "val_loss": 0.48418268680572507, "val_acc": 0.785}, {"epoch": 108, "train_loss": 0.39099505424499514, "val_loss": 0.4850573492050171, "val_acc": 0.785}, {"epoch": 109, "train_loss": 0.39017647624015805, "val_loss": 0.49192482233047485, "val_acc": 0.78}, {"epoch": 110, "train_loss": 0.39130523324012756, "val_loss": 0.48156304359436036, "val_acc": 0.79}, {"epoch": 111, "train_loss": 0.3895662748813629, "val_loss": 0.4956284332275391, "val_acc": 0.78}, {"epoch": 112, "train_loss": 0.39479024648666383, "val_loss": 0.4862379217147827, "val_acc": 0.785}, {"epoch": 113, "train_loss": 0.3973384582996368, "val_loss": 0.49469805002212525, "val_acc": 0.78}, {"epoch": 114, "train_loss": 0.3909472286701202, "val_loss": 0.4839960324764252, "val_acc": 0.785}, {"epoch": 115, "train_loss": 0.3881006705760956, "val_loss": 0.49868579864501955, "val_acc": 0.78}, {"epoch": 116, "train_loss": 0.39014760017395017, "val_loss": 0.4825173592567444, "val_acc": 0.805}, {"epoch": 117, "train_loss": 0.38926897644996644, "val_loss": 0.5055059432983399, "val_acc": 0.785}, {"epoch": 118, "train_loss": 0.38915088415145876, "val_loss": 0.4832801592350006, "val_acc": 0.79}, {"epoch": 119, "train_loss": 0.3900823998451233, "val_loss": 0.4836146080493927, "val_acc": 0.785}, {"epoch": 120, "train_loss": 0.38944864869117735, "val_loss": 0.49476467609405517, "val_acc": 0.78}, {"epoch": 121, "train_loss": 0.38748791098594665, "val_loss": 0.4825659513473511, "val_acc": 0.795}, {"epoch": 122, "train_loss": 0.39291165351867674, "val_loss": 0.49106743454933166, "val_acc": 0.775}, {"epoch": 123, "train_loss": 0.38794785261154174, "val_loss": 0.4941775631904602, "val_acc": 0.785}, {"epoch": 124, "train_loss": 0.38773976802825927, "val_loss": 0.48681092500686646, "val_acc": 0.785}, {"epoch": 125, "train_loss": 0.38807843208312987, "val_loss": 0.4954845452308655, "val_acc": 0.775}, {"epoch": 126, "train_loss": 0.38821383714675906, "val_loss": 0.49233859419822695, "val_acc": 0.78}, {"epoch": 127, "train_loss": 0.38678940415382385, "val_loss": 0.4871236562728882, "val_acc": 0.78}, {"epoch": 128, "train_loss": 0.3895792067050934, "val_loss": 0.49616525411605833, "val_acc": 0.78}, {"epoch": 129, "train_loss": 0.38640682220458983, "val_loss": 0.48801841139793395, "val_acc": 0.79}, {"epoch": 130, "train_loss": 0.3874469590187073, "val_loss": 0.49045061707496646, "val_acc": 0.78}], "final_metrics": {"epoch": 130, "train_loss": 0.3874469590187073, "val_loss": 0.49045061707496646, "val_acc": 0.78}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 80), nn.ReLU(), nn.Linear(80, 40), nn.ReLU(), nn.Linear(40, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\nself.training_epochs = 140", "history": [{"epoch": 1, "train_loss": 1.108455901145935, "val_loss": 1.0585309743881226, "val_acc": 0.42}, {"epoch": 2, "train_loss": 1.0629505372047425, "val_loss": 1.0230427718162536, "val_acc": 0.43}, {"epoch": 3, "train_loss": 1.0318282961845398, "val_loss": 0.9946521735191345, "val_acc": 0.45}, {"epoch": 4, "train_loss": 1.0037545251846314, "val_loss": 0.9683817911148072, "val_acc": 0.51}, {"epoch": 5, "train_loss": 0.9749589014053345, "val_loss": 0.9388172912597657, "val_acc": 0.55}, {"epoch": 6, "train_loss": 0.9438297080993653, "val_loss": 0.9072819042205811, "val_acc": 0.62}, {"epoch": 7, "train_loss": 0.9108302664756774, "val_loss": 0.8718717765808105, "val_acc": 0.635}, {"epoch": 8, "train_loss": 0.8749840784072876, "val_loss": 0.837666642665863, "val_acc": 0.685}, {"epoch": 9, "train_loss": 0.8393957018852234, "val_loss": 0.8009801006317139, "val_acc": 0.705}, {"epoch": 10, "train_loss": 0.8028277969360351, "val_loss": 0.76422621011734, "val_acc": 0.695}, {"epoch": 11, "train_loss": 0.7678090643882751, "val_loss": 0.7319021463394165, "val_acc": 0.715}, {"epoch": 12, "train_loss": 0.7338706970214843, "val_loss": 0.6978935360908508, "val_acc": 0.73}, {"epoch": 13, "train_loss": 0.7027433705329895, "val_loss": 0.6658931159973145, "val_acc": 0.755}, {"epoch": 14, "train_loss": 0.6728544163703919, "val_loss": 0.6402044224739075, "val_acc": 0.76}, {"epoch": 15, "train_loss": 0.6475158143043518, "val_loss": 0.6157945418357849, "val_acc": 0.795}, {"epoch": 16, "train_loss": 0.623383436203003, "val_loss": 0.5947297620773315, "val_acc": 0.805}, {"epoch": 17, "train_loss": 0.6025994420051575, "val_loss": 0.5758677458763123, "val_acc": 0.82}, {"epoch": 18, "train_loss": 0.5834391021728516, "val_loss": 0.556279501914978, "val_acc": 0.83}, {"epoch": 19, "train_loss": 0.5666628289222717, "val_loss": 0.5402959728240967, "val_acc": 0.84}, {"epoch": 20, "train_loss": 0.5556740212440491, "val_loss": 0.5278850841522217, "val_acc": 0.84}, {"epoch": 21, "train_loss": 0.5477862000465393, "val_loss": 0.513047661781311, "val_acc": 0.84}, {"epoch": 22, "train_loss": 0.53102614402771, "val_loss": 0.5074860405921936, "val_acc": 0.815}, {"epoch": 23, "train_loss": 0.5174791789054871, "val_loss": 0.4940973162651062, "val_acc": 0.835}, {"epoch": 24, "train_loss": 0.5094472944736481, "val_loss": 0.4849015736579895, "val_acc": 0.85}, {"epoch": 25, "train_loss": 0.5012456393241882, "val_loss": 0.47633369207382203, "val_acc": 0.845}, {"epoch": 26, "train_loss": 0.49347800493240357, "val_loss": 0.47062785387039185, "val_acc": 0.85}, {"epoch": 27, "train_loss": 0.4872421622276306, "val_loss": 0.4634030818939209, "val_acc": 0.845}, {"epoch": 28, "train_loss": 0.48320114612579346, "val_loss": 0.45842361211776733, "val_acc": 0.855}, {"epoch": 29, "train_loss": 0.4776743686199188, "val_loss": 0.4555079007148743, "val_acc": 0.845}, {"epoch": 30, "train_loss": 0.4720925736427307, "val_loss": 0.4489933729171753, "val_acc": 0.85}, {"epoch": 31, "train_loss": 0.46937586545944215, "val_loss": 0.442660493850708, "val_acc": 0.85}, {"epoch": 32, "train_loss": 0.4636493968963623, "val_loss": 0.43913376808166504, "val_acc": 0.845}, {"epoch": 33, "train_loss": 0.4619553017616272, "val_loss": 0.4398220419883728, "val_acc": 0.84}, {"epoch": 34, "train_loss": 0.4578461956977844, "val_loss": 0.4318308973312378, "val_acc": 0.85}, {"epoch": 35, "train_loss": 0.45607476234436034, "val_loss": 0.429123113155365, "val_acc": 0.86}, {"epoch": 36, "train_loss": 0.45474143743515016, "val_loss": 0.42948797941207884, "val_acc": 0.845}, {"epoch": 37, "train_loss": 0.45412479639053344, "val_loss": 0.4243518042564392, "val_acc": 0.86}, {"epoch": 38, "train_loss": 0.4499033498764038, "val_loss": 0.42317131996154783, "val_acc": 0.86}, {"epoch": 39, "train_loss": 0.44659417748451236, "val_loss": 0.4213146162033081, "val_acc": 0.86}, {"epoch": 40, "train_loss": 0.44501609086990357, "val_loss": 0.4183386445045471, "val_acc": 0.855}, {"epoch": 41, "train_loss": 0.44551854372024535, "val_loss": 0.4174616718292236, "val_acc": 0.85}, {"epoch": 42, "train_loss": 0.446150598526001, "val_loss": 0.41692676782608035, "val_acc": 0.865}, {"epoch": 43, "train_loss": 0.44135125041007994, "val_loss": 0.4159988260269165, "val_acc": 0.86}, {"epoch": 44, "train_loss": 0.44292797565460207, "val_loss": 0.41268465042114255, "val_acc": 0.85}, {"epoch": 45, "train_loss": 0.4389968812465668, "val_loss": 0.41257587671279905, "val_acc": 0.855}, {"epoch": 46, "train_loss": 0.4377917861938477, "val_loss": 0.41017885446548463, "val_acc": 0.865}, {"epoch": 47, "train_loss": 0.4371835434436798, "val_loss": 0.4097576403617859, "val_acc": 0.86}, {"epoch": 48, "train_loss": 0.4368943607807159, "val_loss": 0.4093954133987427, "val_acc": 0.855}, {"epoch": 49, "train_loss": 0.4360215759277344, "val_loss": 0.4078084373474121, "val_acc": 0.86}, {"epoch": 50, "train_loss": 0.4347619342803955, "val_loss": 0.40667975664138795, "val_acc": 0.865}, {"epoch": 51, "train_loss": 0.4342450475692749, "val_loss": 0.40526188611984254, "val_acc": 0.865}, {"epoch": 52, "train_loss": 0.4348590803146362, "val_loss": 0.4047908616065979, "val_acc": 0.86}, {"epoch": 53, "train_loss": 0.4389145731925964, "val_loss": 0.40753000020980834, "val_acc": 0.855}, {"epoch": 54, "train_loss": 0.43807013273239137, "val_loss": 0.4026589560508728, "val_acc": 0.86}, {"epoch": 55, "train_loss": 0.43182278037071226, "val_loss": 0.40387262105941774, "val_acc": 0.865}, {"epoch": 56, "train_loss": 0.43173643469810485, "val_loss": 0.40413058757781983, "val_acc": 0.86}, {"epoch": 57, "train_loss": 0.4313550138473511, "val_loss": 0.4019618320465088, "val_acc": 0.865}, {"epoch": 58, "train_loss": 0.43102689266204836, "val_loss": 0.40197416543960574, "val_acc": 0.865}, {"epoch": 59, "train_loss": 0.43114632487297055, "val_loss": 0.4030470299720764, "val_acc": 0.865}, {"epoch": 60, "train_loss": 0.4305959236621857, "val_loss": 0.4025373315811157, "val_acc": 0.855}, {"epoch": 61, "train_loss": 0.4313398027420044, "val_loss": 0.4011171007156372, "val_acc": 0.86}, {"epoch": 62, "train_loss": 0.4301470136642456, "val_loss": 0.39991360425949096, "val_acc": 0.87}, {"epoch": 63, "train_loss": 0.4281535911560059, "val_loss": 0.4001664853096008, "val_acc": 0.865}, {"epoch": 64, "train_loss": 0.4292512905597687, "val_loss": 0.4020739150047302, "val_acc": 0.85}, {"epoch": 65, "train_loss": 0.430462498664856, "val_loss": 0.40024917125701903, "val_acc": 0.87}, {"epoch": 66, "train_loss": 0.4305953133106232, "val_loss": 0.39957236766815185, "val_acc": 0.865}, {"epoch": 67, "train_loss": 0.42869876980781557, "val_loss": 0.4006537914276123, "val_acc": 0.86}, {"epoch": 68, "train_loss": 0.4288047695159912, "val_loss": 0.40164796113967893, "val_acc": 0.865}, {"epoch": 69, "train_loss": 0.42675759315490724, "val_loss": 0.400885694026947, "val_acc": 0.865}, {"epoch": 70, "train_loss": 0.42805402278900145, "val_loss": 0.3985773205757141, "val_acc": 0.86}, {"epoch": 71, "train_loss": 0.42633599519729615, "val_loss": 0.3991800546646118, "val_acc": 0.86}, {"epoch": 72, "train_loss": 0.4273082804679871, "val_loss": 0.39845493316650393, "val_acc": 0.86}, {"epoch": 73, "train_loss": 0.42676838040351867, "val_loss": 0.39917295694351196, "val_acc": 0.86}, {"epoch": 74, "train_loss": 0.42625542044639586, "val_loss": 0.39851107597351076, "val_acc": 0.87}, {"epoch": 75, "train_loss": 0.4256391835212707, "val_loss": 0.3985821604728699, "val_acc": 0.85}, {"epoch": 76, "train_loss": 0.42970584630966185, "val_loss": 0.39968990564346313, "val_acc": 0.86}, {"epoch": 77, "train_loss": 0.4251546835899353, "val_loss": 0.39892292737960816, "val_acc": 0.86}, {"epoch": 78, "train_loss": 0.4269620144367218, "val_loss": 0.3972179460525513, "val_acc": 0.855}, {"epoch": 79, "train_loss": 0.42482232570648193, "val_loss": 0.39657814502716066, "val_acc": 0.87}, {"epoch": 80, "train_loss": 0.4260255217552185, "val_loss": 0.3977805757522583, "val_acc": 0.86}, {"epoch": 81, "train_loss": 0.4256202268600464, "val_loss": 0.39685844659805297, "val_acc": 0.865}, {"epoch": 82, "train_loss": 0.42585963249206543, "val_loss": 0.39660800457000733, "val_acc": 0.87}, {"epoch": 83, "train_loss": 0.42430105686187747, "val_loss": 0.3963238501548767, "val_acc": 0.86}, {"epoch": 84, "train_loss": 0.4243865120410919, "val_loss": 0.39661847591400146, "val_acc": 0.87}, {"epoch": 85, "train_loss": 0.42414995312690734, "val_loss": 0.397689790725708, "val_acc": 0.855}, {"epoch": 86, "train_loss": 0.4241535079479217, "val_loss": 0.3971391534805298, "val_acc": 0.865}, {"epoch": 87, "train_loss": 0.4240138626098633, "val_loss": 0.3968480038642883, "val_acc": 0.855}, {"epoch": 88, "train_loss": 0.4237416100502014, "val_loss": 0.3965291404724121, "val_acc": 0.865}, {"epoch": 89, "train_loss": 0.4241789412498474, "val_loss": 0.3978183841705322, "val_acc": 0.865}, {"epoch": 90, "train_loss": 0.42356395840644834, "val_loss": 0.3971900010108948, "val_acc": 0.85}, {"epoch": 91, "train_loss": 0.4239458465576172, "val_loss": 0.39620667934417725, "val_acc": 0.865}, {"epoch": 92, "train_loss": 0.42371553301811216, "val_loss": 0.3978803467750549, "val_acc": 0.87}, {"epoch": 93, "train_loss": 0.42282206773757935, "val_loss": 0.3963872766494751, "val_acc": 0.865}, {"epoch": 94, "train_loss": 0.4239506149291992, "val_loss": 0.3953446435928345, "val_acc": 0.865}, {"epoch": 95, "train_loss": 0.42567806005477904, "val_loss": 0.39610896587371824, "val_acc": 0.85}, {"epoch": 96, "train_loss": 0.4242293632030487, "val_loss": 0.3965917682647705, "val_acc": 0.86}, {"epoch": 97, "train_loss": 0.42325093030929567, "val_loss": 0.3956494474411011, "val_acc": 0.87}, {"epoch": 98, "train_loss": 0.4242158758640289, "val_loss": 0.3951742148399353, "val_acc": 0.86}, {"epoch": 99, "train_loss": 0.423913037776947, "val_loss": 0.3946583843231201, "val_acc": 0.865}, {"epoch": 100, "train_loss": 0.42242094993591306, "val_loss": 0.39494282484054566, "val_acc": 0.865}, {"epoch": 101, "train_loss": 0.4214585506916046, "val_loss": 0.3951852202415466, "val_acc": 0.855}, {"epoch": 102, "train_loss": 0.4240705490112305, "val_loss": 0.3959922289848328, "val_acc": 0.87}, {"epoch": 103, "train_loss": 0.42416268825531006, "val_loss": 0.396496057510376, "val_acc": 0.85}, {"epoch": 104, "train_loss": 0.4219855630397797, "val_loss": 0.3977221703529358, "val_acc": 0.87}, {"epoch": 105, "train_loss": 0.4223472499847412, "val_loss": 0.3951712536811829, "val_acc": 0.865}, {"epoch": 106, "train_loss": 0.42119035840034486, "val_loss": 0.39550923347473144, "val_acc": 0.87}, {"epoch": 107, "train_loss": 0.42259456515312194, "val_loss": 0.39554967641830446, "val_acc": 0.855}, {"epoch": 108, "train_loss": 0.42011853456497195, "val_loss": 0.39508321523666384, "val_acc": 0.865}, {"epoch": 109, "train_loss": 0.4219998455047607, "val_loss": 0.39514790534973143, "val_acc": 0.87}, {"epoch": 110, "train_loss": 0.4216434609889984, "val_loss": 0.39373834133148194, "val_acc": 0.865}, {"epoch": 111, "train_loss": 0.42105172991752626, "val_loss": 0.3946428036689758, "val_acc": 0.855}, {"epoch": 112, "train_loss": 0.42183653354644773, "val_loss": 0.3940478086471558, "val_acc": 0.865}, {"epoch": 113, "train_loss": 0.4208099102973938, "val_loss": 0.3948440766334534, "val_acc": 0.865}, {"epoch": 114, "train_loss": 0.421548148393631, "val_loss": 0.3942744302749634, "val_acc": 0.87}, {"epoch": 115, "train_loss": 0.4216343641281128, "val_loss": 0.3925107216835022, "val_acc": 0.865}, {"epoch": 116, "train_loss": 0.4223969864845276, "val_loss": 0.39498416423797605, "val_acc": 0.85}, {"epoch": 117, "train_loss": 0.4236826479434967, "val_loss": 0.3947589898109436, "val_acc": 0.865}, {"epoch": 118, "train_loss": 0.42407954692840577, "val_loss": 0.3927286171913147, "val_acc": 0.87}, {"epoch": 119, "train_loss": 0.4213500279188156, "val_loss": 0.3960737800598145, "val_acc": 0.85}, {"epoch": 120, "train_loss": 0.4211346137523651, "val_loss": 0.3941029953956604, "val_acc": 0.865}, {"epoch": 121, "train_loss": 0.42185752749443056, "val_loss": 0.3935115051269531, "val_acc": 0.865}, {"epoch": 122, "train_loss": 0.4196591806411743, "val_loss": 0.3938738203048706, "val_acc": 0.865}, {"epoch": 123, "train_loss": 0.4204188060760498, "val_loss": 0.394530234336853, "val_acc": 0.865}, {"epoch": 124, "train_loss": 0.41924811244010923, "val_loss": 0.3934489393234253, "val_acc": 0.87}, {"epoch": 125, "train_loss": 0.42216920137405395, "val_loss": 0.39409934520721435, "val_acc": 0.855}, {"epoch": 126, "train_loss": 0.41946716904640197, "val_loss": 0.39399017572402956, "val_acc": 0.865}, {"epoch": 127, "train_loss": 0.41989227175712585, "val_loss": 0.3935039377212524, "val_acc": 0.87}, {"epoch": 128, "train_loss": 0.41931063055992124, "val_loss": 0.39435150861740115, "val_acc": 0.87}, {"epoch": 129, "train_loss": 0.4234549927711487, "val_loss": 0.39317241191864016, "val_acc": 0.87}, {"epoch": 130, "train_loss": 0.4213842213153839, "val_loss": 0.40024466037750245, "val_acc": 0.85}, {"epoch": 131, "train_loss": 0.4214139246940613, "val_loss": 0.3962597918510437, "val_acc": 0.865}, {"epoch": 132, "train_loss": 0.4195429277420044, "val_loss": 0.39293240308761596, "val_acc": 0.865}, {"epoch": 133, "train_loss": 0.4231331789493561, "val_loss": 0.39505051374435424, "val_acc": 0.865}, {"epoch": 134, "train_loss": 0.4196092486381531, "val_loss": 0.3932952785491943, "val_acc": 0.85}, {"epoch": 135, "train_loss": 0.417849440574646, "val_loss": 0.3932775616645813, "val_acc": 0.865}, {"epoch": 136, "train_loss": 0.41883262395858767, "val_loss": 0.39461074113845823, "val_acc": 0.86}, {"epoch": 137, "train_loss": 0.41737505316734314, "val_loss": 0.39277297496795655, "val_acc": 0.865}, {"epoch": 138, "train_loss": 0.417697948217392, "val_loss": 0.3929186749458313, "val_acc": 0.87}, {"epoch": 139, "train_loss": 0.41850183606147767, "val_loss": 0.3945894169807434, "val_acc": 0.865}, {"epoch": 140, "train_loss": 0.4177137565612793, "val_loss": 0.39301362276077273, "val_acc": 0.87}], "final_metrics": {"epoch": 140, "train_loss": 0.4177137565612793, "val_loss": 0.39301362276077273, "val_acc": 0.87}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 110), nn.ReLU(), nn.Linear(110, 55), nn.ReLU(), nn.Linear(55, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\nself.training_epochs = 120", "history": [{"epoch": 1, "train_loss": 1.0979422855377197, "val_loss": 1.0403318405151367, "val_acc": 0.41}, {"epoch": 2, "train_loss": 1.022863109111786, "val_loss": 0.9913738965988159, "val_acc": 0.39}, {"epoch": 3, "train_loss": 0.9896275472640991, "val_loss": 0.9666883754730224, "val_acc": 0.435}, {"epoch": 4, "train_loss": 0.9612098979949951, "val_loss": 0.9445019578933715, "val_acc": 0.49}, {"epoch": 5, "train_loss": 0.9316625142097473, "val_loss": 0.9176412010192871, "val_acc": 0.55}, {"epoch": 6, "train_loss": 0.9016001653671265, "val_loss": 0.887655565738678, "val_acc": 0.585}, {"epoch": 7, "train_loss": 0.8692540454864502, "val_loss": 0.858534197807312, "val_acc": 0.605}, {"epoch": 8, "train_loss": 0.8337570738792419, "val_loss": 0.8263257718086243, "val_acc": 0.64}, {"epoch": 9, "train_loss": 0.7997532510757446, "val_loss": 0.7911934995651245, "val_acc": 0.685}, {"epoch": 10, "train_loss": 0.7618175339698792, "val_loss": 0.7594149518013, "val_acc": 0.66}, {"epoch": 11, "train_loss": 0.7255305194854736, "val_loss": 0.7243139624595643, "val_acc": 0.7}, {"epoch": 12, "train_loss": 0.6913291978836059, "val_loss": 0.693661470413208, "val_acc": 0.7}, {"epoch": 13, "train_loss": 0.6596503448486328, "val_loss": 0.665835382938385, "val_acc": 0.7}, {"epoch": 14, "train_loss": 0.6311017036437988, "val_loss": 0.6362250447273254, "val_acc": 0.735}, {"epoch": 15, "train_loss": 0.6060321998596191, "val_loss": 0.6110677409172058, "val_acc": 0.74}, {"epoch": 16, "train_loss": 0.5819848680496216, "val_loss": 0.5920101428031921, "val_acc": 0.74}, {"epoch": 17, "train_loss": 0.5615399074554444, "val_loss": 0.5749040246009827, "val_acc": 0.75}, {"epoch": 18, "train_loss": 0.5462108016014099, "val_loss": 0.5590261673927307, "val_acc": 0.78}, {"epoch": 19, "train_loss": 0.5287082815170288, "val_loss": 0.551399085521698, "val_acc": 0.775}, {"epoch": 20, "train_loss": 0.5174763250350952, "val_loss": 0.5383180713653565, "val_acc": 0.785}, {"epoch": 21, "train_loss": 0.5046937608718872, "val_loss": 0.5327947545051575, "val_acc": 0.765}, {"epoch": 22, "train_loss": 0.4944071364402771, "val_loss": 0.5179293704032898, "val_acc": 0.79}, {"epoch": 23, "train_loss": 0.48460341453552247, "val_loss": 0.5135331916809082, "val_acc": 0.795}, {"epoch": 24, "train_loss": 0.47740841150283814, "val_loss": 0.5062290549278259, "val_acc": 0.795}, {"epoch": 25, "train_loss": 0.469236524105072, "val_loss": 0.5000643801689147, "val_acc": 0.785}, {"epoch": 26, "train_loss": 0.4650218665599823, "val_loss": 0.49611123323440554, "val_acc": 0.795}, {"epoch": 27, "train_loss": 0.4596149015426636, "val_loss": 0.4916794204711914, "val_acc": 0.79}, {"epoch": 28, "train_loss": 0.45441025018692016, "val_loss": 0.48848848581314086, "val_acc": 0.79}, {"epoch": 29, "train_loss": 0.4484641909599304, "val_loss": 0.48563767433166505, "val_acc": 0.805}, {"epoch": 30, "train_loss": 0.44697327733039854, "val_loss": 0.48206544399261475, "val_acc": 0.795}, {"epoch": 31, "train_loss": 0.4397400331497192, "val_loss": 0.48678563356399535, "val_acc": 0.785}, {"epoch": 32, "train_loss": 0.4400200963020325, "val_loss": 0.4790639424324036, "val_acc": 0.79}, {"epoch": 33, "train_loss": 0.4363912868499756, "val_loss": 0.47953144550323484, "val_acc": 0.785}, {"epoch": 34, "train_loss": 0.4333761441707611, "val_loss": 0.4782314133644104, "val_acc": 0.785}, {"epoch": 35, "train_loss": 0.43288169622421263, "val_loss": 0.4755680584907532, "val_acc": 0.805}, {"epoch": 36, "train_loss": 0.4315700829029083, "val_loss": 0.4744149422645569, "val_acc": 0.785}, {"epoch": 37, "train_loss": 0.426187561750412, "val_loss": 0.47149101495742796, "val_acc": 0.805}, {"epoch": 38, "train_loss": 0.4269804215431213, "val_loss": 0.4777204918861389, "val_acc": 0.8}, {"epoch": 39, "train_loss": 0.4258841609954834, "val_loss": 0.46418127059936526, "val_acc": 0.795}, {"epoch": 40, "train_loss": 0.42353365182876584, "val_loss": 0.46866979837417605, "val_acc": 0.78}, {"epoch": 41, "train_loss": 0.42133907437324525, "val_loss": 0.4696237134933472, "val_acc": 0.79}, {"epoch": 42, "train_loss": 0.4188234519958496, "val_loss": 0.47299000024795534, "val_acc": 0.78}, {"epoch": 43, "train_loss": 0.4192782163619995, "val_loss": 0.4687908673286438, "val_acc": 0.79}, {"epoch": 44, "train_loss": 0.42040638089179994, "val_loss": 0.46803887844085695, "val_acc": 0.8}, {"epoch": 45, "train_loss": 0.4171281671524048, "val_loss": 0.4700654125213623, "val_acc": 0.79}, {"epoch": 46, "train_loss": 0.4185915243625641, "val_loss": 0.46848914384841917, "val_acc": 0.785}, {"epoch": 47, "train_loss": 0.4151984679698944, "val_loss": 0.46781526327133177, "val_acc": 0.805}, {"epoch": 48, "train_loss": 0.4147526860237122, "val_loss": 0.4681882667541504, "val_acc": 0.785}, {"epoch": 49, "train_loss": 0.41238338112831113, "val_loss": 0.46711323261260984, "val_acc": 0.795}, {"epoch": 50, "train_loss": 0.41368084073066713, "val_loss": 0.46996590614318845, "val_acc": 0.805}, {"epoch": 51, "train_loss": 0.41564432621002195, "val_loss": 0.4714279365539551, "val_acc": 0.795}, {"epoch": 52, "train_loss": 0.4125367039442062, "val_loss": 0.46708802700042723, "val_acc": 0.795}, {"epoch": 53, "train_loss": 0.4116224706172943, "val_loss": 0.4702197527885437, "val_acc": 0.785}, {"epoch": 54, "train_loss": 0.40995828866958617, "val_loss": 0.4705886960029602, "val_acc": 0.79}, {"epoch": 55, "train_loss": 0.4118169236183167, "val_loss": 0.4687568211555481, "val_acc": 0.785}, {"epoch": 56, "train_loss": 0.4107008135318756, "val_loss": 0.47274562597274783, "val_acc": 0.79}, {"epoch": 57, "train_loss": 0.41031120300292967, "val_loss": 0.46517612218856813, "val_acc": 0.79}, {"epoch": 58, "train_loss": 0.40921801805496216, "val_loss": 0.47378165006637574, "val_acc": 0.785}, {"epoch": 59, "train_loss": 0.40931384921073916, "val_loss": 0.468652081489563, "val_acc": 0.79}, {"epoch": 60, "train_loss": 0.40896852254867555, "val_loss": 0.4696107220649719, "val_acc": 0.8}, {"epoch": 61, "train_loss": 0.4065055000782013, "val_loss": 0.4700290846824646, "val_acc": 0.8}, {"epoch": 62, "train_loss": 0.4080302846431732, "val_loss": 0.47076867580413817, "val_acc": 0.795}, {"epoch": 63, "train_loss": 0.4056766128540039, "val_loss": 0.47689871072769163, "val_acc": 0.785}, {"epoch": 64, "train_loss": 0.4055285108089447, "val_loss": 0.468326518535614, "val_acc": 0.8}, {"epoch": 65, "train_loss": 0.4111595869064331, "val_loss": 0.47210198640823364, "val_acc": 0.795}, {"epoch": 66, "train_loss": 0.4038728165626526, "val_loss": 0.47246732234954836, "val_acc": 0.785}, {"epoch": 67, "train_loss": 0.40339219450950625, "val_loss": 0.47186938762664793, "val_acc": 0.79}, {"epoch": 68, "train_loss": 0.40374900460243224, "val_loss": 0.4729698920249939, "val_acc": 0.79}, {"epoch": 69, "train_loss": 0.40401770114898683, "val_loss": 0.4698840355873108, "val_acc": 0.79}, {"epoch": 70, "train_loss": 0.40385698676109316, "val_loss": 0.4736251616477966, "val_acc": 0.79}, {"epoch": 71, "train_loss": 0.40410948753356934, "val_loss": 0.4717953443527222, "val_acc": 0.79}, {"epoch": 72, "train_loss": 0.4031278192996979, "val_loss": 0.4766017150878906, "val_acc": 0.79}, {"epoch": 73, "train_loss": 0.40711702466011046, "val_loss": 0.47368707180023195, "val_acc": 0.79}, {"epoch": 74, "train_loss": 0.4108716988563538, "val_loss": 0.4736995792388916, "val_acc": 0.79}, {"epoch": 75, "train_loss": 0.40706936836242674, "val_loss": 0.4702565097808838, "val_acc": 0.785}, {"epoch": 76, "train_loss": 0.4037257528305054, "val_loss": 0.4780872821807861, "val_acc": 0.8}, {"epoch": 77, "train_loss": 0.4001050794124603, "val_loss": 0.4739151167869568, "val_acc": 0.78}, {"epoch": 78, "train_loss": 0.40292893171310423, "val_loss": 0.4739511394500732, "val_acc": 0.79}, {"epoch": 79, "train_loss": 0.4016351664066315, "val_loss": 0.4774208569526672, "val_acc": 0.79}, {"epoch": 80, "train_loss": 0.4015986216068268, "val_loss": 0.4742627739906311, "val_acc": 0.79}, {"epoch": 81, "train_loss": 0.40074054956436156, "val_loss": 0.47447560548782347, "val_acc": 0.795}, {"epoch": 82, "train_loss": 0.40013047218322756, "val_loss": 0.47604768753051757, "val_acc": 0.79}, {"epoch": 83, "train_loss": 0.4008510839939117, "val_loss": 0.4741355299949646, "val_acc": 0.785}, {"epoch": 84, "train_loss": 0.40085879802703855, "val_loss": 0.4775632905960083, "val_acc": 0.79}, {"epoch": 85, "train_loss": 0.4000733995437622, "val_loss": 0.4791523385047913, "val_acc": 0.79}, {"epoch": 86, "train_loss": 0.4014514636993408, "val_loss": 0.47155303716659547, "val_acc": 0.795}, {"epoch": 87, "train_loss": 0.40110040426254273, "val_loss": 0.4782219409942627, "val_acc": 0.785}, {"epoch": 88, "train_loss": 0.4029567790031433, "val_loss": 0.48047816038131713, "val_acc": 0.795}, {"epoch": 89, "train_loss": 0.40092400431632996, "val_loss": 0.4745409393310547, "val_acc": 0.795}, {"epoch": 90, "train_loss": 0.40128177523612973, "val_loss": 0.4777946472167969, "val_acc": 0.785}, {"epoch": 91, "train_loss": 0.39917280435562136, "val_loss": 0.47289486646652223, "val_acc": 0.795}, {"epoch": 92, "train_loss": 0.3990431296825409, "val_loss": 0.47690420150756835, "val_acc": 0.79}, {"epoch": 93, "train_loss": 0.398903421163559, "val_loss": 0.47560853958129884, "val_acc": 0.795}, {"epoch": 94, "train_loss": 0.4009773516654968, "val_loss": 0.4752130150794983, "val_acc": 0.785}, {"epoch": 95, "train_loss": 0.3974984633922577, "val_loss": 0.48443032026290894, "val_acc": 0.79}, {"epoch": 96, "train_loss": 0.39985090017318725, "val_loss": 0.4811362075805664, "val_acc": 0.785}, {"epoch": 97, "train_loss": 0.40017690300941466, "val_loss": 0.4751423287391663, "val_acc": 0.795}, {"epoch": 98, "train_loss": 0.39898144602775576, "val_loss": 0.47834918975830076, "val_acc": 0.79}, {"epoch": 99, "train_loss": 0.3997726655006409, "val_loss": 0.48173266172409057, "val_acc": 0.78}, {"epoch": 100, "train_loss": 0.3990448784828186, "val_loss": 0.4755369091033936, "val_acc": 0.79}, {"epoch": 101, "train_loss": 0.3970633912086487, "val_loss": 0.47954402446746824, "val_acc": 0.795}, {"epoch": 102, "train_loss": 0.40057904958724977, "val_loss": 0.48058428049087526, "val_acc": 0.785}, {"epoch": 103, "train_loss": 0.39638954401016235, "val_loss": 0.47866608619689943, "val_acc": 0.785}, {"epoch": 104, "train_loss": 0.39805139899253844, "val_loss": 0.4799244475364685, "val_acc": 0.785}, {"epoch": 105, "train_loss": 0.39804686784744264, "val_loss": 0.4806451654434204, "val_acc": 0.8}, {"epoch": 106, "train_loss": 0.3975968587398529, "val_loss": 0.478525493144989, "val_acc": 0.79}, {"epoch": 107, "train_loss": 0.3969692635536194, "val_loss": 0.4783328413963318, "val_acc": 0.795}, {"epoch": 108, "train_loss": 0.397339414358139, "val_loss": 0.4830361104011536, "val_acc": 0.8}, {"epoch": 109, "train_loss": 0.39598300457000735, "val_loss": 0.47607115268707273, "val_acc": 0.79}, {"epoch": 110, "train_loss": 0.39642195582389833, "val_loss": 0.47729962587356567, "val_acc": 0.79}, {"epoch": 111, "train_loss": 0.39516640424728394, "val_loss": 0.48109025955200196, "val_acc": 0.785}, {"epoch": 112, "train_loss": 0.39459834814071654, "val_loss": 0.4750090718269348, "val_acc": 0.79}, {"epoch": 113, "train_loss": 0.3940121102333069, "val_loss": 0.48213244676589967, "val_acc": 0.785}, {"epoch": 114, "train_loss": 0.3955472707748413, "val_loss": 0.48152441024780274, "val_acc": 0.795}, {"epoch": 115, "train_loss": 0.39460652709007266, "val_loss": 0.478552885055542, "val_acc": 0.79}, {"epoch": 116, "train_loss": 0.3975681972503662, "val_loss": 0.48236937761306764, "val_acc": 0.785}, {"epoch": 117, "train_loss": 0.39652695894241335, "val_loss": 0.4776399302482605, "val_acc": 0.79}, {"epoch": 118, "train_loss": 0.39733013391494754, "val_loss": 0.4845954251289368, "val_acc": 0.8}, {"epoch": 119, "train_loss": 0.3962960028648376, "val_loss": 0.4785456562042236, "val_acc": 0.79}, {"epoch": 120, "train_loss": 0.3928726935386658, "val_loss": 0.48316403150558473, "val_acc": 0.785}], "final_metrics": {"epoch": 120, "train_loss": 0.3928726935386658, "val_loss": 0.48316403150558473, "val_acc": 0.785}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 90), nn.ReLU(), nn.Linear(90, 45), nn.ReLU(), nn.Linear(45, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.00035)\nself.training_epochs = 135", "history": [{"epoch": 1, "train_loss": 1.0767209720611572, "val_loss": 1.0730793142318726, "val_acc": 0.33}, {"epoch": 2, "train_loss": 1.0422165775299073, "val_loss": 1.0533458137512206, "val_acc": 0.315}, {"epoch": 3, "train_loss": 1.02049569606781, "val_loss": 1.0361093187332153, "val_acc": 0.31}, {"epoch": 4, "train_loss": 0.9979723429679871, "val_loss": 1.0127792167663574, "val_acc": 0.31}, {"epoch": 5, "train_loss": 0.9766299986839294, "val_loss": 0.9906177258491516, "val_acc": 0.35}, {"epoch": 6, "train_loss": 0.9535192775726319, "val_loss": 0.9634240198135376, "val_acc": 0.455}, {"epoch": 7, "train_loss": 0.9265771555900574, "val_loss": 0.936317036151886, "val_acc": 0.535}, {"epoch": 8, "train_loss": 0.8971801733970642, "val_loss": 0.9075378370285034, "val_acc": 0.585}, {"epoch": 9, "train_loss": 0.8653017377853394, "val_loss": 0.8747319269180298, "val_acc": 0.615}, {"epoch": 10, "train_loss": 0.8329396319389343, "val_loss": 0.8443988394737244, "val_acc": 0.645}, {"epoch": 11, "train_loss": 0.8002584171295166, "val_loss": 0.806043918132782, "val_acc": 0.665}, {"epoch": 12, "train_loss": 0.7650601100921631, "val_loss": 0.7728062248229981, "val_acc": 0.68}, {"epoch": 13, "train_loss": 0.7337220215797424, "val_loss": 0.743545470237732, "val_acc": 0.715}, {"epoch": 14, "train_loss": 0.7041466665267945, "val_loss": 0.7161617398262023, "val_acc": 0.715}, {"epoch": 15, "train_loss": 0.6776346707344055, "val_loss": 0.690027768611908, "val_acc": 0.725}, {"epoch": 16, "train_loss": 0.6523042511940003, "val_loss": 0.6629109883308411, "val_acc": 0.775}, {"epoch": 17, "train_loss": 0.6313909602165222, "val_loss": 0.640677855014801, "val_acc": 0.76}, {"epoch": 18, "train_loss": 0.6127304983139038, "val_loss": 0.6206641411781311, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.5928811120986939, "val_loss": 0.6030774903297424, "val_acc": 0.77}, {"epoch": 20, "train_loss": 0.5759649968147278, "val_loss": 0.5859246635437012, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.5615212750434876, "val_loss": 0.5717148303985595, "val_acc": 0.8}, {"epoch": 22, "train_loss": 0.5479676508903504, "val_loss": 0.554256420135498, "val_acc": 0.84}, {"epoch": 23, "train_loss": 0.536798894405365, "val_loss": 0.5425350856781006, "val_acc": 0.815}, {"epoch": 24, "train_loss": 0.5257006621360779, "val_loss": 0.5325853991508483, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.5163445806503296, "val_loss": 0.5224115276336669, "val_acc": 0.83}, {"epoch": 26, "train_loss": 0.5085131061077118, "val_loss": 0.5132401204109192, "val_acc": 0.855}, {"epoch": 27, "train_loss": 0.5000698840618134, "val_loss": 0.5049248468875885, "val_acc": 0.845}, {"epoch": 28, "train_loss": 0.4953140354156494, "val_loss": 0.4997386431694031, "val_acc": 0.85}, {"epoch": 29, "train_loss": 0.48783851623535157, "val_loss": 0.4919756734371185, "val_acc": 0.825}, {"epoch": 30, "train_loss": 0.4816549181938171, "val_loss": 0.48537843823432925, "val_acc": 0.85}, {"epoch": 31, "train_loss": 0.47621726274490356, "val_loss": 0.4794305324554443, "val_acc": 0.855}, {"epoch": 32, "train_loss": 0.4732691240310669, "val_loss": 0.4753677475452423, "val_acc": 0.845}, {"epoch": 33, "train_loss": 0.4681818974018097, "val_loss": 0.4706847548484802, "val_acc": 0.86}, {"epoch": 34, "train_loss": 0.46644967794418335, "val_loss": 0.46480595707893374, "val_acc": 0.86}, {"epoch": 35, "train_loss": 0.46145015954971313, "val_loss": 0.46198718428611757, "val_acc": 0.865}, {"epoch": 36, "train_loss": 0.4575176215171814, "val_loss": 0.4597507417201996, "val_acc": 0.85}, {"epoch": 37, "train_loss": 0.45742413520812986, "val_loss": 0.45694149255752564, "val_acc": 0.855}, {"epoch": 38, "train_loss": 0.452803316116333, "val_loss": 0.4525232577323914, "val_acc": 0.845}, {"epoch": 39, "train_loss": 0.4526948308944702, "val_loss": 0.45075157165527346, "val_acc": 0.87}, {"epoch": 40, "train_loss": 0.44914174437522886, "val_loss": 0.4485279822349548, "val_acc": 0.85}, {"epoch": 41, "train_loss": 0.4479939842224121, "val_loss": 0.44782166838645937, "val_acc": 0.85}, {"epoch": 42, "train_loss": 0.4455475163459778, "val_loss": 0.44517714500427247, "val_acc": 0.87}, {"epoch": 43, "train_loss": 0.44314191102981565, "val_loss": 0.4417670178413391, "val_acc": 0.855}, {"epoch": 44, "train_loss": 0.44243986129760743, "val_loss": 0.4400432074069977, "val_acc": 0.86}, {"epoch": 45, "train_loss": 0.4394218873977661, "val_loss": 0.4394734239578247, "val_acc": 0.865}, {"epoch": 46, "train_loss": 0.43836944699287417, "val_loss": 0.4378328800201416, "val_acc": 0.865}, {"epoch": 47, "train_loss": 0.4386396098136902, "val_loss": 0.4371533489227295, "val_acc": 0.845}, {"epoch": 48, "train_loss": 0.4365415441989899, "val_loss": 0.43507738947868346, "val_acc": 0.87}, {"epoch": 49, "train_loss": 0.43478477478027344, "val_loss": 0.43387948870658877, "val_acc": 0.86}, {"epoch": 50, "train_loss": 0.4332085371017456, "val_loss": 0.43511284947395323, "val_acc": 0.855}, {"epoch": 51, "train_loss": 0.4335540533065796, "val_loss": 0.4345464515686035, "val_acc": 0.86}, {"epoch": 52, "train_loss": 0.43178754091262816, "val_loss": 0.43331568479537963, "val_acc": 0.86}, {"epoch": 53, "train_loss": 0.4310761475563049, "val_loss": 0.43193037509918214, "val_acc": 0.865}, {"epoch": 54, "train_loss": 0.43022644996643067, "val_loss": 0.4314795982837677, "val_acc": 0.855}, {"epoch": 55, "train_loss": 0.4297364342212677, "val_loss": 0.4312209486961365, "val_acc": 0.85}, {"epoch": 56, "train_loss": 0.42865151166915894, "val_loss": 0.42922794818878174, "val_acc": 0.865}, {"epoch": 57, "train_loss": 0.4313914740085602, "val_loss": 0.4305735158920288, "val_acc": 0.865}, {"epoch": 58, "train_loss": 0.42788568139076233, "val_loss": 0.4290494632720947, "val_acc": 0.85}, {"epoch": 59, "train_loss": 0.4294696688652039, "val_loss": 0.42924888134002687, "val_acc": 0.845}, {"epoch": 60, "train_loss": 0.4275307631492615, "val_loss": 0.42747593760490415, "val_acc": 0.855}, {"epoch": 61, "train_loss": 0.4262975287437439, "val_loss": 0.42863873958587645, "val_acc": 0.85}, {"epoch": 62, "train_loss": 0.4265700078010559, "val_loss": 0.4278905642032623, "val_acc": 0.85}, {"epoch": 63, "train_loss": 0.42414039611816406, "val_loss": 0.42677897214889526, "val_acc": 0.85}, {"epoch": 64, "train_loss": 0.4246254086494446, "val_loss": 0.42609097957611086, "val_acc": 0.86}, {"epoch": 65, "train_loss": 0.42332078456878663, "val_loss": 0.4275246524810791, "val_acc": 0.865}, {"epoch": 66, "train_loss": 0.4234270453453064, "val_loss": 0.4272009313106537, "val_acc": 0.855}, {"epoch": 67, "train_loss": 0.42231218099594114, "val_loss": 0.42634756445884703, "val_acc": 0.865}, {"epoch": 68, "train_loss": 0.4239139842987061, "val_loss": 0.42607632994651795, "val_acc": 0.845}, {"epoch": 69, "train_loss": 0.4224327540397644, "val_loss": 0.4285377812385559, "val_acc": 0.845}, {"epoch": 70, "train_loss": 0.42159821271896364, "val_loss": 0.42540827751159666, "val_acc": 0.85}, {"epoch": 71, "train_loss": 0.4224649488925934, "val_loss": 0.4260177397727966, "val_acc": 0.84}, {"epoch": 72, "train_loss": 0.4228884887695312, "val_loss": 0.42445378422737123, "val_acc": 0.85}, {"epoch": 73, "train_loss": 0.42230417728424074, "val_loss": 0.426325865983963, "val_acc": 0.865}, {"epoch": 74, "train_loss": 0.420030198097229, "val_loss": 0.4253618955612183, "val_acc": 0.845}, {"epoch": 75, "train_loss": 0.42049177408218386, "val_loss": 0.4255968189239502, "val_acc": 0.86}, {"epoch": 76, "train_loss": 0.4199738121032715, "val_loss": 0.42660446882247927, "val_acc": 0.85}, {"epoch": 77, "train_loss": 0.42158012628555297, "val_loss": 0.42476642727851865, "val_acc": 0.86}, {"epoch": 78, "train_loss": 0.41999470829963687, "val_loss": 0.4254422962665558, "val_acc": 0.855}, {"epoch": 79, "train_loss": 0.4201505160331726, "val_loss": 0.42607410430908205, "val_acc": 0.855}, {"epoch": 80, "train_loss": 0.41895834922790526, "val_loss": 0.4266138243675232, "val_acc": 0.85}, {"epoch": 81, "train_loss": 0.4208587312698364, "val_loss": 0.4257155042886734, "val_acc": 0.835}, {"epoch": 82, "train_loss": 0.4170143270492554, "val_loss": 0.42707824289798735, "val_acc": 0.85}, {"epoch": 83, "train_loss": 0.4211568021774292, "val_loss": 0.42560730934143065, "val_acc": 0.86}, {"epoch": 84, "train_loss": 0.4188284361362457, "val_loss": 0.4267924749851227, "val_acc": 0.845}, {"epoch": 85, "train_loss": 0.4178782594203949, "val_loss": 0.42422918140888216, "val_acc": 0.86}, {"epoch": 86, "train_loss": 0.4174965953826904, "val_loss": 0.4259575408697128, "val_acc": 0.86}, {"epoch": 87, "train_loss": 0.41675460457801816, "val_loss": 0.4265344351530075, "val_acc": 0.86}, {"epoch": 88, "train_loss": 0.4185693430900574, "val_loss": 0.4275243926048279, "val_acc": 0.845}, {"epoch": 89, "train_loss": 0.4172958981990814, "val_loss": 0.4246184700727463, "val_acc": 0.86}, {"epoch": 90, "train_loss": 0.41844609975814817, "val_loss": 0.4253411841392517, "val_acc": 0.835}, {"epoch": 91, "train_loss": 0.4162669515609741, "val_loss": 0.425125344991684, "val_acc": 0.855}, {"epoch": 92, "train_loss": 0.41689040303230285, "val_loss": 0.4262688183784485, "val_acc": 0.855}, {"epoch": 93, "train_loss": 0.41702377557754516, "val_loss": 0.4268267369270325, "val_acc": 0.855}, {"epoch": 94, "train_loss": 0.4160302758216858, "val_loss": 0.42562866151332857, "val_acc": 0.835}, {"epoch": 95, "train_loss": 0.4158492410182953, "val_loss": 0.4243014121055603, "val_acc": 0.865}, {"epoch": 96, "train_loss": 0.416977516412735, "val_loss": 0.42729951739311217, "val_acc": 0.86}, {"epoch": 97, "train_loss": 0.41570253372192384, "val_loss": 0.4260857915878296, "val_acc": 0.845}, {"epoch": 98, "train_loss": 0.4152436947822571, "val_loss": 0.4275269567966461, "val_acc": 0.855}, {"epoch": 99, "train_loss": 0.41483587861061094, "val_loss": 0.4278072327375412, "val_acc": 0.845}, {"epoch": 100, "train_loss": 0.4149469518661499, "val_loss": 0.4265567010641098, "val_acc": 0.845}, {"epoch": 101, "train_loss": 0.41443307518959044, "val_loss": 0.4261291217803955, "val_acc": 0.85}, {"epoch": 102, "train_loss": 0.41575655341148376, "val_loss": 0.4248093295097351, "val_acc": 0.85}, {"epoch": 103, "train_loss": 0.41501556158065794, "val_loss": 0.42644999325275423, "val_acc": 0.855}, {"epoch": 104, "train_loss": 0.415781614780426, "val_loss": 0.42702944040298463, "val_acc": 0.855}, {"epoch": 105, "train_loss": 0.4145813870429993, "val_loss": 0.42687817454338073, "val_acc": 0.855}, {"epoch": 106, "train_loss": 0.41440771222114564, "val_loss": 0.4253070378303528, "val_acc": 0.85}, {"epoch": 107, "train_loss": 0.41547566175460815, "val_loss": 0.42646603465080263, "val_acc": 0.86}, {"epoch": 108, "train_loss": 0.4138381052017212, "val_loss": 0.42522564589977263, "val_acc": 0.845}, {"epoch": 109, "train_loss": 0.4142658078670502, "val_loss": 0.4281341302394867, "val_acc": 0.86}, {"epoch": 110, "train_loss": 0.4137689816951752, "val_loss": 0.42772199094295504, "val_acc": 0.845}, {"epoch": 111, "train_loss": 0.41719019412994385, "val_loss": 0.4290491199493408, "val_acc": 0.855}, {"epoch": 112, "train_loss": 0.4143096888065338, "val_loss": 0.4269713008403778, "val_acc": 0.845}, {"epoch": 113, "train_loss": 0.4132356071472168, "val_loss": 0.42750306785106656, "val_acc": 0.855}, {"epoch": 114, "train_loss": 0.4126544535160065, "val_loss": 0.42623872637748716, "val_acc": 0.86}, {"epoch": 115, "train_loss": 0.4171335792541504, "val_loss": 0.42527249693870545, "val_acc": 0.86}, {"epoch": 116, "train_loss": 0.4150953531265259, "val_loss": 0.4295617151260376, "val_acc": 0.835}, {"epoch": 117, "train_loss": 0.4122005462646484, "val_loss": 0.4259302747249603, "val_acc": 0.855}, {"epoch": 118, "train_loss": 0.4121441912651062, "val_loss": 0.42884127497673036, "val_acc": 0.85}, {"epoch": 119, "train_loss": 0.4119974946975708, "val_loss": 0.42789722681045533, "val_acc": 0.84}, {"epoch": 120, "train_loss": 0.41238474726676944, "val_loss": 0.42680885791778567, "val_acc": 0.855}, {"epoch": 121, "train_loss": 0.41355241417884825, "val_loss": 0.4276355242729187, "val_acc": 0.84}, {"epoch": 122, "train_loss": 0.4119421935081482, "val_loss": 0.42704065918922424, "val_acc": 0.86}, {"epoch": 123, "train_loss": 0.41171458959579466, "val_loss": 0.4268826508522034, "val_acc": 0.855}, {"epoch": 124, "train_loss": 0.41161973476409913, "val_loss": 0.42774122059345243, "val_acc": 0.835}, {"epoch": 125, "train_loss": 0.4132138013839722, "val_loss": 0.4274293124675751, "val_acc": 0.835}, {"epoch": 126, "train_loss": 0.412023583650589, "val_loss": 0.42809975147247314, "val_acc": 0.86}, {"epoch": 127, "train_loss": 0.4142597043514252, "val_loss": 0.42915090441703796, "val_acc": 0.86}, {"epoch": 128, "train_loss": 0.41163756370544435, "val_loss": 0.42775886714458466, "val_acc": 0.85}, {"epoch": 129, "train_loss": 0.4112728786468506, "val_loss": 0.4270861029624939, "val_acc": 0.845}, {"epoch": 130, "train_loss": 0.41088891625404356, "val_loss": 0.42592661678791044, "val_acc": 0.85}, {"epoch": 131, "train_loss": 0.41067068576812743, "val_loss": 0.42870551645755767, "val_acc": 0.855}, {"epoch": 132, "train_loss": 0.4118092942237854, "val_loss": 0.4284707593917847, "val_acc": 0.84}, {"epoch": 133, "train_loss": 0.41132023453712463, "val_loss": 0.4294003653526306, "val_acc": 0.85}, {"epoch": 134, "train_loss": 0.41123916864395144, "val_loss": 0.4285983073711395, "val_acc": 0.835}, {"epoch": 135, "train_loss": 0.4121480119228363, "val_loss": 0.4288682770729065, "val_acc": 0.855}], "final_metrics": {"epoch": 135, "train_loss": 0.4121480119228363, "val_loss": 0.4288682770729065, "val_acc": 0.855}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 85), nn.ReLU(), nn.Linear(85, 45), nn.ReLU(), nn.Linear(45, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.000375)\nself.training_epochs = 130", "history": [{"epoch": 1, "train_loss": 1.0756847047805786, "val_loss": 1.0397956991195678, "val_acc": 0.38}, {"epoch": 2, "train_loss": 1.0262515830993653, "val_loss": 1.0070442485809326, "val_acc": 0.335}, {"epoch": 3, "train_loss": 0.9960217523574829, "val_loss": 0.9826402282714843, "val_acc": 0.39}, {"epoch": 4, "train_loss": 0.9724202346801758, "val_loss": 0.9578174638748169, "val_acc": 0.46}, {"epoch": 5, "train_loss": 0.9491056609153747, "val_loss": 0.9335575866699218, "val_acc": 0.495}, {"epoch": 6, "train_loss": 0.9260211396217346, "val_loss": 0.9074597072601318, "val_acc": 0.51}, {"epoch": 7, "train_loss": 0.9014029812812805, "val_loss": 0.8820092344284057, "val_acc": 0.62}, {"epoch": 8, "train_loss": 0.8758465933799744, "val_loss": 0.8547793912887574, "val_acc": 0.635}, {"epoch": 9, "train_loss": 0.8495109057426453, "val_loss": 0.8263304471969605, "val_acc": 0.68}, {"epoch": 10, "train_loss": 0.8217650103569031, "val_loss": 0.7967199850082397, "val_acc": 0.7}, {"epoch": 11, "train_loss": 0.793985800743103, "val_loss": 0.76882399559021, "val_acc": 0.72}, {"epoch": 12, "train_loss": 0.7643219351768493, "val_loss": 0.739611611366272, "val_acc": 0.73}, {"epoch": 13, "train_loss": 0.7363814210891724, "val_loss": 0.7112673687934875, "val_acc": 0.76}, {"epoch": 14, "train_loss": 0.7093369889259339, "val_loss": 0.6824459171295166, "val_acc": 0.76}, {"epoch": 15, "train_loss": 0.6838329744338989, "val_loss": 0.6563120675086975, "val_acc": 0.775}, {"epoch": 16, "train_loss": 0.6608082938194275, "val_loss": 0.6336421322822571, "val_acc": 0.79}, {"epoch": 17, "train_loss": 0.6383610105514527, "val_loss": 0.6114264488220215, "val_acc": 0.79}, {"epoch": 18, "train_loss": 0.6189165663719177, "val_loss": 0.591252682209015, "val_acc": 0.795}, {"epoch": 19, "train_loss": 0.6020811891555786, "val_loss": 0.5750736021995544, "val_acc": 0.775}, {"epoch": 20, "train_loss": 0.5855500984191895, "val_loss": 0.5559738397598266, "val_acc": 0.785}, {"epoch": 21, "train_loss": 0.5698696327209473, "val_loss": 0.5396716356277466, "val_acc": 0.8}, {"epoch": 22, "train_loss": 0.5577789521217347, "val_loss": 0.5244110608100891, "val_acc": 0.83}, {"epoch": 23, "train_loss": 0.5454643940925599, "val_loss": 0.5148861718177795, "val_acc": 0.805}, {"epoch": 24, "train_loss": 0.5364066636562348, "val_loss": 0.5002768182754517, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.526612777709961, "val_loss": 0.4938187980651855, "val_acc": 0.815}, {"epoch": 26, "train_loss": 0.5178359508514404, "val_loss": 0.47929044723510744, "val_acc": 0.855}, {"epoch": 27, "train_loss": 0.5104748165607452, "val_loss": 0.473615825176239, "val_acc": 0.84}, {"epoch": 28, "train_loss": 0.503947377204895, "val_loss": 0.46429498195648194, "val_acc": 0.865}, {"epoch": 29, "train_loss": 0.49605844259262083, "val_loss": 0.45595311164855956, "val_acc": 0.855}, {"epoch": 30, "train_loss": 0.4906440663337708, "val_loss": 0.4498991870880127, "val_acc": 0.855}, {"epoch": 31, "train_loss": 0.48644433975219725, "val_loss": 0.4443966245651245, "val_acc": 0.855}, {"epoch": 32, "train_loss": 0.48127354502677916, "val_loss": 0.4353011918067932, "val_acc": 0.87}, {"epoch": 33, "train_loss": 0.47858631253242495, "val_loss": 0.4349892735481262, "val_acc": 0.86}, {"epoch": 34, "train_loss": 0.47304125428199767, "val_loss": 0.42711213827133176, "val_acc": 0.865}, {"epoch": 35, "train_loss": 0.4694972920417786, "val_loss": 0.4218956446647644, "val_acc": 0.88}, {"epoch": 36, "train_loss": 0.4662931978702545, "val_loss": 0.4176033091545105, "val_acc": 0.865}, {"epoch": 37, "train_loss": 0.46517009973526, "val_loss": 0.41434991359710693, "val_acc": 0.86}, {"epoch": 38, "train_loss": 0.46085023403167724, "val_loss": 0.4115324521064758, "val_acc": 0.88}, {"epoch": 39, "train_loss": 0.457870854139328, "val_loss": 0.4079110622406006, "val_acc": 0.865}, {"epoch": 40, "train_loss": 0.45844545841217044, "val_loss": 0.4020510578155518, "val_acc": 0.865}, {"epoch": 41, "train_loss": 0.4553981912136078, "val_loss": 0.40160264492034914, "val_acc": 0.875}, {"epoch": 42, "train_loss": 0.45445356011390686, "val_loss": 0.39699405431747437, "val_acc": 0.87}, {"epoch": 43, "train_loss": 0.4527101695537567, "val_loss": 0.3957277035713196, "val_acc": 0.875}, {"epoch": 44, "train_loss": 0.45236869096755983, "val_loss": 0.3939206099510193, "val_acc": 0.875}, {"epoch": 45, "train_loss": 0.4487354516983032, "val_loss": 0.3937863922119141, "val_acc": 0.865}, {"epoch": 46, "train_loss": 0.4473030912876129, "val_loss": 0.3890600609779358, "val_acc": 0.865}, {"epoch": 47, "train_loss": 0.44687455892562866, "val_loss": 0.3870986342430115, "val_acc": 0.875}, {"epoch": 48, "train_loss": 0.4455111610889435, "val_loss": 0.39407326221466066, "val_acc": 0.85}, {"epoch": 49, "train_loss": 0.444606831073761, "val_loss": 0.38754235982894897, "val_acc": 0.87}, {"epoch": 50, "train_loss": 0.4431260049343109, "val_loss": 0.3828339695930481, "val_acc": 0.875}, {"epoch": 51, "train_loss": 0.4426180028915405, "val_loss": 0.38241743326187133, "val_acc": 0.865}, {"epoch": 52, "train_loss": 0.4458625054359436, "val_loss": 0.38039746761322024, "val_acc": 0.855}, {"epoch": 53, "train_loss": 0.4421180236339569, "val_loss": 0.38244211196899414, "val_acc": 0.875}, {"epoch": 54, "train_loss": 0.44117108345031736, "val_loss": 0.38452893018722534, "val_acc": 0.855}, {"epoch": 55, "train_loss": 0.4416925513744354, "val_loss": 0.3769547009468079, "val_acc": 0.87}, {"epoch": 56, "train_loss": 0.44256502389907837, "val_loss": 0.3816762351989746, "val_acc": 0.88}, {"epoch": 57, "train_loss": 0.4413365113735199, "val_loss": 0.3731840944290161, "val_acc": 0.87}, {"epoch": 58, "train_loss": 0.4395615780353546, "val_loss": 0.37215904712677, "val_acc": 0.865}, {"epoch": 59, "train_loss": 0.43766104221343993, "val_loss": 0.37741554260253907, "val_acc": 0.86}, {"epoch": 60, "train_loss": 0.4368751847743988, "val_loss": 0.37550915718078615, "val_acc": 0.86}, {"epoch": 61, "train_loss": 0.43613812685012815, "val_loss": 0.3737707829475403, "val_acc": 0.865}, {"epoch": 62, "train_loss": 0.4368979275226593, "val_loss": 0.3730996012687683, "val_acc": 0.87}, {"epoch": 63, "train_loss": 0.43623088955879213, "val_loss": 0.3708070397377014, "val_acc": 0.865}, {"epoch": 64, "train_loss": 0.43782341837882993, "val_loss": 0.3742541718482971, "val_acc": 0.865}, {"epoch": 65, "train_loss": 0.43509862780570985, "val_loss": 0.3753505706787109, "val_acc": 0.855}, {"epoch": 66, "train_loss": 0.43490999698638916, "val_loss": 0.37009004831314085, "val_acc": 0.87}, {"epoch": 67, "train_loss": 0.4346983098983765, "val_loss": 0.36739152669906616, "val_acc": 0.875}, {"epoch": 68, "train_loss": 0.4332653713226318, "val_loss": 0.37299739360809325, "val_acc": 0.86}, {"epoch": 69, "train_loss": 0.4341447126865387, "val_loss": 0.3678806519508362, "val_acc": 0.87}, {"epoch": 70, "train_loss": 0.4329692089557648, "val_loss": 0.3689184808731079, "val_acc": 0.87}, {"epoch": 71, "train_loss": 0.4343220567703247, "val_loss": 0.36861783027648926, "val_acc": 0.865}, {"epoch": 72, "train_loss": 0.4316029953956604, "val_loss": 0.37093433380126956, "val_acc": 0.86}, {"epoch": 73, "train_loss": 0.432298469543457, "val_loss": 0.3629180932044983, "val_acc": 0.865}, {"epoch": 74, "train_loss": 0.43127391338348386, "val_loss": 0.3652800726890564, "val_acc": 0.865}, {"epoch": 75, "train_loss": 0.43166566908359527, "val_loss": 0.36728668451309204, "val_acc": 0.87}, {"epoch": 76, "train_loss": 0.431944637298584, "val_loss": 0.361448290348053, "val_acc": 0.865}, {"epoch": 77, "train_loss": 0.43075146913528445, "val_loss": 0.3681955337524414, "val_acc": 0.86}, {"epoch": 78, "train_loss": 0.43148717761039734, "val_loss": 0.3667286729812622, "val_acc": 0.86}, {"epoch": 79, "train_loss": 0.4297431683540344, "val_loss": 0.3625416660308838, "val_acc": 0.87}, {"epoch": 80, "train_loss": 0.4306292974948883, "val_loss": 0.3611582183837891, "val_acc": 0.87}, {"epoch": 81, "train_loss": 0.4289507532119751, "val_loss": 0.36307180166244507, "val_acc": 0.87}, {"epoch": 82, "train_loss": 0.4296851634979248, "val_loss": 0.3636529207229614, "val_acc": 0.865}, {"epoch": 83, "train_loss": 0.42859172224998476, "val_loss": 0.3652682971954346, "val_acc": 0.86}, {"epoch": 84, "train_loss": 0.4295729303359985, "val_loss": 0.3639053773880005, "val_acc": 0.86}, {"epoch": 85, "train_loss": 0.4296590328216553, "val_loss": 0.363084135055542, "val_acc": 0.86}, {"epoch": 86, "train_loss": 0.4306706690788269, "val_loss": 0.35842389583587647, "val_acc": 0.86}, {"epoch": 87, "train_loss": 0.4290158176422119, "val_loss": 0.3597801876068115, "val_acc": 0.87}, {"epoch": 88, "train_loss": 0.4295332133769989, "val_loss": 0.35961141586303713, "val_acc": 0.87}, {"epoch": 89, "train_loss": 0.4277066683769226, "val_loss": 0.36185718297958375, "val_acc": 0.87}, {"epoch": 90, "train_loss": 0.4275879597663879, "val_loss": 0.359580979347229, "val_acc": 0.865}, {"epoch": 91, "train_loss": 0.4275514793395996, "val_loss": 0.3627642798423767, "val_acc": 0.865}, {"epoch": 92, "train_loss": 0.4279260313510895, "val_loss": 0.36088975429534914, "val_acc": 0.86}, {"epoch": 93, "train_loss": 0.4271565937995911, "val_loss": 0.36521645784378054, "val_acc": 0.86}, {"epoch": 94, "train_loss": 0.42667443633079527, "val_loss": 0.3651160955429077, "val_acc": 0.86}, {"epoch": 95, "train_loss": 0.4291219699382782, "val_loss": 0.359832923412323, "val_acc": 0.865}, {"epoch": 96, "train_loss": 0.4280479621887207, "val_loss": 0.3627203130722046, "val_acc": 0.86}, {"epoch": 97, "train_loss": 0.4293697798252106, "val_loss": 0.35835965633392336, "val_acc": 0.86}, {"epoch": 98, "train_loss": 0.4266515851020813, "val_loss": 0.3604771614074707, "val_acc": 0.87}, {"epoch": 99, "train_loss": 0.4267158049345017, "val_loss": 0.3599920344352722, "val_acc": 0.86}, {"epoch": 100, "train_loss": 0.42664372324943545, "val_loss": 0.36327946186065674, "val_acc": 0.865}, {"epoch": 101, "train_loss": 0.4269004917144775, "val_loss": 0.3628973364830017, "val_acc": 0.865}, {"epoch": 102, "train_loss": 0.4254128789901733, "val_loss": 0.36236732482910156, "val_acc": 0.845}, {"epoch": 103, "train_loss": 0.42670750617980957, "val_loss": 0.3611337804794312, "val_acc": 0.86}, {"epoch": 104, "train_loss": 0.4261855363845825, "val_loss": 0.3597351408004761, "val_acc": 0.865}, {"epoch": 105, "train_loss": 0.42753964543342593, "val_loss": 0.3593368363380432, "val_acc": 0.855}, {"epoch": 106, "train_loss": 0.42516517758369443, "val_loss": 0.3605251860618591, "val_acc": 0.865}, {"epoch": 107, "train_loss": 0.42729023218154905, "val_loss": 0.3578358554840088, "val_acc": 0.86}, {"epoch": 108, "train_loss": 0.42464697241783145, "val_loss": 0.3579703283309936, "val_acc": 0.87}, {"epoch": 109, "train_loss": 0.42541921615600586, "val_loss": 0.3545723605155945, "val_acc": 0.865}, {"epoch": 110, "train_loss": 0.4265167462825775, "val_loss": 0.3609513425827026, "val_acc": 0.855}, {"epoch": 111, "train_loss": 0.4257721567153931, "val_loss": 0.35414926052093504, "val_acc": 0.865}, {"epoch": 112, "train_loss": 0.42392834186553957, "val_loss": 0.3598313331604004, "val_acc": 0.865}, {"epoch": 113, "train_loss": 0.424322829246521, "val_loss": 0.3624216818809509, "val_acc": 0.86}, {"epoch": 114, "train_loss": 0.42423105001449585, "val_loss": 0.3560065269470215, "val_acc": 0.86}, {"epoch": 115, "train_loss": 0.4256682538986206, "val_loss": 0.3578282380104065, "val_acc": 0.87}, {"epoch": 116, "train_loss": 0.424167777299881, "val_loss": 0.3609668731689453, "val_acc": 0.85}, {"epoch": 117, "train_loss": 0.42440078377723695, "val_loss": 0.35996355056762697, "val_acc": 0.855}, {"epoch": 118, "train_loss": 0.4267926836013794, "val_loss": 0.3565576410293579, "val_acc": 0.86}, {"epoch": 119, "train_loss": 0.4235697364807129, "val_loss": 0.36248796463012695, "val_acc": 0.86}, {"epoch": 120, "train_loss": 0.42449778914451597, "val_loss": 0.3568482780456543, "val_acc": 0.86}, {"epoch": 121, "train_loss": 0.4250535011291504, "val_loss": 0.3545677161216736, "val_acc": 0.865}, {"epoch": 122, "train_loss": 0.42383169412612914, "val_loss": 0.36063652992248535, "val_acc": 0.865}, {"epoch": 123, "train_loss": 0.42282318353652953, "val_loss": 0.3565356636047363, "val_acc": 0.855}, {"epoch": 124, "train_loss": 0.4223870640993118, "val_loss": 0.3599515914916992, "val_acc": 0.85}, {"epoch": 125, "train_loss": 0.4229096722602844, "val_loss": 0.3603989553451538, "val_acc": 0.85}, {"epoch": 126, "train_loss": 0.4236047637462616, "val_loss": 0.3574677610397339, "val_acc": 0.86}, {"epoch": 127, "train_loss": 0.4242963266372681, "val_loss": 0.35776175022125245, "val_acc": 0.865}, {"epoch": 128, "train_loss": 0.42269509434700014, "val_loss": 0.35606074810028077, "val_acc": 0.86}, {"epoch": 129, "train_loss": 0.42238876700401307, "val_loss": 0.35566417932510375, "val_acc": 0.86}, {"epoch": 130, "train_loss": 0.4224506664276123, "val_loss": 0.35767600059509275, "val_acc": 0.86}], "final_metrics": {"epoch": 130, "train_loss": 0.4224506664276123, "val_loss": 0.35767600059509275, "val_acc": 0.86}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.00035)\nself.training_epochs = 130", "history": [{"epoch": 1, "train_loss": 1.0850245141983033, "val_loss": 1.0539588046073913, "val_acc": 0.395}, {"epoch": 2, "train_loss": 1.0279401278495788, "val_loss": 1.016025619506836, "val_acc": 0.345}, {"epoch": 3, "train_loss": 0.9977662086486816, "val_loss": 0.9869803261756896, "val_acc": 0.36}, {"epoch": 4, "train_loss": 0.973535099029541, "val_loss": 0.9609779953956604, "val_acc": 0.41}, {"epoch": 5, "train_loss": 0.9458212423324585, "val_loss": 0.9396482110023499, "val_acc": 0.47}, {"epoch": 6, "train_loss": 0.9167886853218079, "val_loss": 0.914598069190979, "val_acc": 0.52}, {"epoch": 7, "train_loss": 0.8855202078819275, "val_loss": 0.8860017681121826, "val_acc": 0.57}, {"epoch": 8, "train_loss": 0.8531488537788391, "val_loss": 0.8552757143974304, "val_acc": 0.615}, {"epoch": 9, "train_loss": 0.8191421890258789, "val_loss": 0.8262220215797424, "val_acc": 0.66}, {"epoch": 10, "train_loss": 0.7825997710227967, "val_loss": 0.7932639169692993, "val_acc": 0.64}, {"epoch": 11, "train_loss": 0.7480586671829224, "val_loss": 0.7603422045707703, "val_acc": 0.71}, {"epoch": 12, "train_loss": 0.7152960705757141, "val_loss": 0.7331205129623413, "val_acc": 0.7}, {"epoch": 13, "train_loss": 0.6861313462257386, "val_loss": 0.707150673866272, "val_acc": 0.73}, {"epoch": 14, "train_loss": 0.6578346371650696, "val_loss": 0.6851612377166748, "val_acc": 0.715}, {"epoch": 15, "train_loss": 0.6323174750804901, "val_loss": 0.6594350862503052, "val_acc": 0.76}, {"epoch": 16, "train_loss": 0.6114755272865295, "val_loss": 0.6401454424858093, "val_acc": 0.765}, {"epoch": 17, "train_loss": 0.59159499168396, "val_loss": 0.6223382067680359, "val_acc": 0.755}, {"epoch": 18, "train_loss": 0.5723455023765563, "val_loss": 0.6054463577270508, "val_acc": 0.795}, {"epoch": 19, "train_loss": 0.5567579913139343, "val_loss": 0.5868437671661377, "val_acc": 0.77}, {"epoch": 20, "train_loss": 0.543292052745819, "val_loss": 0.5731534290313721, "val_acc": 0.775}, {"epoch": 21, "train_loss": 0.5291845107078552, "val_loss": 0.5639944863319397, "val_acc": 0.77}, {"epoch": 22, "train_loss": 0.5199987077713013, "val_loss": 0.5485871148109436, "val_acc": 0.805}, {"epoch": 23, "train_loss": 0.5092535674571991, "val_loss": 0.5406384158134461, "val_acc": 0.805}, {"epoch": 24, "train_loss": 0.5005906367301941, "val_loss": 0.5300301456451416, "val_acc": 0.815}, {"epoch": 25, "train_loss": 0.4923942530155182, "val_loss": 0.5232545423507691, "val_acc": 0.81}, {"epoch": 26, "train_loss": 0.4870696783065796, "val_loss": 0.5159013760089874, "val_acc": 0.815}, {"epoch": 27, "train_loss": 0.4802173125743866, "val_loss": 0.505256736278534, "val_acc": 0.82}, {"epoch": 28, "train_loss": 0.4740486264228821, "val_loss": 0.5045376992225648, "val_acc": 0.81}, {"epoch": 29, "train_loss": 0.4674252223968506, "val_loss": 0.4966114544868469, "val_acc": 0.825}, {"epoch": 30, "train_loss": 0.4620351493358612, "val_loss": 0.4908142292499542, "val_acc": 0.81}, {"epoch": 31, "train_loss": 0.4587166619300842, "val_loss": 0.487725111246109, "val_acc": 0.81}, {"epoch": 32, "train_loss": 0.4572367203235626, "val_loss": 0.4846048069000244, "val_acc": 0.81}, {"epoch": 33, "train_loss": 0.45189947009086606, "val_loss": 0.47975933432579043, "val_acc": 0.825}, {"epoch": 34, "train_loss": 0.44973442316055295, "val_loss": 0.47392081141471865, "val_acc": 0.825}, {"epoch": 35, "train_loss": 0.44712526321411133, "val_loss": 0.4707911348342895, "val_acc": 0.83}, {"epoch": 36, "train_loss": 0.44201782703399656, "val_loss": 0.47611560344696047, "val_acc": 0.795}, {"epoch": 37, "train_loss": 0.441233377456665, "val_loss": 0.46704490184783937, "val_acc": 0.825}, {"epoch": 38, "train_loss": 0.44209972143173215, "val_loss": 0.4646417045593262, "val_acc": 0.815}, {"epoch": 39, "train_loss": 0.439241281747818, "val_loss": 0.46694657802581785, "val_acc": 0.815}, {"epoch": 40, "train_loss": 0.43562801122665407, "val_loss": 0.45886956334114076, "val_acc": 0.835}, {"epoch": 41, "train_loss": 0.4346104836463928, "val_loss": 0.46141255855560304, "val_acc": 0.815}, {"epoch": 42, "train_loss": 0.43356279373168943, "val_loss": 0.4570835256576538, "val_acc": 0.825}, {"epoch": 43, "train_loss": 0.43198112964630125, "val_loss": 0.4556839728355408, "val_acc": 0.825}, {"epoch": 44, "train_loss": 0.42952625632286073, "val_loss": 0.45253161907196043, "val_acc": 0.825}, {"epoch": 45, "train_loss": 0.42910430431365965, "val_loss": 0.45660811781883237, "val_acc": 0.82}, {"epoch": 46, "train_loss": 0.42644845843315127, "val_loss": 0.4507168757915497, "val_acc": 0.82}, {"epoch": 47, "train_loss": 0.4284980297088623, "val_loss": 0.454112811088562, "val_acc": 0.815}, {"epoch": 48, "train_loss": 0.4305151450634003, "val_loss": 0.4525071668624878, "val_acc": 0.825}, {"epoch": 49, "train_loss": 0.42719361901283265, "val_loss": 0.44887434840202334, "val_acc": 0.82}, {"epoch": 50, "train_loss": 0.4263396179676056, "val_loss": 0.45362728118896484, "val_acc": 0.825}, {"epoch": 51, "train_loss": 0.4241717302799225, "val_loss": 0.44632815837860107, "val_acc": 0.825}, {"epoch": 52, "train_loss": 0.42326475977897643, "val_loss": 0.44677157282829283, "val_acc": 0.82}, {"epoch": 53, "train_loss": 0.42334131002426145, "val_loss": 0.4470300006866455, "val_acc": 0.825}, {"epoch": 54, "train_loss": 0.421720404624939, "val_loss": 0.4496789026260376, "val_acc": 0.82}, {"epoch": 55, "train_loss": 0.4216273188591003, "val_loss": 0.4459469246864319, "val_acc": 0.825}, {"epoch": 56, "train_loss": 0.4201896977424622, "val_loss": 0.44517876863479616, "val_acc": 0.825}, {"epoch": 57, "train_loss": 0.4204888319969177, "val_loss": 0.4460251641273498, "val_acc": 0.815}, {"epoch": 58, "train_loss": 0.4196992862224579, "val_loss": 0.4441883623600006, "val_acc": 0.825}, {"epoch": 59, "train_loss": 0.4187296724319458, "val_loss": 0.4441239106655121, "val_acc": 0.825}, {"epoch": 60, "train_loss": 0.41830586433410644, "val_loss": 0.4476786386966705, "val_acc": 0.825}, {"epoch": 61, "train_loss": 0.41826935052871705, "val_loss": 0.445303453207016, "val_acc": 0.825}, {"epoch": 62, "train_loss": 0.41877066493034365, "val_loss": 0.44352665066719055, "val_acc": 0.825}, {"epoch": 63, "train_loss": 0.417546466588974, "val_loss": 0.44385003566741943, "val_acc": 0.82}, {"epoch": 64, "train_loss": 0.41780011892318725, "val_loss": 0.4450588548183441, "val_acc": 0.825}, {"epoch": 65, "train_loss": 0.4175808393955231, "val_loss": 0.44574292898178103, "val_acc": 0.82}, {"epoch": 66, "train_loss": 0.4174238431453705, "val_loss": 0.4418638014793396, "val_acc": 0.82}, {"epoch": 67, "train_loss": 0.4156692218780518, "val_loss": 0.44179860591888426, "val_acc": 0.825}, {"epoch": 68, "train_loss": 0.41582828283309936, "val_loss": 0.4426499044895172, "val_acc": 0.815}, {"epoch": 69, "train_loss": 0.4172177505493164, "val_loss": 0.44137311816215513, "val_acc": 0.825}, {"epoch": 70, "train_loss": 0.41644610166549684, "val_loss": 0.4452847754955292, "val_acc": 0.82}, {"epoch": 71, "train_loss": 0.4160319828987122, "val_loss": 0.4419796669483185, "val_acc": 0.825}, {"epoch": 72, "train_loss": 0.4159969663619995, "val_loss": 0.44183409452438355, "val_acc": 0.82}, {"epoch": 73, "train_loss": 0.4148971390724182, "val_loss": 0.441104838848114, "val_acc": 0.82}, {"epoch": 74, "train_loss": 0.41571892142295835, "val_loss": 0.44244975090026856, "val_acc": 0.825}, {"epoch": 75, "train_loss": 0.4182008445262909, "val_loss": 0.4441803741455078, "val_acc": 0.81}, {"epoch": 76, "train_loss": 0.4138998031616211, "val_loss": 0.4425457286834717, "val_acc": 0.825}, {"epoch": 77, "train_loss": 0.41337990045547485, "val_loss": 0.44038495421409607, "val_acc": 0.825}, {"epoch": 78, "train_loss": 0.4133064389228821, "val_loss": 0.44249475479125977, "val_acc": 0.82}, {"epoch": 79, "train_loss": 0.41417751789093016, "val_loss": 0.446627858877182, "val_acc": 0.82}, {"epoch": 80, "train_loss": 0.4131951832771301, "val_loss": 0.4402836465835571, "val_acc": 0.825}, {"epoch": 81, "train_loss": 0.4138750636577606, "val_loss": 0.4424475371837616, "val_acc": 0.82}, {"epoch": 82, "train_loss": 0.412393319606781, "val_loss": 0.4404945993423462, "val_acc": 0.825}, {"epoch": 83, "train_loss": 0.41448050260543823, "val_loss": 0.44065857291221616, "val_acc": 0.825}, {"epoch": 84, "train_loss": 0.4114405596256256, "val_loss": 0.4452304148674011, "val_acc": 0.82}, {"epoch": 85, "train_loss": 0.4123499476909637, "val_loss": 0.4388770425319672, "val_acc": 0.825}, {"epoch": 86, "train_loss": 0.41124118328094483, "val_loss": 0.441052006483078, "val_acc": 0.825}, {"epoch": 87, "train_loss": 0.41160861372947694, "val_loss": 0.44330897808074954, "val_acc": 0.825}, {"epoch": 88, "train_loss": 0.41238282442092894, "val_loss": 0.4397305941581726, "val_acc": 0.825}, {"epoch": 89, "train_loss": 0.4117863440513611, "val_loss": 0.44417726635932925, "val_acc": 0.825}, {"epoch": 90, "train_loss": 0.4103663992881775, "val_loss": 0.44129699349403384, "val_acc": 0.825}, {"epoch": 91, "train_loss": 0.41129921317100526, "val_loss": 0.43958757400512694, "val_acc": 0.825}, {"epoch": 92, "train_loss": 0.414032518863678, "val_loss": 0.4395638334751129, "val_acc": 0.825}, {"epoch": 93, "train_loss": 0.41544580936431885, "val_loss": 0.44148221373558044, "val_acc": 0.82}, {"epoch": 94, "train_loss": 0.4108120596408844, "val_loss": 0.44426429629325864, "val_acc": 0.825}, {"epoch": 95, "train_loss": 0.41040012836456297, "val_loss": 0.43991316556930543, "val_acc": 0.825}, {"epoch": 96, "train_loss": 0.40936134099960325, "val_loss": 0.4374951958656311, "val_acc": 0.82}, {"epoch": 97, "train_loss": 0.40978079438209536, "val_loss": 0.44320754170417787, "val_acc": 0.82}, {"epoch": 98, "train_loss": 0.4098315644264221, "val_loss": 0.438711644411087, "val_acc": 0.825}, {"epoch": 99, "train_loss": 0.4094206404685974, "val_loss": 0.4447831201553345, "val_acc": 0.815}, {"epoch": 100, "train_loss": 0.40925866007804873, "val_loss": 0.44067853450775146, "val_acc": 0.82}, {"epoch": 101, "train_loss": 0.40954631328582763, "val_loss": 0.44258355259895327, "val_acc": 0.82}, {"epoch": 102, "train_loss": 0.41004878282546997, "val_loss": 0.43696357250213624, "val_acc": 0.82}, {"epoch": 103, "train_loss": 0.4097347629070282, "val_loss": 0.4440579295158386, "val_acc": 0.82}, {"epoch": 104, "train_loss": 0.40833860754966733, "val_loss": 0.4402134168148041, "val_acc": 0.82}, {"epoch": 105, "train_loss": 0.41015285372734067, "val_loss": 0.4430578279495239, "val_acc": 0.835}, {"epoch": 106, "train_loss": 0.40871169209480285, "val_loss": 0.44172348856925964, "val_acc": 0.83}, {"epoch": 107, "train_loss": 0.40849204540252687, "val_loss": 0.44110069155693055, "val_acc": 0.825}, {"epoch": 108, "train_loss": 0.40789452075958255, "val_loss": 0.44329296708106997, "val_acc": 0.825}, {"epoch": 109, "train_loss": 0.4100367885828018, "val_loss": 0.44441970586776736, "val_acc": 0.82}, {"epoch": 110, "train_loss": 0.40897135734558104, "val_loss": 0.4441967821121216, "val_acc": 0.82}, {"epoch": 111, "train_loss": 0.40854748606681823, "val_loss": 0.4425589048862457, "val_acc": 0.82}, {"epoch": 112, "train_loss": 0.41008333206176756, "val_loss": 0.4380998945236206, "val_acc": 0.825}, {"epoch": 113, "train_loss": 0.4079249262809753, "val_loss": 0.4458716905117035, "val_acc": 0.825}, {"epoch": 114, "train_loss": 0.4080405783653259, "val_loss": 0.4419035828113556, "val_acc": 0.825}, {"epoch": 115, "train_loss": 0.4074457240104675, "val_loss": 0.44388394117355345, "val_acc": 0.81}, {"epoch": 116, "train_loss": 0.40879709005355835, "val_loss": 0.44211862683296205, "val_acc": 0.82}, {"epoch": 117, "train_loss": 0.40649391651153566, "val_loss": 0.44568373560905455, "val_acc": 0.82}, {"epoch": 118, "train_loss": 0.40822134137153626, "val_loss": 0.4415166175365448, "val_acc": 0.835}, {"epoch": 119, "train_loss": 0.40735395669937136, "val_loss": 0.4470566892623901, "val_acc": 0.82}, {"epoch": 120, "train_loss": 0.4067606294155121, "val_loss": 0.4446811783313751, "val_acc": 0.82}, {"epoch": 121, "train_loss": 0.40708819329738616, "val_loss": 0.44132421016693113, "val_acc": 0.825}, {"epoch": 122, "train_loss": 0.4084883940219879, "val_loss": 0.444143009185791, "val_acc": 0.825}, {"epoch": 123, "train_loss": 0.4085855960845947, "val_loss": 0.44010338306427, "val_acc": 0.825}, {"epoch": 124, "train_loss": 0.40596298456192015, "val_loss": 0.445755352973938, "val_acc": 0.82}, {"epoch": 125, "train_loss": 0.4082092750072479, "val_loss": 0.4435127854347229, "val_acc": 0.83}, {"epoch": 126, "train_loss": 0.4056291198730469, "val_loss": 0.44624142527580263, "val_acc": 0.82}, {"epoch": 127, "train_loss": 0.406233925819397, "val_loss": 0.44057551980018617, "val_acc": 0.825}, {"epoch": 128, "train_loss": 0.40556602120399476, "val_loss": 0.44334868431091307, "val_acc": 0.825}, {"epoch": 129, "train_loss": 0.4073194622993469, "val_loss": 0.44135201930999757, "val_acc": 0.82}, {"epoch": 130, "train_loss": 0.4100949919223785, "val_loss": 0.44389126896858216, "val_acc": 0.82}], "final_metrics": {"epoch": 130, "train_loss": 0.4100949919223785, "val_loss": 0.44389126896858216, "val_acc": 0.82}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 75), nn.ReLU(), nn.Linear(75, 35), nn.ReLU(), nn.Linear(35, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.000375)\nself.training_epochs = 145", "history": [{"epoch": 1, "train_loss": 1.0518476629257203, "val_loss": 1.0768178939819335, "val_acc": 0.285}, {"epoch": 2, "train_loss": 1.026729257106781, "val_loss": 1.0611610317230225, "val_acc": 0.285}, {"epoch": 3, "train_loss": 1.007306890487671, "val_loss": 1.0460556221008301, "val_acc": 0.285}, {"epoch": 4, "train_loss": 0.9895864057540894, "val_loss": 1.0295171642303467, "val_acc": 0.285}, {"epoch": 5, "train_loss": 0.9728440308570862, "val_loss": 1.0109514307975769, "val_acc": 0.34}, {"epoch": 6, "train_loss": 0.9553425979614257, "val_loss": 0.9925847339630127, "val_acc": 0.38}, {"epoch": 7, "train_loss": 0.9370521545410156, "val_loss": 0.9734573554992676, "val_acc": 0.415}, {"epoch": 8, "train_loss": 0.9187188529968262, "val_loss": 0.9505124378204346, "val_acc": 0.46}, {"epoch": 9, "train_loss": 0.8962880516052246, "val_loss": 0.9254259705543518, "val_acc": 0.49}, {"epoch": 10, "train_loss": 0.8739450693130493, "val_loss": 0.9005186653137207, "val_acc": 0.54}, {"epoch": 11, "train_loss": 0.8514307928085327, "val_loss": 0.8722884106636047, "val_acc": 0.6}, {"epoch": 12, "train_loss": 0.8282424092292786, "val_loss": 0.8467114067077637, "val_acc": 0.63}, {"epoch": 13, "train_loss": 0.8044439792633057, "val_loss": 0.8213646984100342, "val_acc": 0.675}, {"epoch": 14, "train_loss": 0.7807788300514221, "val_loss": 0.7930492115020752, "val_acc": 0.72}, {"epoch": 15, "train_loss": 0.7576142907142639, "val_loss": 0.7635862350463867, "val_acc": 0.705}, {"epoch": 16, "train_loss": 0.7331690764427186, "val_loss": 0.7379482889175415, "val_acc": 0.735}, {"epoch": 17, "train_loss": 0.7091707515716553, "val_loss": 0.7128739929199219, "val_acc": 0.745}, {"epoch": 18, "train_loss": 0.6867787122726441, "val_loss": 0.6854066705703735, "val_acc": 0.765}, {"epoch": 19, "train_loss": 0.6664643073081971, "val_loss": 0.662237343788147, "val_acc": 0.765}, {"epoch": 20, "train_loss": 0.6465960121154786, "val_loss": 0.6402432703971863, "val_acc": 0.78}, {"epoch": 21, "train_loss": 0.6281869912147522, "val_loss": 0.6251590752601623, "val_acc": 0.77}, {"epoch": 22, "train_loss": 0.6117111039161682, "val_loss": 0.6045479154586793, "val_acc": 0.785}, {"epoch": 23, "train_loss": 0.5962658524513245, "val_loss": 0.5869530367851258, "val_acc": 0.81}, {"epoch": 24, "train_loss": 0.5840072512626648, "val_loss": 0.5750053322315216, "val_acc": 0.785}, {"epoch": 25, "train_loss": 0.5693453621864318, "val_loss": 0.5584274482727051, "val_acc": 0.825}, {"epoch": 26, "train_loss": 0.5583430886268616, "val_loss": 0.548781955242157, "val_acc": 0.81}, {"epoch": 27, "train_loss": 0.5488499796390534, "val_loss": 0.5374351465702056, "val_acc": 0.815}, {"epoch": 28, "train_loss": 0.5386842823028565, "val_loss": 0.5268630599975586, "val_acc": 0.825}, {"epoch": 29, "train_loss": 0.5302051973342895, "val_loss": 0.5185517179965973, "val_acc": 0.82}, {"epoch": 30, "train_loss": 0.5225842595100403, "val_loss": 0.5092041969299317, "val_acc": 0.825}, {"epoch": 31, "train_loss": 0.5151986050605774, "val_loss": 0.5024510383605957, "val_acc": 0.83}, {"epoch": 32, "train_loss": 0.5093008983135223, "val_loss": 0.49686967611312866, "val_acc": 0.82}, {"epoch": 33, "train_loss": 0.5018135380744934, "val_loss": 0.48810149550437926, "val_acc": 0.83}, {"epoch": 34, "train_loss": 0.49595714688301085, "val_loss": 0.4830014383792877, "val_acc": 0.835}, {"epoch": 35, "train_loss": 0.4905481016635895, "val_loss": 0.47967733144760133, "val_acc": 0.83}, {"epoch": 36, "train_loss": 0.4862023162841797, "val_loss": 0.47414262890815734, "val_acc": 0.825}, {"epoch": 37, "train_loss": 0.48235663652420047, "val_loss": 0.470558694601059, "val_acc": 0.825}, {"epoch": 38, "train_loss": 0.4772204327583313, "val_loss": 0.4653796124458313, "val_acc": 0.83}, {"epoch": 39, "train_loss": 0.47451031565666196, "val_loss": 0.46203577756881714, "val_acc": 0.825}, {"epoch": 40, "train_loss": 0.4708528447151184, "val_loss": 0.4574375367164612, "val_acc": 0.83}, {"epoch": 41, "train_loss": 0.4685422134399414, "val_loss": 0.4573323285579681, "val_acc": 0.82}, {"epoch": 42, "train_loss": 0.4651487910747528, "val_loss": 0.45213724255561827, "val_acc": 0.83}, {"epoch": 43, "train_loss": 0.4624159872531891, "val_loss": 0.4482958197593689, "val_acc": 0.835}, {"epoch": 44, "train_loss": 0.4585395264625549, "val_loss": 0.4468075966835022, "val_acc": 0.825}, {"epoch": 45, "train_loss": 0.4574275803565979, "val_loss": 0.4451026511192322, "val_acc": 0.82}, {"epoch": 46, "train_loss": 0.4544691133499146, "val_loss": 0.44065242528915405, "val_acc": 0.84}, {"epoch": 47, "train_loss": 0.45261388778686523, "val_loss": 0.44025365471839906, "val_acc": 0.825}, {"epoch": 48, "train_loss": 0.4507689011096954, "val_loss": 0.4411997866630554, "val_acc": 0.83}, {"epoch": 49, "train_loss": 0.448628146648407, "val_loss": 0.43703661918640135, "val_acc": 0.83}, {"epoch": 50, "train_loss": 0.4481839275360107, "val_loss": 0.43473625898361207, "val_acc": 0.83}, {"epoch": 51, "train_loss": 0.44496719479560853, "val_loss": 0.43522953748703, "val_acc": 0.82}, {"epoch": 52, "train_loss": 0.44475263357162476, "val_loss": 0.4351497530937195, "val_acc": 0.825}, {"epoch": 53, "train_loss": 0.44409837245941164, "val_loss": 0.43201699256896975, "val_acc": 0.83}, {"epoch": 54, "train_loss": 0.44100475430488584, "val_loss": 0.43098973274230956, "val_acc": 0.82}, {"epoch": 55, "train_loss": 0.44035093307495116, "val_loss": 0.4280399775505066, "val_acc": 0.82}, {"epoch": 56, "train_loss": 0.4397363424301147, "val_loss": 0.4281488919258118, "val_acc": 0.825}, {"epoch": 57, "train_loss": 0.4376843845844269, "val_loss": 0.4253835678100586, "val_acc": 0.83}, {"epoch": 58, "train_loss": 0.4375965428352356, "val_loss": 0.42655680179595945, "val_acc": 0.82}, {"epoch": 59, "train_loss": 0.4367749178409576, "val_loss": 0.4258635175228119, "val_acc": 0.83}, {"epoch": 60, "train_loss": 0.4351717662811279, "val_loss": 0.42488400101661683, "val_acc": 0.83}, {"epoch": 61, "train_loss": 0.43779789686203, "val_loss": 0.4262850713729858, "val_acc": 0.825}, {"epoch": 62, "train_loss": 0.43660295367240903, "val_loss": 0.42399895668029786, "val_acc": 0.83}, {"epoch": 63, "train_loss": 0.43382508277893067, "val_loss": 0.4218324661254883, "val_acc": 0.825}, {"epoch": 64, "train_loss": 0.43325021862983704, "val_loss": 0.4237964451313019, "val_acc": 0.825}, {"epoch": 65, "train_loss": 0.4324187135696411, "val_loss": 0.4222633326053619, "val_acc": 0.83}, {"epoch": 66, "train_loss": 0.4306409525871277, "val_loss": 0.4204358339309692, "val_acc": 0.82}, {"epoch": 67, "train_loss": 0.43002108335494993, "val_loss": 0.4181826460361481, "val_acc": 0.825}, {"epoch": 68, "train_loss": 0.4308029329776764, "val_loss": 0.41908773183822634, "val_acc": 0.82}, {"epoch": 69, "train_loss": 0.42947173237800595, "val_loss": 0.41776153683662415, "val_acc": 0.83}, {"epoch": 70, "train_loss": 0.428749657869339, "val_loss": 0.4184200024604797, "val_acc": 0.825}, {"epoch": 71, "train_loss": 0.42848620772361756, "val_loss": 0.4178675043582916, "val_acc": 0.84}, {"epoch": 72, "train_loss": 0.42721606731414796, "val_loss": 0.41969818830490113, "val_acc": 0.82}, {"epoch": 73, "train_loss": 0.4271166205406189, "val_loss": 0.41867066860198976, "val_acc": 0.825}, {"epoch": 74, "train_loss": 0.4256736099720001, "val_loss": 0.4173934984207153, "val_acc": 0.825}, {"epoch": 75, "train_loss": 0.4262663924694061, "val_loss": 0.41674081087112425, "val_acc": 0.825}, {"epoch": 76, "train_loss": 0.4254733765125275, "val_loss": 0.41773643732070925, "val_acc": 0.82}, {"epoch": 77, "train_loss": 0.42482810258865356, "val_loss": 0.4169992864131927, "val_acc": 0.82}, {"epoch": 78, "train_loss": 0.42454920887947084, "val_loss": 0.4178816294670105, "val_acc": 0.82}, {"epoch": 79, "train_loss": 0.42498580694198607, "val_loss": 0.4154370152950287, "val_acc": 0.835}, {"epoch": 80, "train_loss": 0.42320910215377805, "val_loss": 0.41633583307266236, "val_acc": 0.82}, {"epoch": 81, "train_loss": 0.4248314142227173, "val_loss": 0.4150948333740234, "val_acc": 0.83}, {"epoch": 82, "train_loss": 0.42411221861839293, "val_loss": 0.41520423412323, "val_acc": 0.82}, {"epoch": 83, "train_loss": 0.4238417625427246, "val_loss": 0.4131701135635376, "val_acc": 0.83}, {"epoch": 84, "train_loss": 0.4214919769763947, "val_loss": 0.41412252068519595, "val_acc": 0.82}, {"epoch": 85, "train_loss": 0.42567190289497375, "val_loss": 0.41301158666610716, "val_acc": 0.83}, {"epoch": 86, "train_loss": 0.4221131658554077, "val_loss": 0.41553231358528137, "val_acc": 0.82}, {"epoch": 87, "train_loss": 0.42304829120635984, "val_loss": 0.4165980410575867, "val_acc": 0.82}, {"epoch": 88, "train_loss": 0.42364450693130495, "val_loss": 0.414511079788208, "val_acc": 0.82}, {"epoch": 89, "train_loss": 0.42240756273269653, "val_loss": 0.41299179673194886, "val_acc": 0.835}, {"epoch": 90, "train_loss": 0.4230090260505676, "val_loss": 0.4135386550426483, "val_acc": 0.825}, {"epoch": 91, "train_loss": 0.4213030672073364, "val_loss": 0.4128651881217957, "val_acc": 0.83}, {"epoch": 92, "train_loss": 0.4199116563796997, "val_loss": 0.4134072029590607, "val_acc": 0.825}, {"epoch": 93, "train_loss": 0.41979960203170774, "val_loss": 0.41322309255599976, "val_acc": 0.83}, {"epoch": 94, "train_loss": 0.4226019489765167, "val_loss": 0.41201277494430544, "val_acc": 0.825}, {"epoch": 95, "train_loss": 0.42124143719673157, "val_loss": 0.40997855305671693, "val_acc": 0.83}, {"epoch": 96, "train_loss": 0.4214394128322601, "val_loss": 0.41276069402694704, "val_acc": 0.825}, {"epoch": 97, "train_loss": 0.4194865036010742, "val_loss": 0.41384863972663877, "val_acc": 0.82}, {"epoch": 98, "train_loss": 0.41842875361442566, "val_loss": 0.4109170544147491, "val_acc": 0.84}, {"epoch": 99, "train_loss": 0.4178400278091431, "val_loss": 0.4130822944641113, "val_acc": 0.82}, {"epoch": 100, "train_loss": 0.4178014957904816, "val_loss": 0.4096486759185791, "val_acc": 0.83}, {"epoch": 101, "train_loss": 0.42108733057975767, "val_loss": 0.4104214560985565, "val_acc": 0.83}, {"epoch": 102, "train_loss": 0.41733059883117674, "val_loss": 0.4104923129081726, "val_acc": 0.83}, {"epoch": 103, "train_loss": 0.41779792189598086, "val_loss": 0.4117716407775879, "val_acc": 0.825}, {"epoch": 104, "train_loss": 0.4165302610397339, "val_loss": 0.41051955580711363, "val_acc": 0.825}, {"epoch": 105, "train_loss": 0.4174999105930328, "val_loss": 0.40949986457824705, "val_acc": 0.83}, {"epoch": 106, "train_loss": 0.4169337236881256, "val_loss": 0.4096356725692749, "val_acc": 0.825}, {"epoch": 107, "train_loss": 0.41629436016082766, "val_loss": 0.4109484267234802, "val_acc": 0.825}, {"epoch": 108, "train_loss": 0.4165473699569702, "val_loss": 0.41122539520263673, "val_acc": 0.825}, {"epoch": 109, "train_loss": 0.416807998418808, "val_loss": 0.4083833622932434, "val_acc": 0.84}, {"epoch": 110, "train_loss": 0.41548032641410826, "val_loss": 0.41091474890708923, "val_acc": 0.825}, {"epoch": 111, "train_loss": 0.4189385223388672, "val_loss": 0.41262612462043763, "val_acc": 0.825}, {"epoch": 112, "train_loss": 0.4154084300994873, "val_loss": 0.4099411141872406, "val_acc": 0.825}, {"epoch": 113, "train_loss": 0.4155689644813538, "val_loss": 0.4093060564994812, "val_acc": 0.835}, {"epoch": 114, "train_loss": 0.41590655088424683, "val_loss": 0.4099012899398804, "val_acc": 0.835}, {"epoch": 115, "train_loss": 0.41729109168052675, "val_loss": 0.41183323979377745, "val_acc": 0.825}, {"epoch": 116, "train_loss": 0.41593330264091494, "val_loss": 0.4111553251743317, "val_acc": 0.815}, {"epoch": 117, "train_loss": 0.415571403503418, "val_loss": 0.41105583906173704, "val_acc": 0.825}, {"epoch": 118, "train_loss": 0.41460853338241577, "val_loss": 0.4123350775241852, "val_acc": 0.815}, {"epoch": 119, "train_loss": 0.41388289093971253, "val_loss": 0.4102940356731415, "val_acc": 0.825}, {"epoch": 120, "train_loss": 0.41531976103782653, "val_loss": 0.4094989204406738, "val_acc": 0.835}, {"epoch": 121, "train_loss": 0.41391011357307433, "val_loss": 0.41017681241035464, "val_acc": 0.825}, {"epoch": 122, "train_loss": 0.41564721703529356, "val_loss": 0.4097046864032745, "val_acc": 0.815}, {"epoch": 123, "train_loss": 0.41410078525543215, "val_loss": 0.41046249866485596, "val_acc": 0.825}, {"epoch": 124, "train_loss": 0.41355726003646853, "val_loss": 0.4091294503211975, "val_acc": 0.83}, {"epoch": 125, "train_loss": 0.41346418857574463, "val_loss": 0.40922712564468383, "val_acc": 0.825}, {"epoch": 126, "train_loss": 0.41294978618621825, "val_loss": 0.40878116130828857, "val_acc": 0.815}, {"epoch": 127, "train_loss": 0.41413066267967225, "val_loss": 0.40823700904846194, "val_acc": 0.82}, {"epoch": 128, "train_loss": 0.41411286950111387, "val_loss": 0.40903778076171876, "val_acc": 0.835}, {"epoch": 129, "train_loss": 0.41199916005134585, "val_loss": 0.40924285650253295, "val_acc": 0.815}, {"epoch": 130, "train_loss": 0.4121868121623993, "val_loss": 0.40883799314498903, "val_acc": 0.83}, {"epoch": 131, "train_loss": 0.4121882081031799, "val_loss": 0.4088884711265564, "val_acc": 0.825}, {"epoch": 132, "train_loss": 0.41312564492225645, "val_loss": 0.409882150888443, "val_acc": 0.82}, {"epoch": 133, "train_loss": 0.41231116056442263, "val_loss": 0.40765336990356443, "val_acc": 0.835}, {"epoch": 134, "train_loss": 0.41196785926818846, "val_loss": 0.4078598403930664, "val_acc": 0.83}, {"epoch": 135, "train_loss": 0.41217901349067687, "val_loss": 0.41084623098373413, "val_acc": 0.82}, {"epoch": 136, "train_loss": 0.4123450016975403, "val_loss": 0.409552139043808, "val_acc": 0.815}, {"epoch": 137, "train_loss": 0.4123010814189911, "val_loss": 0.4067280435562134, "val_acc": 0.825}, {"epoch": 138, "train_loss": 0.4122611224651337, "val_loss": 0.4075581359863281, "val_acc": 0.835}, {"epoch": 139, "train_loss": 0.41159657955169676, "val_loss": 0.4085647451877594, "val_acc": 0.825}, {"epoch": 140, "train_loss": 0.4113883423805237, "val_loss": 0.4088693594932556, "val_acc": 0.825}, {"epoch": 141, "train_loss": 0.41210483133792875, "val_loss": 0.40894946217536926, "val_acc": 0.815}, {"epoch": 142, "train_loss": 0.4110675263404846, "val_loss": 0.4086428546905518, "val_acc": 0.825}, {"epoch": 143, "train_loss": 0.41174705028533937, "val_loss": 0.409801025390625, "val_acc": 0.83}, {"epoch": 144, "train_loss": 0.41069926977157595, "val_loss": 0.40868537306785585, "val_acc": 0.825}, {"epoch": 145, "train_loss": 0.41119855880737305, "val_loss": 0.4072958731651306, "val_acc": 0.83}], "final_metrics": {"epoch": 145, "train_loss": 0.41119855880737305, "val_loss": 0.4072958731651306, "val_acc": 0.83}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 95), nn.ReLU(), nn.Linear(95, 50), nn.ReLU(), nn.Linear(50, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.000375)\nself.training_epochs = 130", "history": [{"epoch": 1, "train_loss": 1.0659664011001586, "val_loss": 1.0394282007217408, "val_acc": 0.405}, {"epoch": 2, "train_loss": 1.0246817088127136, "val_loss": 1.025074484348297, "val_acc": 0.305}, {"epoch": 3, "train_loss": 1.0021985721588136, "val_loss": 1.0052319645881653, "val_acc": 0.33}, {"epoch": 4, "train_loss": 0.9791028308868408, "val_loss": 0.984713327884674, "val_acc": 0.35}, {"epoch": 5, "train_loss": 0.954529230594635, "val_loss": 0.9590775156021119, "val_acc": 0.39}, {"epoch": 6, "train_loss": 0.925830078125, "val_loss": 0.930559446811676, "val_acc": 0.445}, {"epoch": 7, "train_loss": 0.8944867539405823, "val_loss": 0.8990695428848267, "val_acc": 0.575}, {"epoch": 8, "train_loss": 0.8628068232536316, "val_loss": 0.8685397243499756, "val_acc": 0.605}, {"epoch": 9, "train_loss": 0.8293735218048096, "val_loss": 0.8377679991722107, "val_acc": 0.63}, {"epoch": 10, "train_loss": 0.7972056341171264, "val_loss": 0.8061168694496155, "val_acc": 0.66}, {"epoch": 11, "train_loss": 0.7629996418952942, "val_loss": 0.7736686158180237, "val_acc": 0.67}, {"epoch": 12, "train_loss": 0.7320517802238464, "val_loss": 0.7459524655342102, "val_acc": 0.71}, {"epoch": 13, "train_loss": 0.6999713683128357, "val_loss": 0.7168076992034912, "val_acc": 0.72}, {"epoch": 14, "train_loss": 0.6713626766204834, "val_loss": 0.6912433218955993, "val_acc": 0.725}, {"epoch": 15, "train_loss": 0.6453178381919861, "val_loss": 0.6675701570510865, "val_acc": 0.76}, {"epoch": 16, "train_loss": 0.6214558815956116, "val_loss": 0.6472154974937439, "val_acc": 0.765}, {"epoch": 17, "train_loss": 0.5991580581665039, "val_loss": 0.6261939239501954, "val_acc": 0.78}, {"epoch": 18, "train_loss": 0.5790323424339294, "val_loss": 0.6076776909828187, "val_acc": 0.785}, {"epoch": 19, "train_loss": 0.5632889270782471, "val_loss": 0.5961719191074372, "val_acc": 0.79}, {"epoch": 20, "train_loss": 0.5457481932640076, "val_loss": 0.5754614770412445, "val_acc": 0.785}, {"epoch": 21, "train_loss": 0.5317234754562378, "val_loss": 0.565747971534729, "val_acc": 0.79}, {"epoch": 22, "train_loss": 0.5184799146652221, "val_loss": 0.5578937399387359, "val_acc": 0.775}, {"epoch": 23, "train_loss": 0.5097559356689453, "val_loss": 0.5448095118999481, "val_acc": 0.785}, {"epoch": 24, "train_loss": 0.4981799507141113, "val_loss": 0.539304496049881, "val_acc": 0.8}, {"epoch": 25, "train_loss": 0.4900291860103607, "val_loss": 0.5282028007507324, "val_acc": 0.795}, {"epoch": 26, "train_loss": 0.4818302929401398, "val_loss": 0.5227189755439758, "val_acc": 0.805}, {"epoch": 27, "train_loss": 0.47413794994354247, "val_loss": 0.5176727461814881, "val_acc": 0.795}, {"epoch": 28, "train_loss": 0.46871745824813843, "val_loss": 0.5107958579063415, "val_acc": 0.8}, {"epoch": 29, "train_loss": 0.46237563848495483, "val_loss": 0.50144655585289, "val_acc": 0.81}, {"epoch": 30, "train_loss": 0.4595668935775757, "val_loss": 0.49692490577697757, "val_acc": 0.8}, {"epoch": 31, "train_loss": 0.45439956068992615, "val_loss": 0.4978003716468811, "val_acc": 0.795}, {"epoch": 32, "train_loss": 0.44889456510543824, "val_loss": 0.49174762606620787, "val_acc": 0.8}, {"epoch": 33, "train_loss": 0.44636724472045897, "val_loss": 0.48648033022880555, "val_acc": 0.805}, {"epoch": 34, "train_loss": 0.4476111912727356, "val_loss": 0.4856650459766388, "val_acc": 0.81}, {"epoch": 35, "train_loss": 0.44103963613510133, "val_loss": 0.4813090574741363, "val_acc": 0.8}, {"epoch": 36, "train_loss": 0.43780153155326845, "val_loss": 0.4771527171134949, "val_acc": 0.815}, {"epoch": 37, "train_loss": 0.4357549023628235, "val_loss": 0.4791618359088898, "val_acc": 0.805}, {"epoch": 38, "train_loss": 0.4319962513446808, "val_loss": 0.47360207080841066, "val_acc": 0.8}, {"epoch": 39, "train_loss": 0.43165887832641603, "val_loss": 0.4747565257549286, "val_acc": 0.805}, {"epoch": 40, "train_loss": 0.434784562587738, "val_loss": 0.46924715280532836, "val_acc": 0.82}, {"epoch": 41, "train_loss": 0.42734990000724793, "val_loss": 0.47386155486106873, "val_acc": 0.8}, {"epoch": 42, "train_loss": 0.4278726029396057, "val_loss": 0.4746768045425415, "val_acc": 0.8}, {"epoch": 43, "train_loss": 0.4272842168807983, "val_loss": 0.46340894281864164, "val_acc": 0.805}, {"epoch": 44, "train_loss": 0.42472939014434813, "val_loss": 0.46747878193855286, "val_acc": 0.805}, {"epoch": 45, "train_loss": 0.4211893653869629, "val_loss": 0.46871546387672425, "val_acc": 0.8}, {"epoch": 46, "train_loss": 0.4219770038127899, "val_loss": 0.4614929664134979, "val_acc": 0.805}, {"epoch": 47, "train_loss": 0.4210172915458679, "val_loss": 0.4632070255279541, "val_acc": 0.805}, {"epoch": 48, "train_loss": 0.4215607261657715, "val_loss": 0.4613502037525177, "val_acc": 0.8}, {"epoch": 49, "train_loss": 0.42012551069259646, "val_loss": 0.46257988691329954, "val_acc": 0.805}, {"epoch": 50, "train_loss": 0.4177842569351196, "val_loss": 0.46129882752895357, "val_acc": 0.795}, {"epoch": 51, "train_loss": 0.4176006746292114, "val_loss": 0.45833929657936096, "val_acc": 0.815}, {"epoch": 52, "train_loss": 0.41602411031723024, "val_loss": 0.4585867458581924, "val_acc": 0.805}, {"epoch": 53, "train_loss": 0.41635719299316404, "val_loss": 0.4606477361917496, "val_acc": 0.805}, {"epoch": 54, "train_loss": 0.41645187497138975, "val_loss": 0.4583927655220032, "val_acc": 0.81}, {"epoch": 55, "train_loss": 0.4146667265892029, "val_loss": 0.4565533071756363, "val_acc": 0.795}, {"epoch": 56, "train_loss": 0.4140967845916748, "val_loss": 0.45534288585186006, "val_acc": 0.8}, {"epoch": 57, "train_loss": 0.41393545031547546, "val_loss": 0.45627091109752654, "val_acc": 0.815}, {"epoch": 58, "train_loss": 0.41432451486587524, "val_loss": 0.45656699895858766, "val_acc": 0.815}, {"epoch": 59, "train_loss": 0.4130214011669159, "val_loss": 0.45316285848617555, "val_acc": 0.815}, {"epoch": 60, "train_loss": 0.4122403609752655, "val_loss": 0.45550911903381347, "val_acc": 0.805}, {"epoch": 61, "train_loss": 0.4114788556098938, "val_loss": 0.4537718892097473, "val_acc": 0.81}, {"epoch": 62, "train_loss": 0.4105935454368591, "val_loss": 0.4531660586595535, "val_acc": 0.81}, {"epoch": 63, "train_loss": 0.4117807912826538, "val_loss": 0.45300403714179993, "val_acc": 0.81}, {"epoch": 64, "train_loss": 0.4137290382385254, "val_loss": 0.45084345996379854, "val_acc": 0.8}, {"epoch": 65, "train_loss": 0.4109894633293152, "val_loss": 0.4537935507297516, "val_acc": 0.805}, {"epoch": 66, "train_loss": 0.4096958875656128, "val_loss": 0.45492573142051695, "val_acc": 0.81}, {"epoch": 67, "train_loss": 0.4121956610679626, "val_loss": 0.4523768550157547, "val_acc": 0.805}, {"epoch": 68, "train_loss": 0.4114309525489807, "val_loss": 0.45087018728256223, "val_acc": 0.8}, {"epoch": 69, "train_loss": 0.4102131676673889, "val_loss": 0.4521626317501068, "val_acc": 0.805}, {"epoch": 70, "train_loss": 0.4098559498786926, "val_loss": 0.45130981743335724, "val_acc": 0.82}, {"epoch": 71, "train_loss": 0.408705290555954, "val_loss": 0.4520241695642471, "val_acc": 0.805}, {"epoch": 72, "train_loss": 0.4107711505889893, "val_loss": 0.4486155778169632, "val_acc": 0.81}, {"epoch": 73, "train_loss": 0.4113915526866913, "val_loss": 0.45300336241722106, "val_acc": 0.815}, {"epoch": 74, "train_loss": 0.40802624583244324, "val_loss": 0.44985155761241913, "val_acc": 0.81}, {"epoch": 75, "train_loss": 0.4078421461582184, "val_loss": 0.4466277498006821, "val_acc": 0.825}, {"epoch": 76, "train_loss": 0.40962678551673887, "val_loss": 0.448378159403801, "val_acc": 0.81}, {"epoch": 77, "train_loss": 0.4071643328666687, "val_loss": 0.4508750921487808, "val_acc": 0.82}, {"epoch": 78, "train_loss": 0.40989386320114135, "val_loss": 0.44982112586498263, "val_acc": 0.82}, {"epoch": 79, "train_loss": 0.4084503960609436, "val_loss": 0.4496135914325714, "val_acc": 0.81}, {"epoch": 80, "train_loss": 0.4086501061916351, "val_loss": 0.4495731347799301, "val_acc": 0.805}, {"epoch": 81, "train_loss": 0.4070525622367859, "val_loss": 0.4505609428882599, "val_acc": 0.81}, {"epoch": 82, "train_loss": 0.41135343313217165, "val_loss": 0.4501498705148697, "val_acc": 0.81}, {"epoch": 83, "train_loss": 0.40794581055641177, "val_loss": 0.4498949921131134, "val_acc": 0.81}, {"epoch": 84, "train_loss": 0.40596447467803953, "val_loss": 0.4477033805847168, "val_acc": 0.83}, {"epoch": 85, "train_loss": 0.40632181644439697, "val_loss": 0.4496625971794128, "val_acc": 0.81}, {"epoch": 86, "train_loss": 0.408141930103302, "val_loss": 0.4550433087348938, "val_acc": 0.805}, {"epoch": 87, "train_loss": 0.40696824073791504, "val_loss": 0.4463837194442749, "val_acc": 0.815}, {"epoch": 88, "train_loss": 0.4061480736732483, "val_loss": 0.4475273525714874, "val_acc": 0.805}, {"epoch": 89, "train_loss": 0.40812004923820494, "val_loss": 0.44513550579547884, "val_acc": 0.81}, {"epoch": 90, "train_loss": 0.40570741772651675, "val_loss": 0.4496550589799881, "val_acc": 0.815}, {"epoch": 91, "train_loss": 0.4065384244918823, "val_loss": 0.4521942549943924, "val_acc": 0.815}, {"epoch": 92, "train_loss": 0.4052799379825592, "val_loss": 0.449784055352211, "val_acc": 0.815}, {"epoch": 93, "train_loss": 0.40583402633666993, "val_loss": 0.445985546708107, "val_acc": 0.825}, {"epoch": 94, "train_loss": 0.40451046466827395, "val_loss": 0.44532670319080353, "val_acc": 0.825}, {"epoch": 95, "train_loss": 0.4060003650188446, "val_loss": 0.44925396621227265, "val_acc": 0.815}, {"epoch": 96, "train_loss": 0.4053899025917053, "val_loss": 0.44765389025211333, "val_acc": 0.815}, {"epoch": 97, "train_loss": 0.4035201907157898, "val_loss": 0.44705109238624574, "val_acc": 0.81}, {"epoch": 98, "train_loss": 0.4048923969268799, "val_loss": 0.4453603917360306, "val_acc": 0.815}, {"epoch": 99, "train_loss": 0.4045998167991638, "val_loss": 0.4508050709962845, "val_acc": 0.82}, {"epoch": 100, "train_loss": 0.40421712160110473, "val_loss": 0.44597219944000244, "val_acc": 0.815}, {"epoch": 101, "train_loss": 0.4033200263977051, "val_loss": 0.44921520352363586, "val_acc": 0.815}, {"epoch": 102, "train_loss": 0.4034912478923798, "val_loss": 0.44977043092250824, "val_acc": 0.81}, {"epoch": 103, "train_loss": 0.4041420340538025, "val_loss": 0.44743305563926694, "val_acc": 0.825}, {"epoch": 104, "train_loss": 0.40447181463241577, "val_loss": 0.44813718855381013, "val_acc": 0.815}, {"epoch": 105, "train_loss": 0.404424774646759, "val_loss": 0.4487569570541382, "val_acc": 0.825}, {"epoch": 106, "train_loss": 0.40395370960235594, "val_loss": 0.4478236365318298, "val_acc": 0.82}, {"epoch": 107, "train_loss": 0.40384889602661134, "val_loss": 0.4477010703086853, "val_acc": 0.82}, {"epoch": 108, "train_loss": 0.405781911611557, "val_loss": 0.44638192415237427, "val_acc": 0.825}, {"epoch": 109, "train_loss": 0.4028928208351135, "val_loss": 0.44830807447433474, "val_acc": 0.815}, {"epoch": 110, "train_loss": 0.40195266366004945, "val_loss": 0.4454588848352432, "val_acc": 0.815}, {"epoch": 111, "train_loss": 0.40151758372783664, "val_loss": 0.4498374843597412, "val_acc": 0.815}, {"epoch": 112, "train_loss": 0.4024383556842804, "val_loss": 0.4494957435131073, "val_acc": 0.81}, {"epoch": 113, "train_loss": 0.40389318704605104, "val_loss": 0.44218700110912323, "val_acc": 0.84}, {"epoch": 114, "train_loss": 0.40557897806167603, "val_loss": 0.4465683525800705, "val_acc": 0.82}, {"epoch": 115, "train_loss": 0.4051676344871521, "val_loss": 0.4481946325302124, "val_acc": 0.82}, {"epoch": 116, "train_loss": 0.40202749252319336, "val_loss": 0.44931761384010316, "val_acc": 0.815}, {"epoch": 117, "train_loss": 0.4044382572174072, "val_loss": 0.45022859394550324, "val_acc": 0.81}, {"epoch": 118, "train_loss": 0.40445677876472474, "val_loss": 0.4492963087558746, "val_acc": 0.81}, {"epoch": 119, "train_loss": 0.40260639786720276, "val_loss": 0.4460389482975006, "val_acc": 0.825}, {"epoch": 120, "train_loss": 0.40091678738594055, "val_loss": 0.4460512149333954, "val_acc": 0.82}, {"epoch": 121, "train_loss": 0.40147542834281924, "val_loss": 0.4478772288560867, "val_acc": 0.82}, {"epoch": 122, "train_loss": 0.40233381032943727, "val_loss": 0.4436494654417038, "val_acc": 0.835}, {"epoch": 123, "train_loss": 0.402892963886261, "val_loss": 0.4487741374969482, "val_acc": 0.81}, {"epoch": 124, "train_loss": 0.40043885350227354, "val_loss": 0.44270383775234223, "val_acc": 0.835}, {"epoch": 125, "train_loss": 0.4008884358406067, "val_loss": 0.4492071855068207, "val_acc": 0.81}, {"epoch": 126, "train_loss": 0.40023642003536225, "val_loss": 0.4481663179397583, "val_acc": 0.82}, {"epoch": 127, "train_loss": 0.402845139503479, "val_loss": 0.44573582291603087, "val_acc": 0.825}, {"epoch": 128, "train_loss": 0.3990980744361877, "val_loss": 0.44956881165504453, "val_acc": 0.815}, {"epoch": 129, "train_loss": 0.4010698139667511, "val_loss": 0.446640545129776, "val_acc": 0.82}, {"epoch": 130, "train_loss": 0.4023961853981018, "val_loss": 0.44498898029327394, "val_acc": 0.845}], "final_metrics": {"epoch": 130, "train_loss": 0.4023961853981018, "val_loss": 0.44498898029327394, "val_acc": 0.845}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 80), nn.ReLU(), nn.Linear(80, 40), nn.ReLU(), nn.Linear(40, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\nself.training_epochs = 140", "history": [{"epoch": 1, "train_loss": 1.108455901145935, "val_loss": 1.0585309743881226, "val_acc": 0.42}, {"epoch": 2, "train_loss": 1.0629505372047425, "val_loss": 1.0230427718162536, "val_acc": 0.43}, {"epoch": 3, "train_loss": 1.0318282961845398, "val_loss": 0.9946521735191345, "val_acc": 0.45}, {"epoch": 4, "train_loss": 1.0037545251846314, "val_loss": 0.9683817911148072, "val_acc": 0.51}, {"epoch": 5, "train_loss": 0.9749589014053345, "val_loss": 0.9388172912597657, "val_acc": 0.55}, {"epoch": 6, "train_loss": 0.9438297080993653, "val_loss": 0.9072819042205811, "val_acc": 0.62}, {"epoch": 7, "train_loss": 0.9108302664756774, "val_loss": 0.8718717765808105, "val_acc": 0.635}, {"epoch": 8, "train_loss": 0.8749840784072876, "val_loss": 0.837666642665863, "val_acc": 0.685}, {"epoch": 9, "train_loss": 0.8393957018852234, "val_loss": 0.8009801006317139, "val_acc": 0.705}, {"epoch": 10, "train_loss": 0.8028277969360351, "val_loss": 0.76422621011734, "val_acc": 0.695}, {"epoch": 11, "train_loss": 0.7678090643882751, "val_loss": 0.7319021463394165, "val_acc": 0.715}, {"epoch": 12, "train_loss": 0.7338706970214843, "val_loss": 0.6978935360908508, "val_acc": 0.73}, {"epoch": 13, "train_loss": 0.7027433705329895, "val_loss": 0.6658931159973145, "val_acc": 0.755}, {"epoch": 14, "train_loss": 0.6728544163703919, "val_loss": 0.6402044224739075, "val_acc": 0.76}, {"epoch": 15, "train_loss": 0.6475158143043518, "val_loss": 0.6157945418357849, "val_acc": 0.795}, {"epoch": 16, "train_loss": 0.623383436203003, "val_loss": 0.5947297620773315, "val_acc": 0.805}, {"epoch": 17, "train_loss": 0.6025994420051575, "val_loss": 0.5758677458763123, "val_acc": 0.82}, {"epoch": 18, "train_loss": 0.5834391021728516, "val_loss": 0.556279501914978, "val_acc": 0.83}, {"epoch": 19, "train_loss": 0.5666628289222717, "val_loss": 0.5402959728240967, "val_acc": 0.84}, {"epoch": 20, "train_loss": 0.5556740212440491, "val_loss": 0.5278850841522217, "val_acc": 0.84}, {"epoch": 21, "train_loss": 0.5477862000465393, "val_loss": 0.513047661781311, "val_acc": 0.84}, {"epoch": 22, "train_loss": 0.53102614402771, "val_loss": 0.5074860405921936, "val_acc": 0.815}, {"epoch": 23, "train_loss": 0.5174791789054871, "val_loss": 0.4940973162651062, "val_acc": 0.835}, {"epoch": 24, "train_loss": 0.5094472944736481, "val_loss": 0.4849015736579895, "val_acc": 0.85}, {"epoch": 25, "train_loss": 0.5012456393241882, "val_loss": 0.47633369207382203, "val_acc": 0.845}, {"epoch": 26, "train_loss": 0.49347800493240357, "val_loss": 0.47062785387039185, "val_acc": 0.85}, {"epoch": 27, "train_loss": 0.4872421622276306, "val_loss": 0.4634030818939209, "val_acc": 0.845}, {"epoch": 28, "train_loss": 0.48320114612579346, "val_loss": 0.45842361211776733, "val_acc": 0.855}, {"epoch": 29, "train_loss": 0.4776743686199188, "val_loss": 0.4555079007148743, "val_acc": 0.845}, {"epoch": 30, "train_loss": 0.4720925736427307, "val_loss": 0.4489933729171753, "val_acc": 0.85}, {"epoch": 31, "train_loss": 0.46937586545944215, "val_loss": 0.442660493850708, "val_acc": 0.85}, {"epoch": 32, "train_loss": 0.4636493968963623, "val_loss": 0.43913376808166504, "val_acc": 0.845}, {"epoch": 33, "train_loss": 0.4619553017616272, "val_loss": 0.4398220419883728, "val_acc": 0.84}, {"epoch": 34, "train_loss": 0.4578461956977844, "val_loss": 0.4318308973312378, "val_acc": 0.85}, {"epoch": 35, "train_loss": 0.45607476234436034, "val_loss": 0.429123113155365, "val_acc": 0.86}, {"epoch": 36, "train_loss": 0.45474143743515016, "val_loss": 0.42948797941207884, "val_acc": 0.845}, {"epoch": 37, "train_loss": 0.45412479639053344, "val_loss": 0.4243518042564392, "val_acc": 0.86}, {"epoch": 38, "train_loss": 0.4499033498764038, "val_loss": 0.42317131996154783, "val_acc": 0.86}, {"epoch": 39, "train_loss": 0.44659417748451236, "val_loss": 0.4213146162033081, "val_acc": 0.86}, {"epoch": 40, "train_loss": 0.44501609086990357, "val_loss": 0.4183386445045471, "val_acc": 0.855}, {"epoch": 41, "train_loss": 0.44551854372024535, "val_loss": 0.4174616718292236, "val_acc": 0.85}, {"epoch": 42, "train_loss": 0.446150598526001, "val_loss": 0.41692676782608035, "val_acc": 0.865}, {"epoch": 43, "train_loss": 0.44135125041007994, "val_loss": 0.4159988260269165, "val_acc": 0.86}, {"epoch": 44, "train_loss": 0.44292797565460207, "val_loss": 0.41268465042114255, "val_acc": 0.85}, {"epoch": 45, "train_loss": 0.4389968812465668, "val_loss": 0.41257587671279905, "val_acc": 0.855}, {"epoch": 46, "train_loss": 0.4377917861938477, "val_loss": 0.41017885446548463, "val_acc": 0.865}, {"epoch": 47, "train_loss": 0.4371835434436798, "val_loss": 0.4097576403617859, "val_acc": 0.86}, {"epoch": 48, "train_loss": 0.4368943607807159, "val_loss": 0.4093954133987427, "val_acc": 0.855}, {"epoch": 49, "train_loss": 0.4360215759277344, "val_loss": 0.4078084373474121, "val_acc": 0.86}, {"epoch": 50, "train_loss": 0.4347619342803955, "val_loss": 0.40667975664138795, "val_acc": 0.865}, {"epoch": 51, "train_loss": 0.4342450475692749, "val_loss": 0.40526188611984254, "val_acc": 0.865}, {"epoch": 52, "train_loss": 0.4348590803146362, "val_loss": 0.4047908616065979, "val_acc": 0.86}, {"epoch": 53, "train_loss": 0.4389145731925964, "val_loss": 0.40753000020980834, "val_acc": 0.855}, {"epoch": 54, "train_loss": 0.43807013273239137, "val_loss": 0.4026589560508728, "val_acc": 0.86}, {"epoch": 55, "train_loss": 0.43182278037071226, "val_loss": 0.40387262105941774, "val_acc": 0.865}, {"epoch": 56, "train_loss": 0.43173643469810485, "val_loss": 0.40413058757781983, "val_acc": 0.86}, {"epoch": 57, "train_loss": 0.4313550138473511, "val_loss": 0.4019618320465088, "val_acc": 0.865}, {"epoch": 58, "train_loss": 0.43102689266204836, "val_loss": 0.40197416543960574, "val_acc": 0.865}, {"epoch": 59, "train_loss": 0.43114632487297055, "val_loss": 0.4030470299720764, "val_acc": 0.865}, {"epoch": 60, "train_loss": 0.4305959236621857, "val_loss": 0.4025373315811157, "val_acc": 0.855}, {"epoch": 61, "train_loss": 0.4313398027420044, "val_loss": 0.4011171007156372, "val_acc": 0.86}, {"epoch": 62, "train_loss": 0.4301470136642456, "val_loss": 0.39991360425949096, "val_acc": 0.87}, {"epoch": 63, "train_loss": 0.4281535911560059, "val_loss": 0.4001664853096008, "val_acc": 0.865}, {"epoch": 64, "train_loss": 0.4292512905597687, "val_loss": 0.4020739150047302, "val_acc": 0.85}, {"epoch": 65, "train_loss": 0.430462498664856, "val_loss": 0.40024917125701903, "val_acc": 0.87}, {"epoch": 66, "train_loss": 0.4305953133106232, "val_loss": 0.39957236766815185, "val_acc": 0.865}, {"epoch": 67, "train_loss": 0.42869876980781557, "val_loss": 0.4006537914276123, "val_acc": 0.86}, {"epoch": 68, "train_loss": 0.4288047695159912, "val_loss": 0.40164796113967893, "val_acc": 0.865}, {"epoch": 69, "train_loss": 0.42675759315490724, "val_loss": 0.400885694026947, "val_acc": 0.865}, {"epoch": 70, "train_loss": 0.42805402278900145, "val_loss": 0.3985773205757141, "val_acc": 0.86}, {"epoch": 71, "train_loss": 0.42633599519729615, "val_loss": 0.3991800546646118, "val_acc": 0.86}, {"epoch": 72, "train_loss": 0.4273082804679871, "val_loss": 0.39845493316650393, "val_acc": 0.86}, {"epoch": 73, "train_loss": 0.42676838040351867, "val_loss": 0.39917295694351196, "val_acc": 0.86}, {"epoch": 74, "train_loss": 0.42625542044639586, "val_loss": 0.39851107597351076, "val_acc": 0.87}, {"epoch": 75, "train_loss": 0.4256391835212707, "val_loss": 0.3985821604728699, "val_acc": 0.85}, {"epoch": 76, "train_loss": 0.42970584630966185, "val_loss": 0.39968990564346313, "val_acc": 0.86}, {"epoch": 77, "train_loss": 0.4251546835899353, "val_loss": 0.39892292737960816, "val_acc": 0.86}, {"epoch": 78, "train_loss": 0.4269620144367218, "val_loss": 0.3972179460525513, "val_acc": 0.855}, {"epoch": 79, "train_loss": 0.42482232570648193, "val_loss": 0.39657814502716066, "val_acc": 0.87}, {"epoch": 80, "train_loss": 0.4260255217552185, "val_loss": 0.3977805757522583, "val_acc": 0.86}, {"epoch": 81, "train_loss": 0.4256202268600464, "val_loss": 0.39685844659805297, "val_acc": 0.865}, {"epoch": 82, "train_loss": 0.42585963249206543, "val_loss": 0.39660800457000733, "val_acc": 0.87}, {"epoch": 83, "train_loss": 0.42430105686187747, "val_loss": 0.3963238501548767, "val_acc": 0.86}, {"epoch": 84, "train_loss": 0.4243865120410919, "val_loss": 0.39661847591400146, "val_acc": 0.87}, {"epoch": 85, "train_loss": 0.42414995312690734, "val_loss": 0.397689790725708, "val_acc": 0.855}, {"epoch": 86, "train_loss": 0.4241535079479217, "val_loss": 0.3971391534805298, "val_acc": 0.865}, {"epoch": 87, "train_loss": 0.4240138626098633, "val_loss": 0.3968480038642883, "val_acc": 0.855}, {"epoch": 88, "train_loss": 0.4237416100502014, "val_loss": 0.3965291404724121, "val_acc": 0.865}, {"epoch": 89, "train_loss": 0.4241789412498474, "val_loss": 0.3978183841705322, "val_acc": 0.865}, {"epoch": 90, "train_loss": 0.42356395840644834, "val_loss": 0.3971900010108948, "val_acc": 0.85}, {"epoch": 91, "train_loss": 0.4239458465576172, "val_loss": 0.39620667934417725, "val_acc": 0.865}, {"epoch": 92, "train_loss": 0.42371553301811216, "val_loss": 0.3978803467750549, "val_acc": 0.87}, {"epoch": 93, "train_loss": 0.42282206773757935, "val_loss": 0.3963872766494751, "val_acc": 0.865}, {"epoch": 94, "train_loss": 0.4239506149291992, "val_loss": 0.3953446435928345, "val_acc": 0.865}, {"epoch": 95, "train_loss": 0.42567806005477904, "val_loss": 0.39610896587371824, "val_acc": 0.85}, {"epoch": 96, "train_loss": 0.4242293632030487, "val_loss": 0.3965917682647705, "val_acc": 0.86}, {"epoch": 97, "train_loss": 0.42325093030929567, "val_loss": 0.3956494474411011, "val_acc": 0.87}, {"epoch": 98, "train_loss": 0.4242158758640289, "val_loss": 0.3951742148399353, "val_acc": 0.86}, {"epoch": 99, "train_loss": 0.423913037776947, "val_loss": 0.3946583843231201, "val_acc": 0.865}, {"epoch": 100, "train_loss": 0.42242094993591306, "val_loss": 0.39494282484054566, "val_acc": 0.865}, {"epoch": 101, "train_loss": 0.4214585506916046, "val_loss": 0.3951852202415466, "val_acc": 0.855}, {"epoch": 102, "train_loss": 0.4240705490112305, "val_loss": 0.3959922289848328, "val_acc": 0.87}, {"epoch": 103, "train_loss": 0.42416268825531006, "val_loss": 0.396496057510376, "val_acc": 0.85}, {"epoch": 104, "train_loss": 0.4219855630397797, "val_loss": 0.3977221703529358, "val_acc": 0.87}, {"epoch": 105, "train_loss": 0.4223472499847412, "val_loss": 0.3951712536811829, "val_acc": 0.865}, {"epoch": 106, "train_loss": 0.42119035840034486, "val_loss": 0.39550923347473144, "val_acc": 0.87}, {"epoch": 107, "train_loss": 0.42259456515312194, "val_loss": 0.39554967641830446, "val_acc": 0.855}, {"epoch": 108, "train_loss": 0.42011853456497195, "val_loss": 0.39508321523666384, "val_acc": 0.865}, {"epoch": 109, "train_loss": 0.4219998455047607, "val_loss": 0.39514790534973143, "val_acc": 0.87}, {"epoch": 110, "train_loss": 0.4216434609889984, "val_loss": 0.39373834133148194, "val_acc": 0.865}, {"epoch": 111, "train_loss": 0.42105172991752626, "val_loss": 0.3946428036689758, "val_acc": 0.855}, {"epoch": 112, "train_loss": 0.42183653354644773, "val_loss": 0.3940478086471558, "val_acc": 0.865}, {"epoch": 113, "train_loss": 0.4208099102973938, "val_loss": 0.3948440766334534, "val_acc": 0.865}, {"epoch": 114, "train_loss": 0.421548148393631, "val_loss": 0.3942744302749634, "val_acc": 0.87}, {"epoch": 115, "train_loss": 0.4216343641281128, "val_loss": 0.3925107216835022, "val_acc": 0.865}, {"epoch": 116, "train_loss": 0.4223969864845276, "val_loss": 0.39498416423797605, "val_acc": 0.85}, {"epoch": 117, "train_loss": 0.4236826479434967, "val_loss": 0.3947589898109436, "val_acc": 0.865}, {"epoch": 118, "train_loss": 0.42407954692840577, "val_loss": 0.3927286171913147, "val_acc": 0.87}, {"epoch": 119, "train_loss": 0.4213500279188156, "val_loss": 0.3960737800598145, "val_acc": 0.85}, {"epoch": 120, "train_loss": 0.4211346137523651, "val_loss": 0.3941029953956604, "val_acc": 0.865}, {"epoch": 121, "train_loss": 0.42185752749443056, "val_loss": 0.3935115051269531, "val_acc": 0.865}, {"epoch": 122, "train_loss": 0.4196591806411743, "val_loss": 0.3938738203048706, "val_acc": 0.865}, {"epoch": 123, "train_loss": 0.4204188060760498, "val_loss": 0.394530234336853, "val_acc": 0.865}, {"epoch": 124, "train_loss": 0.41924811244010923, "val_loss": 0.3934489393234253, "val_acc": 0.87}, {"epoch": 125, "train_loss": 0.42216920137405395, "val_loss": 0.39409934520721435, "val_acc": 0.855}, {"epoch": 126, "train_loss": 0.41946716904640197, "val_loss": 0.39399017572402956, "val_acc": 0.865}, {"epoch": 127, "train_loss": 0.41989227175712585, "val_loss": 0.3935039377212524, "val_acc": 0.87}, {"epoch": 128, "train_loss": 0.41931063055992124, "val_loss": 0.39435150861740115, "val_acc": 0.87}, {"epoch": 129, "train_loss": 0.4234549927711487, "val_loss": 0.39317241191864016, "val_acc": 0.87}, {"epoch": 130, "train_loss": 0.4213842213153839, "val_loss": 0.40024466037750245, "val_acc": 0.85}, {"epoch": 131, "train_loss": 0.4214139246940613, "val_loss": 0.3962597918510437, "val_acc": 0.865}, {"epoch": 132, "train_loss": 0.4195429277420044, "val_loss": 0.39293240308761596, "val_acc": 0.865}, {"epoch": 133, "train_loss": 0.4231331789493561, "val_loss": 0.39505051374435424, "val_acc": 0.865}, {"epoch": 134, "train_loss": 0.4196092486381531, "val_loss": 0.3932952785491943, "val_acc": 0.85}, {"epoch": 135, "train_loss": 0.417849440574646, "val_loss": 0.3932775616645813, "val_acc": 0.865}, {"epoch": 136, "train_loss": 0.41883262395858767, "val_loss": 0.39461074113845823, "val_acc": 0.86}, {"epoch": 137, "train_loss": 0.41737505316734314, "val_loss": 0.39277297496795655, "val_acc": 0.865}, {"epoch": 138, "train_loss": 0.417697948217392, "val_loss": 0.3929186749458313, "val_acc": 0.87}, {"epoch": 139, "train_loss": 0.41850183606147767, "val_loss": 0.3945894169807434, "val_acc": 0.865}, {"epoch": 140, "train_loss": 0.4177137565612793, "val_loss": 0.39301362276077273, "val_acc": 0.87}], "final_metrics": {"epoch": 140, "train_loss": 0.4177137565612793, "val_loss": 0.39301362276077273, "val_acc": 0.87}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 90), nn.ReLU(), nn.Linear(90, 45), nn.ReLU(), nn.Linear(45, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.00035)\nself.training_epochs = 135", "history": [{"epoch": 1, "train_loss": 1.0767209720611572, "val_loss": 1.0730793142318726, "val_acc": 0.33}, {"epoch": 2, "train_loss": 1.0422165775299073, "val_loss": 1.0533458137512206, "val_acc": 0.315}, {"epoch": 3, "train_loss": 1.02049569606781, "val_loss": 1.0361093187332153, "val_acc": 0.31}, {"epoch": 4, "train_loss": 0.9979723429679871, "val_loss": 1.0127792167663574, "val_acc": 0.31}, {"epoch": 5, "train_loss": 0.9766299986839294, "val_loss": 0.9906177258491516, "val_acc": 0.35}, {"epoch": 6, "train_loss": 0.9535192775726319, "val_loss": 0.9634240198135376, "val_acc": 0.455}, {"epoch": 7, "train_loss": 0.9265771555900574, "val_loss": 0.936317036151886, "val_acc": 0.535}, {"epoch": 8, "train_loss": 0.8971801733970642, "val_loss": 0.9075378370285034, "val_acc": 0.585}, {"epoch": 9, "train_loss": 0.8653017377853394, "val_loss": 0.8747319269180298, "val_acc": 0.615}, {"epoch": 10, "train_loss": 0.8329396319389343, "val_loss": 0.8443988394737244, "val_acc": 0.645}, {"epoch": 11, "train_loss": 0.8002584171295166, "val_loss": 0.806043918132782, "val_acc": 0.665}, {"epoch": 12, "train_loss": 0.7650601100921631, "val_loss": 0.7728062248229981, "val_acc": 0.68}, {"epoch": 13, "train_loss": 0.7337220215797424, "val_loss": 0.743545470237732, "val_acc": 0.715}, {"epoch": 14, "train_loss": 0.7041466665267945, "val_loss": 0.7161617398262023, "val_acc": 0.715}, {"epoch": 15, "train_loss": 0.6776346707344055, "val_loss": 0.690027768611908, "val_acc": 0.725}, {"epoch": 16, "train_loss": 0.6523042511940003, "val_loss": 0.6629109883308411, "val_acc": 0.775}, {"epoch": 17, "train_loss": 0.6313909602165222, "val_loss": 0.640677855014801, "val_acc": 0.76}, {"epoch": 18, "train_loss": 0.6127304983139038, "val_loss": 0.6206641411781311, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.5928811120986939, "val_loss": 0.6030774903297424, "val_acc": 0.77}, {"epoch": 20, "train_loss": 0.5759649968147278, "val_loss": 0.5859246635437012, "val_acc": 0.815}, {"epoch": 21, "train_loss": 0.5615212750434876, "val_loss": 0.5717148303985595, "val_acc": 0.8}, {"epoch": 22, "train_loss": 0.5479676508903504, "val_loss": 0.554256420135498, "val_acc": 0.84}, {"epoch": 23, "train_loss": 0.536798894405365, "val_loss": 0.5425350856781006, "val_acc": 0.815}, {"epoch": 24, "train_loss": 0.5257006621360779, "val_loss": 0.5325853991508483, "val_acc": 0.825}, {"epoch": 25, "train_loss": 0.5163445806503296, "val_loss": 0.5224115276336669, "val_acc": 0.83}, {"epoch": 26, "train_loss": 0.5085131061077118, "val_loss": 0.5132401204109192, "val_acc": 0.855}, {"epoch": 27, "train_loss": 0.5000698840618134, "val_loss": 0.5049248468875885, "val_acc": 0.845}, {"epoch": 28, "train_loss": 0.4953140354156494, "val_loss": 0.4997386431694031, "val_acc": 0.85}, {"epoch": 29, "train_loss": 0.48783851623535157, "val_loss": 0.4919756734371185, "val_acc": 0.825}, {"epoch": 30, "train_loss": 0.4816549181938171, "val_loss": 0.48537843823432925, "val_acc": 0.85}, {"epoch": 31, "train_loss": 0.47621726274490356, "val_loss": 0.4794305324554443, "val_acc": 0.855}, {"epoch": 32, "train_loss": 0.4732691240310669, "val_loss": 0.4753677475452423, "val_acc": 0.845}, {"epoch": 33, "train_loss": 0.4681818974018097, "val_loss": 0.4706847548484802, "val_acc": 0.86}, {"epoch": 34, "train_loss": 0.46644967794418335, "val_loss": 0.46480595707893374, "val_acc": 0.86}, {"epoch": 35, "train_loss": 0.46145015954971313, "val_loss": 0.46198718428611757, "val_acc": 0.865}, {"epoch": 36, "train_loss": 0.4575176215171814, "val_loss": 0.4597507417201996, "val_acc": 0.85}, {"epoch": 37, "train_loss": 0.45742413520812986, "val_loss": 0.45694149255752564, "val_acc": 0.855}, {"epoch": 38, "train_loss": 0.452803316116333, "val_loss": 0.4525232577323914, "val_acc": 0.845}, {"epoch": 39, "train_loss": 0.4526948308944702, "val_loss": 0.45075157165527346, "val_acc": 0.87}, {"epoch": 40, "train_loss": 0.44914174437522886, "val_loss": 0.4485279822349548, "val_acc": 0.85}, {"epoch": 41, "train_loss": 0.4479939842224121, "val_loss": 0.44782166838645937, "val_acc": 0.85}, {"epoch": 42, "train_loss": 0.4455475163459778, "val_loss": 0.44517714500427247, "val_acc": 0.87}, {"epoch": 43, "train_loss": 0.44314191102981565, "val_loss": 0.4417670178413391, "val_acc": 0.855}, {"epoch": 44, "train_loss": 0.44243986129760743, "val_loss": 0.4400432074069977, "val_acc": 0.86}, {"epoch": 45, "train_loss": 0.4394218873977661, "val_loss": 0.4394734239578247, "val_acc": 0.865}, {"epoch": 46, "train_loss": 0.43836944699287417, "val_loss": 0.4378328800201416, "val_acc": 0.865}, {"epoch": 47, "train_loss": 0.4386396098136902, "val_loss": 0.4371533489227295, "val_acc": 0.845}, {"epoch": 48, "train_loss": 0.4365415441989899, "val_loss": 0.43507738947868346, "val_acc": 0.87}, {"epoch": 49, "train_loss": 0.43478477478027344, "val_loss": 0.43387948870658877, "val_acc": 0.86}, {"epoch": 50, "train_loss": 0.4332085371017456, "val_loss": 0.43511284947395323, "val_acc": 0.855}, {"epoch": 51, "train_loss": 0.4335540533065796, "val_loss": 0.4345464515686035, "val_acc": 0.86}, {"epoch": 52, "train_loss": 0.43178754091262816, "val_loss": 0.43331568479537963, "val_acc": 0.86}, {"epoch": 53, "train_loss": 0.4310761475563049, "val_loss": 0.43193037509918214, "val_acc": 0.865}, {"epoch": 54, "train_loss": 0.43022644996643067, "val_loss": 0.4314795982837677, "val_acc": 0.855}, {"epoch": 55, "train_loss": 0.4297364342212677, "val_loss": 0.4312209486961365, "val_acc": 0.85}, {"epoch": 56, "train_loss": 0.42865151166915894, "val_loss": 0.42922794818878174, "val_acc": 0.865}, {"epoch": 57, "train_loss": 0.4313914740085602, "val_loss": 0.4305735158920288, "val_acc": 0.865}, {"epoch": 58, "train_loss": 0.42788568139076233, "val_loss": 0.4290494632720947, "val_acc": 0.85}, {"epoch": 59, "train_loss": 0.4294696688652039, "val_loss": 0.42924888134002687, "val_acc": 0.845}, {"epoch": 60, "train_loss": 0.4275307631492615, "val_loss": 0.42747593760490415, "val_acc": 0.855}, {"epoch": 61, "train_loss": 0.4262975287437439, "val_loss": 0.42863873958587645, "val_acc": 0.85}, {"epoch": 62, "train_loss": 0.4265700078010559, "val_loss": 0.4278905642032623, "val_acc": 0.85}, {"epoch": 63, "train_loss": 0.42414039611816406, "val_loss": 0.42677897214889526, "val_acc": 0.85}, {"epoch": 64, "train_loss": 0.4246254086494446, "val_loss": 0.42609097957611086, "val_acc": 0.86}, {"epoch": 65, "train_loss": 0.42332078456878663, "val_loss": 0.4275246524810791, "val_acc": 0.865}, {"epoch": 66, "train_loss": 0.4234270453453064, "val_loss": 0.4272009313106537, "val_acc": 0.855}, {"epoch": 67, "train_loss": 0.42231218099594114, "val_loss": 0.42634756445884703, "val_acc": 0.865}, {"epoch": 68, "train_loss": 0.4239139842987061, "val_loss": 0.42607632994651795, "val_acc": 0.845}, {"epoch": 69, "train_loss": 0.4224327540397644, "val_loss": 0.4285377812385559, "val_acc": 0.845}, {"epoch": 70, "train_loss": 0.42159821271896364, "val_loss": 0.42540827751159666, "val_acc": 0.85}, {"epoch": 71, "train_loss": 0.4224649488925934, "val_loss": 0.4260177397727966, "val_acc": 0.84}, {"epoch": 72, "train_loss": 0.4228884887695312, "val_loss": 0.42445378422737123, "val_acc": 0.85}, {"epoch": 73, "train_loss": 0.42230417728424074, "val_loss": 0.426325865983963, "val_acc": 0.865}, {"epoch": 74, "train_loss": 0.420030198097229, "val_loss": 0.4253618955612183, "val_acc": 0.845}, {"epoch": 75, "train_loss": 0.42049177408218386, "val_loss": 0.4255968189239502, "val_acc": 0.86}, {"epoch": 76, "train_loss": 0.4199738121032715, "val_loss": 0.42660446882247927, "val_acc": 0.85}, {"epoch": 77, "train_loss": 0.42158012628555297, "val_loss": 0.42476642727851865, "val_acc": 0.86}, {"epoch": 78, "train_loss": 0.41999470829963687, "val_loss": 0.4254422962665558, "val_acc": 0.855}, {"epoch": 79, "train_loss": 0.4201505160331726, "val_loss": 0.42607410430908205, "val_acc": 0.855}, {"epoch": 80, "train_loss": 0.41895834922790526, "val_loss": 0.4266138243675232, "val_acc": 0.85}, {"epoch": 81, "train_loss": 0.4208587312698364, "val_loss": 0.4257155042886734, "val_acc": 0.835}, {"epoch": 82, "train_loss": 0.4170143270492554, "val_loss": 0.42707824289798735, "val_acc": 0.85}, {"epoch": 83, "train_loss": 0.4211568021774292, "val_loss": 0.42560730934143065, "val_acc": 0.86}, {"epoch": 84, "train_loss": 0.4188284361362457, "val_loss": 0.4267924749851227, "val_acc": 0.845}, {"epoch": 85, "train_loss": 0.4178782594203949, "val_loss": 0.42422918140888216, "val_acc": 0.86}, {"epoch": 86, "train_loss": 0.4174965953826904, "val_loss": 0.4259575408697128, "val_acc": 0.86}, {"epoch": 87, "train_loss": 0.41675460457801816, "val_loss": 0.4265344351530075, "val_acc": 0.86}, {"epoch": 88, "train_loss": 0.4185693430900574, "val_loss": 0.4275243926048279, "val_acc": 0.845}, {"epoch": 89, "train_loss": 0.4172958981990814, "val_loss": 0.4246184700727463, "val_acc": 0.86}, {"epoch": 90, "train_loss": 0.41844609975814817, "val_loss": 0.4253411841392517, "val_acc": 0.835}, {"epoch": 91, "train_loss": 0.4162669515609741, "val_loss": 0.425125344991684, "val_acc": 0.855}, {"epoch": 92, "train_loss": 0.41689040303230285, "val_loss": 0.4262688183784485, "val_acc": 0.855}, {"epoch": 93, "train_loss": 0.41702377557754516, "val_loss": 0.4268267369270325, "val_acc": 0.855}, {"epoch": 94, "train_loss": 0.4160302758216858, "val_loss": 0.42562866151332857, "val_acc": 0.835}, {"epoch": 95, "train_loss": 0.4158492410182953, "val_loss": 0.4243014121055603, "val_acc": 0.865}, {"epoch": 96, "train_loss": 0.416977516412735, "val_loss": 0.42729951739311217, "val_acc": 0.86}, {"epoch": 97, "train_loss": 0.41570253372192384, "val_loss": 0.4260857915878296, "val_acc": 0.845}, {"epoch": 98, "train_loss": 0.4152436947822571, "val_loss": 0.4275269567966461, "val_acc": 0.855}, {"epoch": 99, "train_loss": 0.41483587861061094, "val_loss": 0.4278072327375412, "val_acc": 0.845}, {"epoch": 100, "train_loss": 0.4149469518661499, "val_loss": 0.4265567010641098, "val_acc": 0.845}, {"epoch": 101, "train_loss": 0.41443307518959044, "val_loss": 0.4261291217803955, "val_acc": 0.85}, {"epoch": 102, "train_loss": 0.41575655341148376, "val_loss": 0.4248093295097351, "val_acc": 0.85}, {"epoch": 103, "train_loss": 0.41501556158065794, "val_loss": 0.42644999325275423, "val_acc": 0.855}, {"epoch": 104, "train_loss": 0.415781614780426, "val_loss": 0.42702944040298463, "val_acc": 0.855}, {"epoch": 105, "train_loss": 0.4145813870429993, "val_loss": 0.42687817454338073, "val_acc": 0.855}, {"epoch": 106, "train_loss": 0.41440771222114564, "val_loss": 0.4253070378303528, "val_acc": 0.85}, {"epoch": 107, "train_loss": 0.41547566175460815, "val_loss": 0.42646603465080263, "val_acc": 0.86}, {"epoch": 108, "train_loss": 0.4138381052017212, "val_loss": 0.42522564589977263, "val_acc": 0.845}, {"epoch": 109, "train_loss": 0.4142658078670502, "val_loss": 0.4281341302394867, "val_acc": 0.86}, {"epoch": 110, "train_loss": 0.4137689816951752, "val_loss": 0.42772199094295504, "val_acc": 0.845}, {"epoch": 111, "train_loss": 0.41719019412994385, "val_loss": 0.4290491199493408, "val_acc": 0.855}, {"epoch": 112, "train_loss": 0.4143096888065338, "val_loss": 0.4269713008403778, "val_acc": 0.845}, {"epoch": 113, "train_loss": 0.4132356071472168, "val_loss": 0.42750306785106656, "val_acc": 0.855}, {"epoch": 114, "train_loss": 0.4126544535160065, "val_loss": 0.42623872637748716, "val_acc": 0.86}, {"epoch": 115, "train_loss": 0.4171335792541504, "val_loss": 0.42527249693870545, "val_acc": 0.86}, {"epoch": 116, "train_loss": 0.4150953531265259, "val_loss": 0.4295617151260376, "val_acc": 0.835}, {"epoch": 117, "train_loss": 0.4122005462646484, "val_loss": 0.4259302747249603, "val_acc": 0.855}, {"epoch": 118, "train_loss": 0.4121441912651062, "val_loss": 0.42884127497673036, "val_acc": 0.85}, {"epoch": 119, "train_loss": 0.4119974946975708, "val_loss": 0.42789722681045533, "val_acc": 0.84}, {"epoch": 120, "train_loss": 0.41238474726676944, "val_loss": 0.42680885791778567, "val_acc": 0.855}, {"epoch": 121, "train_loss": 0.41355241417884825, "val_loss": 0.4276355242729187, "val_acc": 0.84}, {"epoch": 122, "train_loss": 0.4119421935081482, "val_loss": 0.42704065918922424, "val_acc": 0.86}, {"epoch": 123, "train_loss": 0.41171458959579466, "val_loss": 0.4268826508522034, "val_acc": 0.855}, {"epoch": 124, "train_loss": 0.41161973476409913, "val_loss": 0.42774122059345243, "val_acc": 0.835}, {"epoch": 125, "train_loss": 0.4132138013839722, "val_loss": 0.4274293124675751, "val_acc": 0.835}, {"epoch": 126, "train_loss": 0.412023583650589, "val_loss": 0.42809975147247314, "val_acc": 0.86}, {"epoch": 127, "train_loss": 0.4142597043514252, "val_loss": 0.42915090441703796, "val_acc": 0.86}, {"epoch": 128, "train_loss": 0.41163756370544435, "val_loss": 0.42775886714458466, "val_acc": 0.85}, {"epoch": 129, "train_loss": 0.4112728786468506, "val_loss": 0.4270861029624939, "val_acc": 0.845}, {"epoch": 130, "train_loss": 0.41088891625404356, "val_loss": 0.42592661678791044, "val_acc": 0.85}, {"epoch": 131, "train_loss": 0.41067068576812743, "val_loss": 0.42870551645755767, "val_acc": 0.855}, {"epoch": 132, "train_loss": 0.4118092942237854, "val_loss": 0.4284707593917847, "val_acc": 0.84}, {"epoch": 133, "train_loss": 0.41132023453712463, "val_loss": 0.4294003653526306, "val_acc": 0.85}, {"epoch": 134, "train_loss": 0.41123916864395144, "val_loss": 0.4285983073711395, "val_acc": 0.835}, {"epoch": 135, "train_loss": 0.4121480119228363, "val_loss": 0.4288682770729065, "val_acc": 0.855}], "final_metrics": {"epoch": 135, "train_loss": 0.4121480119228363, "val_loss": 0.4288682770729065, "val_acc": 0.855}}
