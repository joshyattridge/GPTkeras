{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 3))\nself.optimizer = torch.optim.Adam(self.layers.parameters(), lr=0.001)\nself.training_epochs = 100", "history": [{"epoch": 1, "train_loss": 1.0595626258850097, "val_loss": 1.0068160724639892, "val_acc": 0.43}, {"epoch": 2, "train_loss": 1.0088771963119507, "val_loss": 0.9623183250427246, "val_acc": 0.43}, {"epoch": 3, "train_loss": 0.9700658178329468, "val_loss": 0.9294786977767945, "val_acc": 0.54}, {"epoch": 4, "train_loss": 0.9299127888679505, "val_loss": 0.889850823879242, "val_acc": 0.605}, {"epoch": 5, "train_loss": 0.8875908803939819, "val_loss": 0.8442879891395569, "val_acc": 0.64}, {"epoch": 6, "train_loss": 0.8435244464874267, "val_loss": 0.8002852511405945, "val_acc": 0.725}, {"epoch": 7, "train_loss": 0.7993751764297485, "val_loss": 0.7495915222167969, "val_acc": 0.745}, {"epoch": 8, "train_loss": 0.752416410446167, "val_loss": 0.7031195831298828, "val_acc": 0.785}, {"epoch": 9, "train_loss": 0.7088494110107422, "val_loss": 0.6634768080711365, "val_acc": 0.83}, {"epoch": 10, "train_loss": 0.6688266515731811, "val_loss": 0.6215266036987305, "val_acc": 0.82}, {"epoch": 11, "train_loss": 0.6335796618461609, "val_loss": 0.5972605466842651, "val_acc": 0.84}, {"epoch": 12, "train_loss": 0.6054159259796142, "val_loss": 0.5637059164047241, "val_acc": 0.84}, {"epoch": 13, "train_loss": 0.58044109582901, "val_loss": 0.5466242289543152, "val_acc": 0.835}, {"epoch": 14, "train_loss": 0.5606176424026489, "val_loss": 0.5242197895050049, "val_acc": 0.84}, {"epoch": 15, "train_loss": 0.5446358013153076, "val_loss": 0.5058375597000122, "val_acc": 0.835}, {"epoch": 16, "train_loss": 0.5313220334053039, "val_loss": 0.496264488697052, "val_acc": 0.84}, {"epoch": 17, "train_loss": 0.5207154881954194, "val_loss": 0.4857616829872131, "val_acc": 0.845}, {"epoch": 18, "train_loss": 0.5121553277969361, "val_loss": 0.4788446712493897, "val_acc": 0.84}, {"epoch": 19, "train_loss": 0.5039226293563843, "val_loss": 0.4706725478172302, "val_acc": 0.84}, {"epoch": 20, "train_loss": 0.5004364061355591, "val_loss": 0.46253137111663817, "val_acc": 0.835}, {"epoch": 21, "train_loss": 0.4940879559516907, "val_loss": 0.4821743845939636, "val_acc": 0.78}, {"epoch": 22, "train_loss": 0.4971613430976868, "val_loss": 0.4494232869148254, "val_acc": 0.83}, {"epoch": 23, "train_loss": 0.4882037615776062, "val_loss": 0.4697067975997925, "val_acc": 0.785}, {"epoch": 24, "train_loss": 0.4891538965702057, "val_loss": 0.44364375591278077, "val_acc": 0.84}, {"epoch": 25, "train_loss": 0.4796245360374451, "val_loss": 0.44748664617538453, "val_acc": 0.835}, {"epoch": 26, "train_loss": 0.4780886459350586, "val_loss": 0.4479010915756226, "val_acc": 0.82}, {"epoch": 27, "train_loss": 0.4736859893798828, "val_loss": 0.44094181537628174, "val_acc": 0.835}, {"epoch": 28, "train_loss": 0.4753070831298828, "val_loss": 0.43806756973266603, "val_acc": 0.845}, {"epoch": 29, "train_loss": 0.475564923286438, "val_loss": 0.4483862090110779, "val_acc": 0.805}, {"epoch": 30, "train_loss": 0.47125734329223634, "val_loss": 0.4380440020561218, "val_acc": 0.835}, {"epoch": 31, "train_loss": 0.47580655217170714, "val_loss": 0.4314389657974243, "val_acc": 0.84}, {"epoch": 32, "train_loss": 0.4720836925506592, "val_loss": 0.44179199934005736, "val_acc": 0.83}, {"epoch": 33, "train_loss": 0.4717879545688629, "val_loss": 0.4305768203735352, "val_acc": 0.84}, {"epoch": 34, "train_loss": 0.46746046423912047, "val_loss": 0.43887594223022464, "val_acc": 0.81}, {"epoch": 35, "train_loss": 0.46707565784454347, "val_loss": 0.43564457416534424, "val_acc": 0.825}, {"epoch": 36, "train_loss": 0.4658014631271362, "val_loss": 0.4367023801803589, "val_acc": 0.805}, {"epoch": 37, "train_loss": 0.4639891576766968, "val_loss": 0.43280332565307617, "val_acc": 0.845}, {"epoch": 38, "train_loss": 0.4736816143989563, "val_loss": 0.44531540393829344, "val_acc": 0.8}, {"epoch": 39, "train_loss": 0.4723925924301147, "val_loss": 0.4274821019172668, "val_acc": 0.83}, {"epoch": 40, "train_loss": 0.46514490485191345, "val_loss": 0.43456000804901124, "val_acc": 0.82}, {"epoch": 41, "train_loss": 0.46521347522735595, "val_loss": 0.4320386910438538, "val_acc": 0.84}, {"epoch": 42, "train_loss": 0.461769802570343, "val_loss": 0.4416393041610718, "val_acc": 0.8}, {"epoch": 43, "train_loss": 0.4625997948646545, "val_loss": 0.43336718559265136, "val_acc": 0.825}, {"epoch": 44, "train_loss": 0.46819051265716555, "val_loss": 0.4402057337760925, "val_acc": 0.81}, {"epoch": 45, "train_loss": 0.4676187694072723, "val_loss": 0.4304760766029358, "val_acc": 0.845}, {"epoch": 46, "train_loss": 0.46479379892349243, "val_loss": 0.44810322284698484, "val_acc": 0.795}, {"epoch": 47, "train_loss": 0.4655420029163361, "val_loss": 0.4322468090057373, "val_acc": 0.82}, {"epoch": 48, "train_loss": 0.45958305478096007, "val_loss": 0.4408095359802246, "val_acc": 0.8}, {"epoch": 49, "train_loss": 0.4589424824714661, "val_loss": 0.431742901802063, "val_acc": 0.825}, {"epoch": 50, "train_loss": 0.4596033310890198, "val_loss": 0.4414495778083801, "val_acc": 0.81}, {"epoch": 51, "train_loss": 0.46168169260025027, "val_loss": 0.4345774006843567, "val_acc": 0.81}, {"epoch": 52, "train_loss": 0.45955071091651917, "val_loss": 0.4290773916244507, "val_acc": 0.81}, {"epoch": 53, "train_loss": 0.46107982873916625, "val_loss": 0.4369751667976379, "val_acc": 0.815}, {"epoch": 54, "train_loss": 0.456864640712738, "val_loss": 0.43821356296539304, "val_acc": 0.815}, {"epoch": 55, "train_loss": 0.45573943972587583, "val_loss": 0.4371085381507874, "val_acc": 0.81}, {"epoch": 56, "train_loss": 0.45654585123062136, "val_loss": 0.4348832941055298, "val_acc": 0.82}, {"epoch": 57, "train_loss": 0.4578931295871735, "val_loss": 0.4382275891304016, "val_acc": 0.81}, {"epoch": 58, "train_loss": 0.4554137229919434, "val_loss": 0.4340892934799194, "val_acc": 0.815}, {"epoch": 59, "train_loss": 0.4564861989021301, "val_loss": 0.43723340272903444, "val_acc": 0.805}, {"epoch": 60, "train_loss": 0.45507694125175474, "val_loss": 0.4326503300666809, "val_acc": 0.83}, {"epoch": 61, "train_loss": 0.4571441161632538, "val_loss": 0.43094430685043333, "val_acc": 0.815}, {"epoch": 62, "train_loss": 0.45492727279663087, "val_loss": 0.44155346632003784, "val_acc": 0.81}, {"epoch": 63, "train_loss": 0.4562655782699585, "val_loss": 0.43809410095214846, "val_acc": 0.81}, {"epoch": 64, "train_loss": 0.4570233583450317, "val_loss": 0.4292604494094849, "val_acc": 0.83}, {"epoch": 65, "train_loss": 0.4576853823661804, "val_loss": 0.44569414138793945, "val_acc": 0.81}, {"epoch": 66, "train_loss": 0.4566004228591919, "val_loss": 0.4303150749206543, "val_acc": 0.83}, {"epoch": 67, "train_loss": 0.4548638153076172, "val_loss": 0.4370235204696655, "val_acc": 0.82}, {"epoch": 68, "train_loss": 0.45939022064208984, "val_loss": 0.42897441148757937, "val_acc": 0.835}, {"epoch": 69, "train_loss": 0.45224732637405396, "val_loss": 0.4453179931640625, "val_acc": 0.8}, {"epoch": 70, "train_loss": 0.4568363642692566, "val_loss": 0.426938533782959, "val_acc": 0.845}, {"epoch": 71, "train_loss": 0.46187887907028197, "val_loss": 0.4511476135253906, "val_acc": 0.81}, {"epoch": 72, "train_loss": 0.45409263014793394, "val_loss": 0.42717321872711184, "val_acc": 0.83}, {"epoch": 73, "train_loss": 0.4546369457244873, "val_loss": 0.44174129962921144, "val_acc": 0.805}, {"epoch": 74, "train_loss": 0.46206037044525144, "val_loss": 0.43355226039886474, "val_acc": 0.81}, {"epoch": 75, "train_loss": 0.4551782190799713, "val_loss": 0.43554909944534304, "val_acc": 0.835}, {"epoch": 76, "train_loss": 0.4540405786037445, "val_loss": 0.4343737721443176, "val_acc": 0.815}, {"epoch": 77, "train_loss": 0.4533762764930725, "val_loss": 0.43941703557968137, "val_acc": 0.81}, {"epoch": 78, "train_loss": 0.4515989124774933, "val_loss": 0.4317787742614746, "val_acc": 0.82}, {"epoch": 79, "train_loss": 0.4518492817878723, "val_loss": 0.4423226523399353, "val_acc": 0.805}, {"epoch": 80, "train_loss": 0.450393648147583, "val_loss": 0.4336076831817627, "val_acc": 0.805}, {"epoch": 81, "train_loss": 0.4564161789417267, "val_loss": 0.4313393688201904, "val_acc": 0.83}, {"epoch": 82, "train_loss": 0.4567489504814148, "val_loss": 0.4425631356239319, "val_acc": 0.81}, {"epoch": 83, "train_loss": 0.4505826866626739, "val_loss": 0.4376764059066772, "val_acc": 0.805}, {"epoch": 84, "train_loss": 0.45216067910194396, "val_loss": 0.4419706654548645, "val_acc": 0.8}, {"epoch": 85, "train_loss": 0.4544227910041809, "val_loss": 0.4406378936767578, "val_acc": 0.815}, {"epoch": 86, "train_loss": 0.4515754795074463, "val_loss": 0.44186681509017944, "val_acc": 0.81}, {"epoch": 87, "train_loss": 0.45092101097106935, "val_loss": 0.43567403554916384, "val_acc": 0.82}, {"epoch": 88, "train_loss": 0.4510562264919281, "val_loss": 0.44145511388778685, "val_acc": 0.805}, {"epoch": 89, "train_loss": 0.4491230618953705, "val_loss": 0.4411036062240601, "val_acc": 0.81}, {"epoch": 90, "train_loss": 0.450222282409668, "val_loss": 0.4377379822731018, "val_acc": 0.81}, {"epoch": 91, "train_loss": 0.44895791411399844, "val_loss": 0.43657986879348754, "val_acc": 0.805}, {"epoch": 92, "train_loss": 0.4528678715229034, "val_loss": 0.4358322954177856, "val_acc": 0.81}, {"epoch": 93, "train_loss": 0.45260641813278196, "val_loss": 0.4347193646430969, "val_acc": 0.82}, {"epoch": 94, "train_loss": 0.45745558381080625, "val_loss": 0.4454478096961975, "val_acc": 0.805}, {"epoch": 95, "train_loss": 0.4566746318340302, "val_loss": 0.44161606550216675, "val_acc": 0.83}, {"epoch": 96, "train_loss": 0.4467658638954163, "val_loss": 0.4391321659088135, "val_acc": 0.82}, {"epoch": 97, "train_loss": 0.4504458475112915, "val_loss": 0.44497140884399417, "val_acc": 0.805}, {"epoch": 98, "train_loss": 0.4497568917274475, "val_loss": 0.43614198923110964, "val_acc": 0.81}, {"epoch": 99, "train_loss": 0.45103605270385744, "val_loss": 0.43980628490448, "val_acc": 0.815}, {"epoch": 100, "train_loss": 0.44945297479629515, "val_loss": 0.43658377408981325, "val_acc": 0.815}], "final_metrics": {"epoch": 100, "train_loss": 0.44945297479629515, "val_loss": 0.43658377408981325, "val_acc": 0.815}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 3))\nself.optimizer = torch.optim.Adam(self.layers.parameters(), lr=0.0005)\nself.training_epochs = 50", "history": [{"epoch": 1, "train_loss": 1.05742830991745, "val_loss": 0.975399329662323, "val_acc": 0.395}, {"epoch": 2, "train_loss": 0.9768313002586365, "val_loss": 0.9123545145988464, "val_acc": 0.535}, {"epoch": 3, "train_loss": 0.8997454333305359, "val_loss": 0.8288955664634705, "val_acc": 0.695}, {"epoch": 4, "train_loss": 0.8112583374977111, "val_loss": 0.7404750800132751, "val_acc": 0.745}, {"epoch": 5, "train_loss": 0.7206141018867492, "val_loss": 0.6573786878585816, "val_acc": 0.795}, {"epoch": 6, "train_loss": 0.6477230596542358, "val_loss": 0.6013747572898864, "val_acc": 0.805}, {"epoch": 7, "train_loss": 0.5931630420684815, "val_loss": 0.5434987497329712, "val_acc": 0.825}, {"epoch": 8, "train_loss": 0.5634096074104309, "val_loss": 0.5143990278244018, "val_acc": 0.815}, {"epoch": 9, "train_loss": 0.5425983893871308, "val_loss": 0.48390668153762817, "val_acc": 0.82}, {"epoch": 10, "train_loss": 0.5182130479812622, "val_loss": 0.4854900813102722, "val_acc": 0.81}, {"epoch": 11, "train_loss": 0.5042640829086303, "val_loss": 0.4717781949043274, "val_acc": 0.82}, {"epoch": 12, "train_loss": 0.4957637476921082, "val_loss": 0.474891095161438, "val_acc": 0.81}, {"epoch": 13, "train_loss": 0.49421090364456177, "val_loss": 0.4472127342224121, "val_acc": 0.83}, {"epoch": 14, "train_loss": 0.48918397188186646, "val_loss": 0.44221856355667116, "val_acc": 0.815}, {"epoch": 15, "train_loss": 0.4839588189125061, "val_loss": 0.4574938917160034, "val_acc": 0.8}, {"epoch": 16, "train_loss": 0.4797909379005432, "val_loss": 0.43713549137115476, "val_acc": 0.83}, {"epoch": 17, "train_loss": 0.4800537133216858, "val_loss": 0.43357095718383787, "val_acc": 0.81}, {"epoch": 18, "train_loss": 0.47819714903831484, "val_loss": 0.45491756916046144, "val_acc": 0.79}, {"epoch": 19, "train_loss": 0.47085410833358765, "val_loss": 0.42812246561050415, "val_acc": 0.825}, {"epoch": 20, "train_loss": 0.4742720532417297, "val_loss": 0.4788506937026977, "val_acc": 0.79}, {"epoch": 21, "train_loss": 0.4685840308666229, "val_loss": 0.43082679271698, "val_acc": 0.83}, {"epoch": 22, "train_loss": 0.47472209215164185, "val_loss": 0.4353518104553223, "val_acc": 0.835}, {"epoch": 23, "train_loss": 0.4746144986152649, "val_loss": 0.44810965061187746, "val_acc": 0.8}, {"epoch": 24, "train_loss": 0.4714666199684143, "val_loss": 0.4376767015457153, "val_acc": 0.81}, {"epoch": 25, "train_loss": 0.4701280510425568, "val_loss": 0.44229286193847656, "val_acc": 0.805}, {"epoch": 26, "train_loss": 0.4681528353691101, "val_loss": 0.4516985511779785, "val_acc": 0.795}, {"epoch": 27, "train_loss": 0.47327929854393, "val_loss": 0.43394999742507934, "val_acc": 0.815}, {"epoch": 28, "train_loss": 0.4629960596561432, "val_loss": 0.441421959400177, "val_acc": 0.805}, {"epoch": 29, "train_loss": 0.4637592601776123, "val_loss": 0.4312010931968689, "val_acc": 0.82}, {"epoch": 30, "train_loss": 0.47869061708450317, "val_loss": 0.46128058195114136, "val_acc": 0.81}, {"epoch": 31, "train_loss": 0.47799679279327395, "val_loss": 0.47041276454925535, "val_acc": 0.775}, {"epoch": 32, "train_loss": 0.4731379544734955, "val_loss": 0.43361793041229246, "val_acc": 0.83}, {"epoch": 33, "train_loss": 0.47171859741210936, "val_loss": 0.4542679333686829, "val_acc": 0.8}, {"epoch": 34, "train_loss": 0.47044564127922056, "val_loss": 0.44926856756210326, "val_acc": 0.81}, {"epoch": 35, "train_loss": 0.46295692920684817, "val_loss": 0.43013255834579467, "val_acc": 0.835}, {"epoch": 36, "train_loss": 0.4635706281661987, "val_loss": 0.43897677659988404, "val_acc": 0.82}, {"epoch": 37, "train_loss": 0.46061900854110716, "val_loss": 0.45455280065536496, "val_acc": 0.81}, {"epoch": 38, "train_loss": 0.4578274798393249, "val_loss": 0.4342868089675903, "val_acc": 0.835}, {"epoch": 39, "train_loss": 0.4607786893844604, "val_loss": 0.4543402004241943, "val_acc": 0.795}, {"epoch": 40, "train_loss": 0.4544782090187073, "val_loss": 0.4369900393486023, "val_acc": 0.825}, {"epoch": 41, "train_loss": 0.45775238633155824, "val_loss": 0.44195725202560426, "val_acc": 0.82}, {"epoch": 42, "train_loss": 0.4599119484424591, "val_loss": 0.45425578355789187, "val_acc": 0.795}, {"epoch": 43, "train_loss": 0.4626470184326172, "val_loss": 0.4346997499465942, "val_acc": 0.825}, {"epoch": 44, "train_loss": 0.45744449377059937, "val_loss": 0.43657121419906614, "val_acc": 0.815}, {"epoch": 45, "train_loss": 0.45876227617263793, "val_loss": 0.4447957634925842, "val_acc": 0.81}, {"epoch": 46, "train_loss": 0.45728047490119933, "val_loss": 0.43519312143325806, "val_acc": 0.83}, {"epoch": 47, "train_loss": 0.45645220041275025, "val_loss": 0.446121027469635, "val_acc": 0.805}, {"epoch": 48, "train_loss": 0.45511435747146606, "val_loss": 0.46673151254653933, "val_acc": 0.795}, {"epoch": 49, "train_loss": 0.4610969734191894, "val_loss": 0.4366114091873169, "val_acc": 0.81}, {"epoch": 50, "train_loss": 0.4597530329227448, "val_loss": 0.44319350719451905, "val_acc": 0.815}], "final_metrics": {"epoch": 50, "train_loss": 0.4597530329227448, "val_loss": 0.44319350719451905, "val_acc": 0.815}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 3))\nself.optimizer = torch.optim.Adam(self.layers.parameters(), lr=0.0003)\nself.training_epochs = 100", "history": [{"epoch": 1, "train_loss": 1.0174697947502136, "val_loss": 0.8920547676086426, "val_acc": 0.49}, {"epoch": 2, "train_loss": 0.8722138237953186, "val_loss": 0.7641806793212891, "val_acc": 0.665}, {"epoch": 3, "train_loss": 0.7283870077133179, "val_loss": 0.6284761238098144, "val_acc": 0.78}, {"epoch": 4, "train_loss": 0.6167803502082825, "val_loss": 0.5461131596565246, "val_acc": 0.825}, {"epoch": 5, "train_loss": 0.5563547992706299, "val_loss": 0.49891239643096924, "val_acc": 0.815}, {"epoch": 6, "train_loss": 0.5243417656421662, "val_loss": 0.49668627738952636, "val_acc": 0.785}, {"epoch": 7, "train_loss": 0.5196771240234375, "val_loss": 0.45971845149993895, "val_acc": 0.815}, {"epoch": 8, "train_loss": 0.48703383207321166, "val_loss": 0.4725551772117615, "val_acc": 0.805}, {"epoch": 9, "train_loss": 0.5110860109329224, "val_loss": 0.46798497438430786, "val_acc": 0.805}, {"epoch": 10, "train_loss": 0.5070964050292969, "val_loss": 0.4449088001251221, "val_acc": 0.815}, {"epoch": 11, "train_loss": 0.47676422357559206, "val_loss": 0.47934536933898925, "val_acc": 0.785}, {"epoch": 12, "train_loss": 0.4840715003013611, "val_loss": 0.44012500047683717, "val_acc": 0.815}, {"epoch": 13, "train_loss": 0.4716373491287231, "val_loss": 0.4623080110549927, "val_acc": 0.795}, {"epoch": 14, "train_loss": 0.4765660357475281, "val_loss": 0.44255794525146486, "val_acc": 0.815}, {"epoch": 15, "train_loss": 0.48474943041801455, "val_loss": 0.45498530387878416, "val_acc": 0.815}, {"epoch": 16, "train_loss": 0.49584357261657713, "val_loss": 0.5620286798477173, "val_acc": 0.755}, {"epoch": 17, "train_loss": 0.5073310732841492, "val_loss": 0.44993655443191527, "val_acc": 0.795}, {"epoch": 18, "train_loss": 0.4782196402549744, "val_loss": 0.43942041635513307, "val_acc": 0.805}, {"epoch": 19, "train_loss": 0.47398789048194884, "val_loss": 0.48446744680404663, "val_acc": 0.785}, {"epoch": 20, "train_loss": 0.4811925256252289, "val_loss": 0.4406015586853027, "val_acc": 0.81}, {"epoch": 21, "train_loss": 0.47430980324745176, "val_loss": 0.44452417850494386, "val_acc": 0.81}, {"epoch": 22, "train_loss": 0.47759639978408813, "val_loss": 0.4394661235809326, "val_acc": 0.825}, {"epoch": 23, "train_loss": 0.5042941379547119, "val_loss": 0.44566834688186646, "val_acc": 0.795}, {"epoch": 24, "train_loss": 0.5090550422668457, "val_loss": 0.44826685428619384, "val_acc": 0.81}, {"epoch": 25, "train_loss": 0.48809216618537904, "val_loss": 0.45732348918914795, "val_acc": 0.82}, {"epoch": 26, "train_loss": 0.47665265798568723, "val_loss": 0.4623699164390564, "val_acc": 0.795}, {"epoch": 27, "train_loss": 0.48391918182373045, "val_loss": 0.5042712926864624, "val_acc": 0.785}, {"epoch": 28, "train_loss": 0.4919442629814148, "val_loss": 0.48733829975128173, "val_acc": 0.78}, {"epoch": 29, "train_loss": 0.4681669211387634, "val_loss": 0.44474525928497316, "val_acc": 0.81}, {"epoch": 30, "train_loss": 0.4603960704803467, "val_loss": 0.43278478145599364, "val_acc": 0.83}, {"epoch": 31, "train_loss": 0.4629466426372528, "val_loss": 0.45340275049209594, "val_acc": 0.82}, {"epoch": 32, "train_loss": 0.4621902406215668, "val_loss": 0.4386313605308533, "val_acc": 0.82}, {"epoch": 33, "train_loss": 0.4637091875076294, "val_loss": 0.4341811203956604, "val_acc": 0.825}, {"epoch": 34, "train_loss": 0.46800618767738345, "val_loss": 0.44619773864746093, "val_acc": 0.825}, {"epoch": 35, "train_loss": 0.46006491899490354, "val_loss": 0.446684889793396, "val_acc": 0.82}, {"epoch": 36, "train_loss": 0.45965444326400756, "val_loss": 0.47729833364486696, "val_acc": 0.79}, {"epoch": 37, "train_loss": 0.46103121757507326, "val_loss": 0.43806124687194825, "val_acc": 0.81}, {"epoch": 38, "train_loss": 0.45662440299987794, "val_loss": 0.4545695972442627, "val_acc": 0.815}, {"epoch": 39, "train_loss": 0.45694002985954285, "val_loss": 0.44014089107513427, "val_acc": 0.82}, {"epoch": 40, "train_loss": 0.45980124473571776, "val_loss": 0.45294848442077634, "val_acc": 0.82}, {"epoch": 41, "train_loss": 0.46105772733688355, "val_loss": 0.4576251482963562, "val_acc": 0.805}, {"epoch": 42, "train_loss": 0.4557640600204468, "val_loss": 0.44029692649841307, "val_acc": 0.8}, {"epoch": 43, "train_loss": 0.4639893460273743, "val_loss": 0.4916092920303345, "val_acc": 0.795}, {"epoch": 44, "train_loss": 0.4619858551025391, "val_loss": 0.4451442337036133, "val_acc": 0.805}, {"epoch": 45, "train_loss": 0.4565838015079498, "val_loss": 0.4526989459991455, "val_acc": 0.81}, {"epoch": 46, "train_loss": 0.45872127532958984, "val_loss": 0.46252166032791137, "val_acc": 0.81}, {"epoch": 47, "train_loss": 0.4615571641921997, "val_loss": 0.4753943729400635, "val_acc": 0.8}, {"epoch": 48, "train_loss": 0.466476526260376, "val_loss": 0.4923898696899414, "val_acc": 0.8}, {"epoch": 49, "train_loss": 0.4630554211139679, "val_loss": 0.4734285593032837, "val_acc": 0.795}, {"epoch": 50, "train_loss": 0.4595000696182251, "val_loss": 0.4988755416870117, "val_acc": 0.795}, {"epoch": 51, "train_loss": 0.4546321666240692, "val_loss": 0.4425652456283569, "val_acc": 0.83}, {"epoch": 52, "train_loss": 0.45788938999176027, "val_loss": 0.4436618614196777, "val_acc": 0.825}, {"epoch": 53, "train_loss": 0.45249853491783143, "val_loss": 0.4502056837081909, "val_acc": 0.81}, {"epoch": 54, "train_loss": 0.45670764803886416, "val_loss": 0.4577386522293091, "val_acc": 0.815}, {"epoch": 55, "train_loss": 0.4615708136558533, "val_loss": 0.45893900394439696, "val_acc": 0.815}, {"epoch": 56, "train_loss": 0.4612317967414856, "val_loss": 0.44562928676605223, "val_acc": 0.815}, {"epoch": 57, "train_loss": 0.46030705332756044, "val_loss": 0.5035982418060303, "val_acc": 0.78}, {"epoch": 58, "train_loss": 0.4555705237388611, "val_loss": 0.45218739986419676, "val_acc": 0.81}, {"epoch": 59, "train_loss": 0.4534893488883972, "val_loss": 0.4514552354812622, "val_acc": 0.8}, {"epoch": 60, "train_loss": 0.4602083820104599, "val_loss": 0.4540027856826782, "val_acc": 0.805}, {"epoch": 61, "train_loss": 0.45287792682647704, "val_loss": 0.4463452196121216, "val_acc": 0.82}, {"epoch": 62, "train_loss": 0.447940092086792, "val_loss": 0.4825636339187622, "val_acc": 0.8}, {"epoch": 63, "train_loss": 0.4559413170814514, "val_loss": 0.45018343925476073, "val_acc": 0.82}, {"epoch": 64, "train_loss": 0.4587567138671875, "val_loss": 0.44042378902435303, "val_acc": 0.815}, {"epoch": 65, "train_loss": 0.4501571559906006, "val_loss": 0.4526842713356018, "val_acc": 0.815}, {"epoch": 66, "train_loss": 0.4566474175453186, "val_loss": 0.4765030527114868, "val_acc": 0.795}, {"epoch": 67, "train_loss": 0.45283912420272826, "val_loss": 0.47340786695480347, "val_acc": 0.8}, {"epoch": 68, "train_loss": 0.45023332595825194, "val_loss": 0.45743340015411377, "val_acc": 0.81}, {"epoch": 69, "train_loss": 0.44486829757690427, "val_loss": 0.43951269626617434, "val_acc": 0.825}, {"epoch": 70, "train_loss": 0.4515085744857788, "val_loss": 0.4596182608604431, "val_acc": 0.815}, {"epoch": 71, "train_loss": 0.4490704202651978, "val_loss": 0.47012657165527344, "val_acc": 0.795}, {"epoch": 72, "train_loss": 0.4462927484512329, "val_loss": 0.4984503698348999, "val_acc": 0.78}, {"epoch": 73, "train_loss": 0.44954691648483275, "val_loss": 0.4501209378242493, "val_acc": 0.8}, {"epoch": 74, "train_loss": 0.45188981771469117, "val_loss": 0.4995675754547119, "val_acc": 0.78}, {"epoch": 75, "train_loss": 0.46769924521446227, "val_loss": 0.45715370655059817, "val_acc": 0.81}, {"epoch": 76, "train_loss": 0.45754342317581176, "val_loss": 0.4742040777206421, "val_acc": 0.805}, {"epoch": 77, "train_loss": 0.4483020210266113, "val_loss": 0.46911814689636233, "val_acc": 0.8}, {"epoch": 78, "train_loss": 0.4443976092338562, "val_loss": 0.4649254989624023, "val_acc": 0.79}, {"epoch": 79, "train_loss": 0.4521749448776245, "val_loss": 0.5020469808578492, "val_acc": 0.775}, {"epoch": 80, "train_loss": 0.4603061556816101, "val_loss": 0.44635212659835816, "val_acc": 0.805}, {"epoch": 81, "train_loss": 0.448536376953125, "val_loss": 0.47000940799713137, "val_acc": 0.815}, {"epoch": 82, "train_loss": 0.4428216767311096, "val_loss": 0.4526730298995972, "val_acc": 0.805}, {"epoch": 83, "train_loss": 0.44279746532440184, "val_loss": 0.46995857954025266, "val_acc": 0.8}, {"epoch": 84, "train_loss": 0.439044326543808, "val_loss": 0.48829657554626466, "val_acc": 0.795}, {"epoch": 85, "train_loss": 0.44347391366958616, "val_loss": 0.4508706665039062, "val_acc": 0.815}, {"epoch": 86, "train_loss": 0.4444263660907745, "val_loss": 0.45642796754837034, "val_acc": 0.815}, {"epoch": 87, "train_loss": 0.44381893634796143, "val_loss": 0.4750388669967651, "val_acc": 0.81}, {"epoch": 88, "train_loss": 0.4394697570800781, "val_loss": 0.4571006536483765, "val_acc": 0.81}, {"epoch": 89, "train_loss": 0.4424291133880615, "val_loss": 0.4648869752883911, "val_acc": 0.805}, {"epoch": 90, "train_loss": 0.4446013331413269, "val_loss": 0.45956850051879883, "val_acc": 0.815}, {"epoch": 91, "train_loss": 0.44072386503219607, "val_loss": 0.4498399209976196, "val_acc": 0.815}, {"epoch": 92, "train_loss": 0.4441362249851227, "val_loss": 0.48326298475265506, "val_acc": 0.795}, {"epoch": 93, "train_loss": 0.4473669481277466, "val_loss": 0.4578929567337036, "val_acc": 0.815}, {"epoch": 94, "train_loss": 0.4415219843387604, "val_loss": 0.4729612731933594, "val_acc": 0.8}, {"epoch": 95, "train_loss": 0.4476910090446472, "val_loss": 0.5055745315551757, "val_acc": 0.795}, {"epoch": 96, "train_loss": 0.44209002137184145, "val_loss": 0.4669864892959595, "val_acc": 0.81}, {"epoch": 97, "train_loss": 0.4363152003288269, "val_loss": 0.45661283493041993, "val_acc": 0.81}, {"epoch": 98, "train_loss": 0.4382843732833862, "val_loss": 0.46797812938690186, "val_acc": 0.79}, {"epoch": 99, "train_loss": 0.43566734313964844, "val_loss": 0.4534124183654785, "val_acc": 0.805}, {"epoch": 100, "train_loss": 0.4379686713218689, "val_loss": 0.4571902894973755, "val_acc": 0.815}], "final_metrics": {"epoch": 100, "train_loss": 0.4379686713218689, "val_loss": 0.4571902894973755, "val_acc": 0.815}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0006)\nself.training_epochs = 100", "history": [{"epoch": 1, "train_loss": 1.1467168426513672, "val_loss": 1.0981527805328368, "val_acc": 0.395}, {"epoch": 2, "train_loss": 1.0525821685791015, "val_loss": 1.0229510927200318, "val_acc": 0.475}, {"epoch": 3, "train_loss": 1.0065190362930299, "val_loss": 0.9785563325881959, "val_acc": 0.485}, {"epoch": 4, "train_loss": 0.9759204936027527, "val_loss": 0.9464148378372192, "val_acc": 0.485}, {"epoch": 5, "train_loss": 0.9460350513458252, "val_loss": 0.9178861737251282, "val_acc": 0.54}, {"epoch": 6, "train_loss": 0.9158437180519104, "val_loss": 0.8874769330024719, "val_acc": 0.585}, {"epoch": 7, "train_loss": 0.8819150614738465, "val_loss": 0.8536124491691589, "val_acc": 0.625}, {"epoch": 8, "train_loss": 0.8476793122291565, "val_loss": 0.8177473330497742, "val_acc": 0.675}, {"epoch": 9, "train_loss": 0.810800883769989, "val_loss": 0.7815242385864258, "val_acc": 0.7}, {"epoch": 10, "train_loss": 0.7706522560119629, "val_loss": 0.7441159772872925, "val_acc": 0.765}, {"epoch": 11, "train_loss": 0.7319556474685669, "val_loss": 0.7026652956008911, "val_acc": 0.78}, {"epoch": 12, "train_loss": 0.6940419220924378, "val_loss": 0.6664413690567017, "val_acc": 0.82}, {"epoch": 13, "train_loss": 0.6599581933021545, "val_loss": 0.6355170249938965, "val_acc": 0.815}, {"epoch": 14, "train_loss": 0.6293179869651795, "val_loss": 0.6073082494735718, "val_acc": 0.83}, {"epoch": 15, "train_loss": 0.6051888418197632, "val_loss": 0.5824438524246216, "val_acc": 0.835}, {"epoch": 16, "train_loss": 0.5842472720146179, "val_loss": 0.5640590286254883, "val_acc": 0.83}, {"epoch": 17, "train_loss": 0.5658746862411499, "val_loss": 0.5466110706329346, "val_acc": 0.83}, {"epoch": 18, "train_loss": 0.5512442469596863, "val_loss": 0.5306653714179993, "val_acc": 0.83}, {"epoch": 19, "train_loss": 0.5385151672363281, "val_loss": 0.5183167886734009, "val_acc": 0.835}, {"epoch": 20, "train_loss": 0.5298837494850158, "val_loss": 0.5060278582572937, "val_acc": 0.84}, {"epoch": 21, "train_loss": 0.5173383605480194, "val_loss": 0.49572062730789185, "val_acc": 0.825}, {"epoch": 22, "train_loss": 0.5104787158966064, "val_loss": 0.4883779168128967, "val_acc": 0.825}, {"epoch": 23, "train_loss": 0.5049414074420929, "val_loss": 0.4841272664070129, "val_acc": 0.83}, {"epoch": 24, "train_loss": 0.4979959750175476, "val_loss": 0.4757137942314148, "val_acc": 0.83}, {"epoch": 25, "train_loss": 0.4952211356163025, "val_loss": 0.47692950248718263, "val_acc": 0.815}, {"epoch": 26, "train_loss": 0.49356077432632445, "val_loss": 0.4661607074737549, "val_acc": 0.815}, {"epoch": 27, "train_loss": 0.48339557766914365, "val_loss": 0.4714045548439026, "val_acc": 0.8}, {"epoch": 28, "train_loss": 0.48839890837669375, "val_loss": 0.45448587894439696, "val_acc": 0.835}, {"epoch": 29, "train_loss": 0.47924998044967654, "val_loss": 0.4623350524902344, "val_acc": 0.81}, {"epoch": 30, "train_loss": 0.47788898468017577, "val_loss": 0.45889710187911986, "val_acc": 0.805}, {"epoch": 31, "train_loss": 0.4754198980331421, "val_loss": 0.44920820236206055, "val_acc": 0.815}, {"epoch": 32, "train_loss": 0.4761179232597351, "val_loss": 0.45240151166915893, "val_acc": 0.84}, {"epoch": 33, "train_loss": 0.4696725153923035, "val_loss": 0.4504745054244995, "val_acc": 0.815}, {"epoch": 34, "train_loss": 0.4728363466262817, "val_loss": 0.44456509590148924, "val_acc": 0.835}, {"epoch": 35, "train_loss": 0.4707707929611206, "val_loss": 0.4535046410560608, "val_acc": 0.815}, {"epoch": 36, "train_loss": 0.47282554149627687, "val_loss": 0.4427499008178711, "val_acc": 0.825}, {"epoch": 37, "train_loss": 0.4655302846431732, "val_loss": 0.4447235155105591, "val_acc": 0.81}, {"epoch": 38, "train_loss": 0.46617096424102783, "val_loss": 0.44395616054534914, "val_acc": 0.81}, {"epoch": 39, "train_loss": 0.46475069761276244, "val_loss": 0.44485910892486574, "val_acc": 0.815}, {"epoch": 40, "train_loss": 0.46453803181648257, "val_loss": 0.4363896322250366, "val_acc": 0.83}, {"epoch": 41, "train_loss": 0.46282414317131043, "val_loss": 0.4428298568725586, "val_acc": 0.81}, {"epoch": 42, "train_loss": 0.46343215465545656, "val_loss": 0.44040861845016477, "val_acc": 0.83}, {"epoch": 43, "train_loss": 0.46197046518325807, "val_loss": 0.4384927868843079, "val_acc": 0.815}, {"epoch": 44, "train_loss": 0.4646203351020813, "val_loss": 0.44535915851593016, "val_acc": 0.81}, {"epoch": 45, "train_loss": 0.46582722425460815, "val_loss": 0.43320303678512573, "val_acc": 0.815}, {"epoch": 46, "train_loss": 0.4602483105659485, "val_loss": 0.43938973903656003, "val_acc": 0.815}, {"epoch": 47, "train_loss": 0.46182712316513064, "val_loss": 0.44214366912841796, "val_acc": 0.82}, {"epoch": 48, "train_loss": 0.46190508127212526, "val_loss": 0.4367193150520325, "val_acc": 0.81}, {"epoch": 49, "train_loss": 0.46142646312713625, "val_loss": 0.4384293484687805, "val_acc": 0.815}, {"epoch": 50, "train_loss": 0.4595055556297302, "val_loss": 0.43934035301208496, "val_acc": 0.815}, {"epoch": 51, "train_loss": 0.4613054656982422, "val_loss": 0.44021149635314943, "val_acc": 0.82}, {"epoch": 52, "train_loss": 0.46070014238357543, "val_loss": 0.43884427547454835, "val_acc": 0.82}, {"epoch": 53, "train_loss": 0.4584649419784546, "val_loss": 0.4326984167098999, "val_acc": 0.815}, {"epoch": 54, "train_loss": 0.4587182831764221, "val_loss": 0.4384883666038513, "val_acc": 0.81}, {"epoch": 55, "train_loss": 0.45863779306411745, "val_loss": 0.4338670516014099, "val_acc": 0.805}, {"epoch": 56, "train_loss": 0.45789040565490724, "val_loss": 0.44531667470932007, "val_acc": 0.8}, {"epoch": 57, "train_loss": 0.4556708300113678, "val_loss": 0.43444046020507815, "val_acc": 0.83}, {"epoch": 58, "train_loss": 0.45661319732666017, "val_loss": 0.4379788517951965, "val_acc": 0.82}, {"epoch": 59, "train_loss": 0.4558184027671814, "val_loss": 0.43842005491256714, "val_acc": 0.81}, {"epoch": 60, "train_loss": 0.45931177377700805, "val_loss": 0.434033088684082, "val_acc": 0.81}, {"epoch": 61, "train_loss": 0.45677786707878115, "val_loss": 0.44403547048568726, "val_acc": 0.81}, {"epoch": 62, "train_loss": 0.4577578854560852, "val_loss": 0.43472466468811033, "val_acc": 0.82}, {"epoch": 63, "train_loss": 0.4594885575771332, "val_loss": 0.43530476808547974, "val_acc": 0.805}, {"epoch": 64, "train_loss": 0.4595878088474274, "val_loss": 0.44216720342636107, "val_acc": 0.815}, {"epoch": 65, "train_loss": 0.457929300069809, "val_loss": 0.4354310250282288, "val_acc": 0.805}, {"epoch": 66, "train_loss": 0.45684974193572997, "val_loss": 0.43885293245315554, "val_acc": 0.81}, {"epoch": 67, "train_loss": 0.45562248468399047, "val_loss": 0.44260137796401977, "val_acc": 0.815}, {"epoch": 68, "train_loss": 0.45553701639175415, "val_loss": 0.4341903781890869, "val_acc": 0.815}, {"epoch": 69, "train_loss": 0.45531578302383424, "val_loss": 0.4413549757003784, "val_acc": 0.805}, {"epoch": 70, "train_loss": 0.45376338958740237, "val_loss": 0.43612338066101075, "val_acc": 0.805}, {"epoch": 71, "train_loss": 0.4537080001831055, "val_loss": 0.4354015588760376, "val_acc": 0.81}, {"epoch": 72, "train_loss": 0.45402308702468874, "val_loss": 0.4437131881713867, "val_acc": 0.81}, {"epoch": 73, "train_loss": 0.45511074423789977, "val_loss": 0.4329813480377197, "val_acc": 0.81}, {"epoch": 74, "train_loss": 0.4553468108177185, "val_loss": 0.4379787492752075, "val_acc": 0.81}, {"epoch": 75, "train_loss": 0.45620954751968384, "val_loss": 0.4429515862464905, "val_acc": 0.815}, {"epoch": 76, "train_loss": 0.45437233805656435, "val_loss": 0.43501652240753175, "val_acc": 0.81}, {"epoch": 77, "train_loss": 0.45209871768951415, "val_loss": 0.44128095388412475, "val_acc": 0.8}, {"epoch": 78, "train_loss": 0.45598971605300903, "val_loss": 0.43534652948379515, "val_acc": 0.83}, {"epoch": 79, "train_loss": 0.4530110478401184, "val_loss": 0.4371797180175781, "val_acc": 0.8}, {"epoch": 80, "train_loss": 0.4540305984020233, "val_loss": 0.4362567949295044, "val_acc": 0.805}, {"epoch": 81, "train_loss": 0.45398648977279665, "val_loss": 0.44186986207962037, "val_acc": 0.825}, {"epoch": 82, "train_loss": 0.4521236133575439, "val_loss": 0.44251581430435183, "val_acc": 0.8}, {"epoch": 83, "train_loss": 0.4521400213241577, "val_loss": 0.4353618669509888, "val_acc": 0.805}, {"epoch": 84, "train_loss": 0.4546504604816437, "val_loss": 0.43935030937194824, "val_acc": 0.81}, {"epoch": 85, "train_loss": 0.45411895751953124, "val_loss": 0.439594144821167, "val_acc": 0.82}, {"epoch": 86, "train_loss": 0.4498390436172485, "val_loss": 0.4419629430770874, "val_acc": 0.805}, {"epoch": 87, "train_loss": 0.4535907435417175, "val_loss": 0.4397535705566406, "val_acc": 0.8}, {"epoch": 88, "train_loss": 0.4507227981090546, "val_loss": 0.43951390027999876, "val_acc": 0.81}, {"epoch": 89, "train_loss": 0.45214691162109377, "val_loss": 0.445698504447937, "val_acc": 0.8}, {"epoch": 90, "train_loss": 0.4507388854026794, "val_loss": 0.4336211013793945, "val_acc": 0.805}, {"epoch": 91, "train_loss": 0.4525550651550293, "val_loss": 0.44554840326309203, "val_acc": 0.805}, {"epoch": 92, "train_loss": 0.44955299615859984, "val_loss": 0.436931848526001, "val_acc": 0.81}, {"epoch": 93, "train_loss": 0.45122724533081054, "val_loss": 0.4410391855239868, "val_acc": 0.81}, {"epoch": 94, "train_loss": 0.4520698618888855, "val_loss": 0.4371036195755005, "val_acc": 0.81}, {"epoch": 95, "train_loss": 0.45127551317214964, "val_loss": 0.4425753474235535, "val_acc": 0.8}, {"epoch": 96, "train_loss": 0.4523266577720642, "val_loss": 0.44065939903259277, "val_acc": 0.805}, {"epoch": 97, "train_loss": 0.4555850899219513, "val_loss": 0.4407809662818909, "val_acc": 0.8}, {"epoch": 98, "train_loss": 0.4524273085594177, "val_loss": 0.4430235719680786, "val_acc": 0.8}, {"epoch": 99, "train_loss": 0.44941084504127504, "val_loss": 0.437322301864624, "val_acc": 0.815}, {"epoch": 100, "train_loss": 0.4516192102432251, "val_loss": 0.4396928095817566, "val_acc": 0.81}], "final_metrics": {"epoch": 100, "train_loss": 0.4516192102432251, "val_loss": 0.4396928095817566, "val_acc": 0.81}}
{"generated_layers": "self.layers = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 3))\nself.optimizer = torch.optim.Adam(self.parameters(), lr=0.0004)\nself.training_epochs = 100", "history": [{"epoch": 1, "train_loss": 1.0851153469085693, "val_loss": 0.9964551782608032, "val_acc": 0.405}, {"epoch": 2, "train_loss": 0.9952462434768676, "val_loss": 0.9513607931137085, "val_acc": 0.425}, {"epoch": 3, "train_loss": 0.9519425702095031, "val_loss": 0.9132184219360352, "val_acc": 0.51}, {"epoch": 4, "train_loss": 0.9087529444694519, "val_loss": 0.8754278421401978, "val_acc": 0.65}, {"epoch": 5, "train_loss": 0.8668731355667114, "val_loss": 0.8324991989135743, "val_acc": 0.705}, {"epoch": 6, "train_loss": 0.8154263615608215, "val_loss": 0.7853413724899292, "val_acc": 0.72}, {"epoch": 7, "train_loss": 0.772118158340454, "val_loss": 0.7389865112304688, "val_acc": 0.75}, {"epoch": 8, "train_loss": 0.7279857754707336, "val_loss": 0.7015955662727356, "val_acc": 0.795}, {"epoch": 9, "train_loss": 0.6861609554290772, "val_loss": 0.6626648616790771, "val_acc": 0.785}, {"epoch": 10, "train_loss": 0.6506479549407959, "val_loss": 0.6287606239318848, "val_acc": 0.805}, {"epoch": 11, "train_loss": 0.6214825081825256, "val_loss": 0.5971081686019898, "val_acc": 0.825}, {"epoch": 12, "train_loss": 0.5969170022010803, "val_loss": 0.580463080406189, "val_acc": 0.8}, {"epoch": 13, "train_loss": 0.5770606994628906, "val_loss": 0.555523419380188, "val_acc": 0.825}, {"epoch": 14, "train_loss": 0.5590347504615784, "val_loss": 0.5416855931282043, "val_acc": 0.785}, {"epoch": 15, "train_loss": 0.5439889621734619, "val_loss": 0.5317776823043823, "val_acc": 0.82}, {"epoch": 16, "train_loss": 0.5342247581481934, "val_loss": 0.5142155027389527, "val_acc": 0.81}, {"epoch": 17, "train_loss": 0.5217130315303803, "val_loss": 0.5029386281967163, "val_acc": 0.81}, {"epoch": 18, "train_loss": 0.5136717057228088, "val_loss": 0.5018087410926819, "val_acc": 0.82}, {"epoch": 19, "train_loss": 0.5064746260643005, "val_loss": 0.4864290714263916, "val_acc": 0.83}, {"epoch": 20, "train_loss": 0.5016687691211701, "val_loss": 0.4828329014778137, "val_acc": 0.825}, {"epoch": 21, "train_loss": 0.49514220356941224, "val_loss": 0.4790704560279846, "val_acc": 0.805}, {"epoch": 22, "train_loss": 0.4906030321121216, "val_loss": 0.4720108771324158, "val_acc": 0.83}, {"epoch": 23, "train_loss": 0.48901321649551394, "val_loss": 0.4737638759613037, "val_acc": 0.815}, {"epoch": 24, "train_loss": 0.4838319766521454, "val_loss": 0.4603999900817871, "val_acc": 0.835}, {"epoch": 25, "train_loss": 0.4848992657661438, "val_loss": 0.47388357400894165, "val_acc": 0.81}, {"epoch": 26, "train_loss": 0.4823842191696167, "val_loss": 0.45808149814605714, "val_acc": 0.82}, {"epoch": 27, "train_loss": 0.477980899810791, "val_loss": 0.4582328224182129, "val_acc": 0.825}, {"epoch": 28, "train_loss": 0.4809519553184509, "val_loss": 0.45765684604644774, "val_acc": 0.815}, {"epoch": 29, "train_loss": 0.4817423284053802, "val_loss": 0.46346038818359375, "val_acc": 0.815}, {"epoch": 30, "train_loss": 0.476200966835022, "val_loss": 0.4590443181991577, "val_acc": 0.815}, {"epoch": 31, "train_loss": 0.4772342705726624, "val_loss": 0.458591468334198, "val_acc": 0.805}, {"epoch": 32, "train_loss": 0.4755511498451233, "val_loss": 0.4523442006111145, "val_acc": 0.825}, {"epoch": 33, "train_loss": 0.47125995039939883, "val_loss": 0.45697941780090334, "val_acc": 0.8}, {"epoch": 34, "train_loss": 0.4712201523780823, "val_loss": 0.45188360929489135, "val_acc": 0.815}, {"epoch": 35, "train_loss": 0.46817299604415896, "val_loss": 0.4526000714302063, "val_acc": 0.8}, {"epoch": 36, "train_loss": 0.466114467382431, "val_loss": 0.44923981189727785, "val_acc": 0.81}, {"epoch": 37, "train_loss": 0.46842294812202456, "val_loss": 0.44864667654037477, "val_acc": 0.825}, {"epoch": 38, "train_loss": 0.4677907860279083, "val_loss": 0.4473396396636963, "val_acc": 0.815}, {"epoch": 39, "train_loss": 0.4689425051212311, "val_loss": 0.44460790157318114, "val_acc": 0.82}, {"epoch": 40, "train_loss": 0.4681792402267456, "val_loss": 0.45354227542877196, "val_acc": 0.82}, {"epoch": 41, "train_loss": 0.46366239547729493, "val_loss": 0.44684544324874875, "val_acc": 0.82}, {"epoch": 42, "train_loss": 0.46439246654510496, "val_loss": 0.44667681455612185, "val_acc": 0.815}, {"epoch": 43, "train_loss": 0.4665071678161621, "val_loss": 0.4453610873222351, "val_acc": 0.81}, {"epoch": 44, "train_loss": 0.47188503861427306, "val_loss": 0.45605712890625, "val_acc": 0.81}, {"epoch": 45, "train_loss": 0.4635258531570435, "val_loss": 0.44463439464569093, "val_acc": 0.82}, {"epoch": 46, "train_loss": 0.4611962068080902, "val_loss": 0.4534553408622742, "val_acc": 0.805}, {"epoch": 47, "train_loss": 0.4621036469936371, "val_loss": 0.4515306711196899, "val_acc": 0.825}, {"epoch": 48, "train_loss": 0.46489731550216673, "val_loss": 0.45144201517105104, "val_acc": 0.815}, {"epoch": 49, "train_loss": 0.46303692102432253, "val_loss": 0.4485002565383911, "val_acc": 0.805}, {"epoch": 50, "train_loss": 0.46386431574821474, "val_loss": 0.4460511445999146, "val_acc": 0.805}, {"epoch": 51, "train_loss": 0.46036975622177123, "val_loss": 0.44496877908706667, "val_acc": 0.81}, {"epoch": 52, "train_loss": 0.4638143873214722, "val_loss": 0.46133899688720703, "val_acc": 0.805}, {"epoch": 53, "train_loss": 0.46332604765892027, "val_loss": 0.44483609914779665, "val_acc": 0.825}, {"epoch": 54, "train_loss": 0.4582581555843353, "val_loss": 0.449866042137146, "val_acc": 0.805}, {"epoch": 55, "train_loss": 0.460232800245285, "val_loss": 0.44154892444610594, "val_acc": 0.8}, {"epoch": 56, "train_loss": 0.4604004955291748, "val_loss": 0.4508959150314331, "val_acc": 0.815}, {"epoch": 57, "train_loss": 0.4597862195968628, "val_loss": 0.44184993505477904, "val_acc": 0.81}, {"epoch": 58, "train_loss": 0.4639210867881775, "val_loss": 0.45283505201339724, "val_acc": 0.8}, {"epoch": 59, "train_loss": 0.45935924410820006, "val_loss": 0.44219777345657346, "val_acc": 0.81}, {"epoch": 60, "train_loss": 0.4583366334438324, "val_loss": 0.44454622507095337, "val_acc": 0.805}, {"epoch": 61, "train_loss": 0.45858437538146973, "val_loss": 0.4493874478340149, "val_acc": 0.81}, {"epoch": 62, "train_loss": 0.4575109195709228, "val_loss": 0.44370338678359983, "val_acc": 0.805}, {"epoch": 63, "train_loss": 0.4584579312801361, "val_loss": 0.44572357177734373, "val_acc": 0.815}, {"epoch": 64, "train_loss": 0.45787292122840884, "val_loss": 0.45307124614715577, "val_acc": 0.81}, {"epoch": 65, "train_loss": 0.4669289565086365, "val_loss": 0.4413035249710083, "val_acc": 0.835}, {"epoch": 66, "train_loss": 0.4650749206542969, "val_loss": 0.4631823515892029, "val_acc": 0.795}, {"epoch": 67, "train_loss": 0.4613643956184387, "val_loss": 0.4448717713356018, "val_acc": 0.82}, {"epoch": 68, "train_loss": 0.45852349281311033, "val_loss": 0.44397587299346924, "val_acc": 0.8}, {"epoch": 69, "train_loss": 0.46004181504249575, "val_loss": 0.44037690162658694, "val_acc": 0.815}, {"epoch": 70, "train_loss": 0.46127440690994265, "val_loss": 0.4571854305267334, "val_acc": 0.805}, {"epoch": 71, "train_loss": 0.4582201623916626, "val_loss": 0.4444377732276916, "val_acc": 0.8}, {"epoch": 72, "train_loss": 0.45410863995552064, "val_loss": 0.4531582927703857, "val_acc": 0.8}, {"epoch": 73, "train_loss": 0.45901745080947876, "val_loss": 0.45074815034866333, "val_acc": 0.805}, {"epoch": 74, "train_loss": 0.45536615133285524, "val_loss": 0.4453257441520691, "val_acc": 0.825}, {"epoch": 75, "train_loss": 0.45613156914711, "val_loss": 0.4465115237236023, "val_acc": 0.8}, {"epoch": 76, "train_loss": 0.45678129434585574, "val_loss": 0.4481328368186951, "val_acc": 0.805}, {"epoch": 77, "train_loss": 0.45913320064544677, "val_loss": 0.45290312767028806, "val_acc": 0.815}, {"epoch": 78, "train_loss": 0.4564935517311096, "val_loss": 0.449192681312561, "val_acc": 0.815}, {"epoch": 79, "train_loss": 0.4569813346862793, "val_loss": 0.44149240493774417, "val_acc": 0.82}, {"epoch": 80, "train_loss": 0.45720252752304075, "val_loss": 0.4466379928588867, "val_acc": 0.815}, {"epoch": 81, "train_loss": 0.45995706677436826, "val_loss": 0.45333008766174315, "val_acc": 0.81}, {"epoch": 82, "train_loss": 0.46083951950073243, "val_loss": 0.45003610372543335, "val_acc": 0.805}, {"epoch": 83, "train_loss": 0.4539601004123688, "val_loss": 0.4479224157333374, "val_acc": 0.815}, {"epoch": 84, "train_loss": 0.45744875907897947, "val_loss": 0.4472213292121887, "val_acc": 0.815}, {"epoch": 85, "train_loss": 0.45472874760627746, "val_loss": 0.45046533823013307, "val_acc": 0.82}, {"epoch": 86, "train_loss": 0.4553234624862671, "val_loss": 0.4438063836097717, "val_acc": 0.8}, {"epoch": 87, "train_loss": 0.4533086609840393, "val_loss": 0.4468675518035889, "val_acc": 0.82}, {"epoch": 88, "train_loss": 0.4542219889163971, "val_loss": 0.4495851135253906, "val_acc": 0.82}, {"epoch": 89, "train_loss": 0.45462912917137144, "val_loss": 0.44646578788757324, "val_acc": 0.805}, {"epoch": 90, "train_loss": 0.45335628390312194, "val_loss": 0.44913448572158815, "val_acc": 0.81}, {"epoch": 91, "train_loss": 0.45316762924194337, "val_loss": 0.4475805687904358, "val_acc": 0.8}, {"epoch": 92, "train_loss": 0.4535812175273895, "val_loss": 0.4483357000350952, "val_acc": 0.815}, {"epoch": 93, "train_loss": 0.45384971141815184, "val_loss": 0.4471970868110657, "val_acc": 0.805}, {"epoch": 94, "train_loss": 0.4549519824981689, "val_loss": 0.4511662197113037, "val_acc": 0.825}, {"epoch": 95, "train_loss": 0.4567132043838501, "val_loss": 0.4530392861366272, "val_acc": 0.82}, {"epoch": 96, "train_loss": 0.45401999473571775, "val_loss": 0.44357696294784543, "val_acc": 0.805}, {"epoch": 97, "train_loss": 0.45736945867538453, "val_loss": 0.45065377235412596, "val_acc": 0.81}, {"epoch": 98, "train_loss": 0.4544392514228821, "val_loss": 0.4513808107376099, "val_acc": 0.805}, {"epoch": 99, "train_loss": 0.45610377073287967, "val_loss": 0.4538519644737244, "val_acc": 0.795}, {"epoch": 100, "train_loss": 0.45267740488052366, "val_loss": 0.4444296932220459, "val_acc": 0.81}], "final_metrics": {"epoch": 100, "train_loss": 0.45267740488052366, "val_loss": 0.4444296932220459, "val_acc": 0.81}}
